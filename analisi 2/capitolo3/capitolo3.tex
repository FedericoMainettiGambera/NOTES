\section*{Calcolo differenziale per funzioni reali di più variabili}
\subsection*{Continuità di una funzione in più variabili:}
Una funzione $f \;\;:\;\; \mathbb{R}^n \rightarrow  \mathbb{R}$ è continua in un punto $x_0$ se 
\[
    \lim_{x\rightarrow x_0} f(x) = f(x_0)
\]
La continuità di una funzione è anche deducibile dal fatto che sia costituita (somma/prodotto/quoziente/certe volte anche composizione) da funzioni elementari continue.\newline
\subsection*{Calcolo di limiti in più variabili}
\subsubsection*{Non esistenza del limite}
Per mostrare che un certa funzione in più varibili non ammette limite in un determinato punto, è sufficiente determinare duee curve passasnti per il punto lungo le quali la funzione assume limiti diversi.
\textbf{es.} 
\[
    \lim_{(x,y)\rightarrow (0,0)} \frac{xy}{x^2+y^2}
\]
Analiziamo la funzione lungo due curve:
\begin{itemize}
    \item con $y=x$ ottengo $f(x,x) = \frac{1}{2}$
    \item con $y=-x$ ottengo $f(x,-x) = - \frac{1}{2}$
\end{itemize}
non ammette limite.\newline
\subsubsection*{Uso di maggiorazioni con funzioni radiali per provare l'esistenza del limite}
Per dimostrare l'esistenza di un limite per $(x,y) \rightarrow (0,0)$, si impone $x=\rho \cdot  cos(\theta)$ e $y= \rho \cdot sin(\theta)$, successivamente si l'intera funzione sotto modulo e procede con semplificazioni e maggiorazioni (per eliminare i seni e i coseni). E' essenziale che la funzione non dipenda da $\theta$.\newline
Più in generale se si volesse calcolare il limite per $(x,y) \rightarrow (x_0, y_0)$ si pongono $x=x_0 +\rho \cdot  cos(\theta)$ e $y= y_0 + \rho \cdot sin(\theta)$
\subsection*{Topologia in $\mathbb{R}^n$ e proprità delle funzioni continua}
Sia $E$ un sottoinsieme di $\mathbb{R}^n$, un punto $x_0$ si dice:
\begin{itemize}
    \item interno ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E$;
    \item esterno ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E^c$;
    \item di frontiera per $E$, se ogni intorno centrato in $x_0$ contiene almeno un punto di $E$ e uno di $E^c$.
\end{itemize}
Un insieme $E \subseteq \mathbb{R}^n$ si dice:
\begin{itemize}
    \item aperto, se ogni suo punto è interno a $E$;
    \item chiuso, se il suo complementare è aperto.
\end{itemize} 
Sia $E$ un sottoinsieme di $\mathbb{R}^n$, si dice:
\begin{itemize}
    \item interno di $E$, e si indica con $E^o$, l'insieme dei punti interni di $E$;
    \item frontiera o brodo di $E$, e si indica con $\delta E$, l'insieme dei punti di frontiera di $E$;
    \item chiusura di $E$, e si indica con $\bar{E}$, l'insieme $E \cup \delta E$.
\end{itemize}
Alcune informazioni extra: 
\begin{itemize}
    \item si ha sempre $E^o \subseteq \delta E \subseteq \bar{E}$;
    \item il complementare di un aperto è chiuso e viceversa; 
    \item esistono insiemi nè aperti nè chiusi, gli unici insiemi sia aperti sia chiusi sono quello vuoto e $\mathbb{R}^n$;
    \item l'unione di una famiglia qualsiasi (anche infinita) di insiemi aperti e l'intersezione di un numero finito di insiemi aperti sono insiemi aperti 
    \item l'intersezione di una famiglia qualsiasi (anche infinita) di insiemi chiusi è l'unione di un numero finito di insiemi chiusi sono insiemi chiusi;
    \item un insieme aperto non contiene nessuno dei suoi punti di frontiera, un insieme chiuso contiene tutti i suoi punti di frontiera.
\end{itemize}
Un inieme si dice:
\begin{itemize}
    \item limitato se esiste un intorno che lo contiene tutto;
    \item connesso se per ogni coppia di punti dell'insieme, esiste un arco continuo che che li connette contenuto nell'insieme.
\end{itemize}
Proprietà topologiche delle funzioni continue:
\begin{itemize}
    \item \textbf{teor.} Teorema di Weierstrass. Sia $E \subset \mathbb{R}^n$ un insieme chiuso e limitato e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua, allora $f$ ammette massimo e minimo in $E$, cioè esistono $x_m$ e $x_M$ tali che $f(x_m) \leq f(x) \leq f(x_M)$ per ogni $x in E$.
    \item \textbf{teor.} Teorema degli zeri. Sia $E$ un insieme connesso di $\mathbb{R}^n$ e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua. Se $x$, $y$ sono due punti di $E$ tali che $ f(x) < 0$ e $f(y) > 0$, allora esiste un terzo punto $z \in E$ in cui $f$ si annulla. In particolare, lungo ogni arco di curva continua contenuto in $E$ che congiunge $x$ e $y$, c'è almeno un punto in cui $f$ si annulla.
\end{itemize}
\subsection*{Derivate parziali, piano tangente, differenziale}
\subsubsection*{Derivata parziale}
Calcolo di una derivata parziale tramite la definizione di rapporto incrementale in un punto $(x_0, y_0)$. \newline
Per prima fissiamo $y=y_0$ e deriviamo rispetto alla $x$:
\[
    \frac{\delta f}{\delta x} (x_0,y_0) = \lim_{h\rightarrow 0}\frac{f(x_0+h, y_0)- f(x_0,y_0)}{h}.
\]
Successivamente facciamo l'opposto, cioè fissiamo $x=x_0$ e deriviamo rispetto alla $y$:
\[
    \frac{\delta f}{\delta y} (x_0,y_0) = \lim_{k\rightarrow 0}\frac{f(x_0, y_0+k)- f(x_0,y_0)}{k}
\]
Una funzione $f: A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ si dice derivabile in un punto del suo dominio se in quel punto esistono tutte le sue derivate parziali; si dice derivabile in $A$ se è derivabile in ogni punto di $A$.\newline
Se $f$ è derivabile in un punto, chiameremo gradiente ( $\nabla f(x)$ ) il vettore delle sue derivate parziali.
\subsubsection*{Piano tangente}
Costruire il piano tangente a una funzione in due variabili in un punto $(x_0, y_0)$:
\begin{enumerate}
    \item troviamo la retta tangente alla funzione nel piano $y=y_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0)\\
            &y=y_0 \\
        \end{cases}
    \]
    \item troviamo la retta tangente alla funzione nel piano $x=x_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)\\
            &x=x_0 \\
        \end{cases}
    \]
    \item costruiamo il piano che contiene entrambe le rette:
    \[
        z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)
    \]
\end{enumerate}
Il procedimento appena mostrato individua il piano tangente nell'ipotesi che esso esista, potrebbe però non esserci.
\subsubsection*{Differenziabilità e approssimazione lineare}
In due o più variabili la sola derivabilità non implica nè continuità nè l'esistenza del piano tangente.\newline
Concetto di differenziabilità in più variabili: l'incremento di $f$ è uguale all'incremento calcolato lungo il piano tangente, più un infinitesimo di ordine superiore rispetto alla lunghezza dell'incremento $(h,k)$ delle variabili indipendenti. In formule:
\[
    f(x_0 + h, y_0 + k) - f(x_0, y_0) = \frac{\delta f}{\delta x} (x_0,y_0) \cdot (x-x_0) + \frac{\delta f}{\delta y}(x_0, y_0) \cdot (y-y_0) + o(\sqrt{h^2 + k^2})
\]
per $(h,k) \rightarrow  (0,0)$.\newline
Tutto ciò che è prima dell'uguale (primo membro) rappresenta l'incremento della funzione, i primi due addendi del secondo membro rappresentano l'incremento calacolato lungo il pinao tangente. Ricordiamo che l'ultimo addendo rappresenta una funzione tale che $\lim_{(h,k)\rightarrow (0,0)}\frac{o(\sqrt{h^2 + k^2})}{\sqrt{h^2 + k^2}} = 0$.\newline
Se l'equazione di prima è soddisfatta, diremo che la funzione è differenziabile in $(x_0, y_0)$.\newline

Da notare che la differenziabilità implica la derivabilità, cioè se una funzione è differenziabile in un punto, allora è anche derivabile nello stesso.\newline

Se $f$ è differenziabile in $x_0$, si dice differenziale di $f$ calcolato in $x_0$ la funzione lineare $df(x_0) \;\;:\;\; \mathbb{R}^n \rightarrow  \mathbb{R}$ definita da:
\[
    df(x_0) \;\;:\;\;h \rightarrow  \nabla f(x_0) \cdot h.
\] 
Nel caso in due varibiali, il numero $\nabla f(x_0) \cdot h$ rappresenta l'incremento della funzione nel passare da $x_0$ a $x_0+h$, calcolato lungo il piano tangente al grafico di $f$ in $x_0$.\newline

L'approssimazione dell'incremento di $f$ con il suo differenziale prende il nome di linearizzazione.

\subsubsection*{Verifica della differenziabilità}
Per dimostrare la differenziabilità in un punto $(x_0,y_0)$ bisogna provare che:
\[
    \lim_{h,k\rightarrow 0,0}\frac{f(x_0+h, y_0+k)-\{
        f(x_0,y_0)+ \frac{\delta f}{\delta x}(x_0,y_0) h + \frac{\delta f}{\delta y}(x_0,y_0)k
        \}}{\sqrt{h^2+k^2}} =0.
\]
Ma per certi casi perticolari esistono criteri molto comodi e più semplici.\newline

Teorema di condizione sufficiente di differenziabilità: se le derivate parziali di $f$ esistono in un intorno di $x_0$ e sono continue in $x_0$, allora $f$ è differenziabile in $x_0$.\newline
In particolare se le derivate parziali esistono e sono continue in tutto $A$, allora $f$ è differenziabile in tutto $A$.\newline
Una funzione le cui derivate parziali esistono e sono continue in tutto $A$ si dice di classe $C^1(A)$, dunque: $f \in C^1(A) \rightarrow$ f differenziabile in $A$.
\subsubsection*{Derivate direzionali}
Si dice derivata direzionale della funzione $f$ rispetto al versore $v$, nel punto $x_0$, il limite
\[
    D_vf(x_0) = \lim_{t\rightarrow 0}\frac{f(x_0 + tv) - f(x_0)}{t}
\]
purchè esista finito.\newline
Detto in maniera diversa signifca considerare la restrizione della funzione $f$ alla direzione della retta passante per $x_0$ con direzione $v$, cioè $g(t)= f(x_0+tv)$, e calcolarne la derivata, cioè $D_vf(x_0)= g'(0)$.\newline

Formula del gradiente: $D_vf(x_0) = \nabla f(x_0) \cdot v =  \sum_{i=1}^{n} \frac{\delta f}{\delta x_i}(x_0) \cdot  v_i$. Cioè la derivata direzionale è il prodotto scalare del gradiente con il versore nella direzione in cui si deriva, quindi tutte le derivate direzionali sono combinazioni lineari delle derivate parziali. Nel caso in due variabili la formula si riduce a $D_vf(x_0) = \nabla f(x_0,y_0) \cdot v = \frac{\delta f}{\delta x} (x_0,y_0)cos(\theta) + \frac{\delta f}{\delta y}(x_0, y_0) sin(\theta)$.\newline

Da notare è che $\nabla f(x_0)$ indica la direzione di massima crescita di $f$, ossia la direzione di massima derivata direzionale, invece $-\nabla f(x_0)$ rappresenta la direzione di minima derivata direzionale, infine nelle direzioni ortogonali al gradiente le derivate direzionali sono nulle.
\subsubsection*{Riepilogo}
\begin{itemize}
    \item $f \in C^1(A) \Rightarrow f$ differenziabile in $A$ (cioè $f$ ha iperpiano tangente) $\Rightarrow f$ è continua, derivabile, ha derivate direzionali, vale la formula del gradiente.
    \item $f$ continua, derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ differenziabile
    \item $f$ derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ continua 
\end{itemize}
\subsubsection*{Calcolo delle derivate}
\[
    \delta(\alpha \cdot f + \beta \cdot g) = \alpha \delta(f) + \beta \delta (g)
\]
\[
    \delta(f \cdot  g) = g \cdot \delta(f) + f \delta(g)
\]
\[
    \delta(\frac{f}{g}) = \frac{ g \cdot \delta(f) - f \delta(g)}{g^2}
\]
\[
    h(x) = f(g(x)) = g \circ f \Rightarrow h'(x) = f'(g(x)) g'(x)
\]
\subsubsection*{Gradiente di una funzione radiale}
Si chiama funzione radiale una funzione $h$ che dipende solo dalla distanza di dall'origine, ossia
\[
    h(x) = g(|x|).
\]
ponendo $\rho = |x| = \sqrt{\sum_{j=1}^{n}x_j^2}$ si ha:
\[
    \nabla_\rho = ( \frac{x_1}{\rho}, \frac{x_2}{\rho}, \dots, \frac{x_n}{\rho}).
\]
\[
    \nabla h(x) = g'(|x|)( \frac{x_1}{|x|}, \dots, \frac{x_n}{|x|})
\]
\[
    |\nabla h(x)| = |g'(|x|)|
\]
\subsubsection*{Ortogonalità del gradiente con le curve di livello}
Il gradiente è ortogolane il ogni punto alle linee di livello
\subsubsection*{Equazione del trasporto}
Si definisce equazione del trasporto la seguente:
\[
    c \frac{\delta u}{\delta x} + \frac{\delta u}{\delta t} = 0 \;\;\;\;\; \;\;\;\;\; (?)
\]
\newline
Teorema del valor medio. Sia $A \subset \mathbb{R}^n$ un aperto e $f \;\;:\;\; A \rightarrow \mathbb{R}$ una funzione differenziabile in $A$. Allora per ogni coppia di punti $x_0, x_1 \in A$, esiste un punto $x^*$ tale per cui:
\[
    f(x_1)- f(x_0) = \nabla f(x^*) \cdot  (x_1 + x_0).
\]
In particolare:
\[
    |f(x_1) - f(x_0)| \leq |\nabla f(x^*)| \cdot |(x_1 + x_0)|.
\]
\subsection*{Derivate di ordine superiore e approssimazioni successive}
\subsubsection*{Derivate di ordine superiore}
Teorema di Schwartz. Sia $f \;\;:\;\; A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ con $A$ aperto. Supponiamo che (per certi indici $i,j \in \{1,2,\dots,n\}$) le derivate seconde miste $f_{x_i, x_j}$ e $f_{x_j, x_i}$ esistano in un certo $x_0$ e siano continue in $x_0$; allora esse coincidono in $x_0$.\newline

Una funzione che ha tutte le derivate parziali seconde continue in un aperto $A$ si dice di classe $C^2(A)$.\newline
Se $f \in C^2(A)$, allora $f \in C^1(A)$ (in particolare $f$ è differenziabile), le derivate parziali prima sono differenziabili, le derivate parziali seconde sono continue, le derivate seconde miste sono uguali.
\subsection*{Differenziale secondo, matrice hessiana, formula di Taylor al secondo ordine}
Se $f \in C^2(A)$ e $x_0 \in A$, si dice differenziale secondo di $f$ in $x_0$ la funzione
\[
    d^2f(x_0) \;\;:\;\; h \rightarrow \sum_{i=1}^{n}\sum_{j=0}^{n}\frac{\delta^2(f)}{\delta(x_i) \delta(x_j)}(x_0)h_ih_j.
\]
I vari coefficienti $\frac{\delta^2(f)}{\delta(x_i) \delta(x_j)}(x_0)$ possono essere ordinati in una matrice detta Hessiana:
\[
    H_f(x_0) =\begin{matrix}
        f_{x_1x_1}(x_0) \;\; &f_{x_1x_2}(x_0) \;\; &\dots \;\; &f_{x_1x_n}(x_0)\\
        f_{x_2x_1}(x_0) \;\; &f_{x_2x_2}(x_0) \;\; &\dots \;\; &f_{x_2x_n}(x_0)\\
        \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
        f_{x_nx_1}(x_0) \;\; &f_{x_nx_2}(x_0) \;\; &\dots \;\; &f_{x_nx_n}(x_0)
    \end{matrix}
\]  
In particolare, per due variabili:
\[
    H_f(x_0, y_0) = \begin{matrix}
        f_{xx}(x_0, y_0) \;\; f_{xy}(x_0, y_0)\\
        f_{yx}(x_0, y_0) \;\; f_{yy}(x_0, y_0)
    \end{matrix}
\]
Se $f$ è di classe $C^2$, la matrice Hessiana è simmetrica.\newline

Formula di Taylor (resto secondo Lagrange). Sia $f \in c^2(A)$; per ogni $x_0 \in A$ e $h \in \mathbb{R}^n$, tale che $x_0 + h \in A$, esiste un numero reale $\delta \in (0,1)$, dipendente da $x_0$ e $h$, tale che:
\[
    f(x_0 + h) = f(x_0) +\sum_{i=1}^{n}\frac{\delta f}{\delta x_i}(x_0)h_i + \frac{1}{2}\sum_{i,j=1}^{n}\frac{\delta^2f}{\delta x_i \delta x_j}(x_0 + \delta h) h_i h_j.
\]\newline

Formula di Taylor (resto secondo Peano). Sia $f \in C^2(a)$. Per ogni $x_0 \in A$ vale la formula:
\[
    f(x_0 + h) = f(x_0) +\sum_{i=1}^{n}\frac{\delta f}{\delta x_i}(x_0)h_i + \frac{1}{2}\sum_{i,j=1}^{n}\frac{\delta^2f}{\delta x_i \delta x_j}(x_0) h_i h_j +o(|h|^2).
\]

\subsection*{Ottimizzaione, estremi liberi}
\subsubsection*{Generalità sui problemi di ottimizzazione}
\begin{itemize}
    \item $x_0$ è detto punto di massimo (minimo) globale se per ogni $x$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$);
    \item $x_0$ è detto punto di massimo (minimo) locale se esiste un intorno di $x_0$ detto $U$ tale per cui per ogni $x \in U$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$).
\end{itemize}
\subsubsection*{Estremi liberi, condizioni necessarie del prim'ordine}
Teorema di Fermat. Sia $f \;\;:\;\; A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$, con $A$ aperto e $x_0 \in A$ un punto di massimo o minimo locale per $f$. Se $f$ è derivabile in $x_0$, allora $\nabla f(x_0) = 0$.\newline

I punti in cui il gradiente di una funzione si annulla si dicono punti critici o stazionari di $f$. Una volta individuati tutti i punti stazionari, si può iniziare un'analisi su di essi per verificare se sono o meno punti di massimo o minimo. Se non lo sono essi prendono il nome di punti di sella o colle. Da notare particolarmente è che una funzione può assumere valori di massimo o minimo anche in punti in cui non è derivabile, dunque questi punti vanno analizzati separatamente.
\subsubsection*{Forme quadratiche, classificazione}
Un modo per determinare la natura di un punto stazionario è quello di analizzare il segno dell'incremento $\nabla f(x_0) = f(x_0 + h ) - f(x_0)$. Se infatti si riesce a stabilire che $\nabla f(x_0)$ si mantiene di segno positivo o negativo, per ogni $h$ di modulo abbastanza piccolo, possiamo dedurre che $x_0$ è punto di minimo o massimo locale. Se invece al variare di $h$, $\nabla f(x_0)$ cambia segno, siamo in presenza di un punto si sella.\newline
Lo studio del segno di $\nabla f(x_0)$ riconduce all'analisi del segno del polinomio omogeneo di secondo grado nelle componenti di $h$ (che prende il nome di forma quadratica) dato da
\[
    \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j.
\]
Ogni forma quadratica risulta associata a una matrice simmetrica $M$. Nel caso del differenziale la matrice $M$ coincide con la matrice Hessiana.\newline
Il segno della forma quadratica è quindi studiabile analizzando la sua matrice $M = \begin{matrix}
    a \;\;\; b\\
    b \;\;\; c\\
\end{matrix}$ con $a\neq 0$ nel seguente modo:
\begin{itemize}
    \item è definitivamente positiva (negativa) se e solo se $det(M)>0$ e $a >0$ ($a<0$);
    \item indefinita se $det(M)<0$;
    \item semidefinita positiva (negativa) se e solo se $det(M)=0$ e $a>0$ ($a<0$).
\end{itemize}
Se $a = 0$ e $c\neq 0$, nelle affermazioni precedenti occore sostituire $a$ con $c$.
\subsubsection*{Forme quadratiche, test degli autovalori}
Un importante test per determinare il segno di una funzione quadratica in $\mathbb{R}^n$ è basato sul segno degli autovalori della matrice $M$.\newline
Ricordiamo che un numero complesso $\lambda$ e un vettore non nullo $v \in \mathbb{C}^n$ si dicono, rispettivamente, autovalore e autovettore (di $\lambda$) di una matrice $M$ di ordine $n$, se soddisfano la relazione:
\[
    Mv = \lambda v
\]
oppure
\[
    (M-\lambda I_n)v = 0.
\]
Quest'ultima equazione ha soluzioni $v$ non nulle se e solo se la matrice dei coefficienti e singolare, ovvero se $\lambda$ è soluzione dell'equazione caratteristica:
\[
    det(M-\lambda I) = 0
\]
esistono esattamente $n$ autovalori di $M$ ciascuno contato secondo la propria molteplicità.\newline
Le matrici $M$ simmetriche hanno prorpietà importanti:
\begin{itemize}
    \item gli autovalori di $M$ sono reali e possiedono autovettori reali;
    \item esistono $n$ autovettori lineari che costituiscono una base ortonormale in $\mathbb{R}^n$;
    \item La matrice $S = {w_1, w_2, \dots, w_n}$ le cui colonne sono gli autovettori lineari è orotognale e diagonalizza $M$, precisamente:
    \[
        S^TMS = \Lambda = \begin{matrix}
            \lambda_1 \;\; &0 \;\; &\dots \;\; &0\\
            0 \;\; &\lambda_2 \;\; &\dots \;\; &0\\
            \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
            0 \;\; &0 \;\; &\dots \;\; &\lambda_n
        \end{matrix}
    \]
\end{itemize}
Tornando allo studio del segno della forma quadratica con la sua matrice $M$:
\begin{itemize}
    \item definitivamente positiva (negativa) se e solo se tutti gli autovalori di $M$ sono positivi (negativi);
    \item semidefinita positiva (negativa) se e solo se tutti gli autovalori di $M$ sono $\geq 0$ ($\leq 0$) e almeno uno di essi è nullo;
    \item indefinita se $M$ ha almeno un autovalore positivo e uno negativo.
\end{itemize}
Da notare è che per una forma quadratica l'origine è sempre un punto stazionario.
\subsubsection*{Studio della natura dei punti critici}
Per estrarre informazioni su un punto critico $x_0$ occorre studiare e classificare la forma quadratica 
\[
    q(h) = \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j = h^TH_f(x_0)h
\]
dove $H_f(x_0)$ è la matrice Hessiana di $f$ in $x_0$.\newline
\begin{itemize}
    \item se la forma quadratica è definitivamente positiva (negativa), allora $x_0$ è un punto di minimo (massimo) locale forte;
    \item se la forma quadratica è indefinita, allora $x_0$ è un punto di sella;
    \item se la forma quadratica è in $x_0$ semidefinita positiva (negativa) e non nulla, allora $x_0$ è di minimo (massimo) debole oppure di colle; la situazione cambia se la forma quadratica è semidefinita positiva (negativa) non solo in $x_0$, ma anche per ogni $x$ in un intorno di $x_0$, f è convessa (concava), dunque un punto di minimo (massimo) debole;
    \item se la forma quadratica è nulla, allora non possiamo estrarne informazioni significanti.
\end{itemize}
Vediamo una strategia da seguire:
\begin{enumerate}
    \item si isolano i punti di $f$ che non sono regolari (es. non derivabili una o due volte). Questi punti dovranno essere analizzati separatamente;
    \item trovare i punti critici risolvendo:
        \[
            \begin{cases}
                &f_{x_1}(x_1,x_2,\dots,x_n) = 0\\
                &f_{x_2}(x_1,x_2,\dots,x_n) = 0\\
                &\dots\\
                &f_{x_n}(x_1,x_2,\dots,x_n) = 0\\
            \end{cases}
        \]
    \item si studia il segno della forma quadratica per ogni punto critico, se è definita o indefinita si giunge a una conclusione con le regole dette precedentemente, se è nulla o semidefinita si ricorre a uno studio diretto di $\nabla f(x_0)$ in un intorno di $x_0$.
\end{enumerate}
Più precisamente, nel caso bidimensionale, per ogni punto critico:
\begin{enumerate}
    \item si calcola l'Hessiana:
        \[
            H_f(x_0, y_0) = \begin{matrix}
                f_{xx}(x_0,y_0) \;\;\; & f_{xy}(x_0,y_0)\\
                f_yx(x_0,y_0) \;\;\; & f_{yy}(x_0,y_0)\\
            \end{matrix}
        \]
    \item se $det(H_f(x_0, y_0)) > 0$ e
        \begin{itemize}
            \item $f_{xx}(x_0,y_0)>0$ allora $(x_0, y_0)$ è di minimo locale forte;
            \item $f_{xx}(x_0,y_0)<0$ allora $(x_0, y_0)$ è di massimo locale forte;
        \end{itemize}
        (si noti che in questo caso $f_{xx}(x_0,y_0)$ e $f_{yy}(x_0,y_0)$ hanno lo stesso segno).
    \item se $det(H_f(x_0, y_0)) < 0$ allora $(x_0, y_0)$ è punto di sella;
    \item se $det(H_f(x_0, y_0)) = 0$ occore un analisi ulteriore. 
\end{enumerate}
\subsection*{Funzioni convesse di n variabili}
\subsubsection*{Generalità sulle funzioni convesse}
Un insieme $\Omega \subseteq \mathbb{R}^n$ si dice convesso se per ogni coppia di punti $x_1, x_2 \in \Omega$ si ha che $[x_1,x_2]\subseteq \Omega$ (dove col simbolo $[x_1,x_2]$ si denota il segmento con estremi $x_1,x_2$); si dice strettamente convesso se per ogni coppia di punti $x_1,x_2 \in \Omega$ il segmento $(x_1,x_2)$ privato degli estremi è strettamente contenuto in $\Omega$.\newline

Si dice epigrafico di una funzione $f \;\;:\;\; \Omega \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ l'insieme
\[
    epi(f)=\{(x,z) \in \mathbb{R}^{n+1} \;\;:\;\; z\geq f(x), x \in\Omega\}
\]
\newline

Si dice che una funzione è convessa se $epi(f)$ è un sottoinsieme convesso, si dice che una funzione è concava se $-f$ è convessa.\newline
Formalmente si dice che una funzione è convessa se e solo se per ogni $x_1,x_2$, $t \in [0,1]$ vale la condizione
\[
    f(tx_2 + (1-t)x_1) \leq tf(x_2) + (1-t)f(x_1).
\]
Si noti che $tx_2 + (1-t)x_1$ percorre il segmento $[x_1,x_2]$ al variare di $t \in[0,1]$\newline

Se $f$ è convessa allora:
\begin{itemize}
    \item $f$ è continua;
    \item $f$ ha derivate parziali destre e sinistre in ogni punto;
    \item nei punti in cui è derivabile, $f$ è anche divverenziabile.
\end{itemize}

Teorema di convessità e piano tangente. Sia $f \;\;:\;\; \Omega \rightarrow \mathbb{R}$ differenziabile in $\Omega$. Allora $f$ è convessa in $\Omega$ se e solo se per ogni coppia di punti $x_0, x \in \Omega$ si ha:
\[
    f(x) \geq f(x_0) + \nabla f(x_0) \cdot (x-x_0).
\]
In due dimensioni:
\[
    f(x,y) \geq f(x_0,y_0) + \frac{\delta f}{\delta x}(x_0,y_0)(x-x_0)+\frac{\delta f}{\delta y}(x_0, y_0)(y-y_0).
\]
che geometricamente signifca che il paino tangente in $x-0, y_0$ sta sotto $f$.\newline

Teorema di convessità e matrice Hessiana. Sia $f \in C^2(\Omega)$, con $\Omega$ aperto convesso in $\mathbb{R}^n$. Se per ogni $x_0$ in $\Omega$ la forma quadratica $d^2f(x_0)$ è semidefinita positiva, allora $f$ è convessa in $\Omega$.
\subsubsection*{Ottimizzazione di funzioni convesse e concave}
Nelle funzioni convesse (concave) i punti stazionari, se esistono, rappresentano minimi (massimi) globali. Inoltre se la funzione è strettamente convessa (concava), il punto critico è di minimo (massimo) globale forte, quindi, in particolare, è unico.
\subsection*{Funzioni definite implicitamente}
\subsubsection*{Funzione implicita di una variabile}
