\section*{4- Calcolo differenziale per funzioni di più variabili a valori vettoriali}
\rule{\textwidth}{2pt}
\subsection*{Funzioni di più variabili a valori vettoriali: generalità}
\[
    \vec{f}: A\subseteq \mathbb{R}^n \rightarrow  \mathbb{R}^m
\]
Mostriamo alcuni esempi di oggetti rappresentabili tramite funzioni di più variabili a valori vettoriali.\newline
\rule{\textwidth}{0,4pt}
\subsubsection*{Superfici in forma parametrica}
Le superfici in forma parametrica, sono solo un caso particolare del concetto più asatratto e generale di varietà $k$-dimensionale in forma parametrica in $\mathbb{R}^m$. (Più avanti sarà discusso meglio).\newline
\rule{\textwidth}{0,4pt}
\subsubsection*{Trasformazioni di coordinate}
\[
    \vec{f}: A\subseteq \mathbb{R}^n \rightarrow  \mathbb{R}^n
\]
\textbf{Coordinate polari nel piano.} Il punto $(x,y) \in \mathbb{R}^2$ può essere anche individuato in forma polare ed è molto comodo se si è in presenza di simmetrie rispetto all'origine. Ricordando che la distanza dall'origine è $\rho = \sqrt{x^2+ y^2}$ e $\theta$ è l'angolo formato tra il vettore $(x,y)$ e l'asse $x$, otteniamo:
\[
    \begin{cases}
        x = \rho cos(\theta)\\
        y = \rho sin(\theta)
    \end{cases} \;\;\;\;\text{con} \; \rho \in[0,+\infty), \theta \in [0,2\pi) \;\text{o qualunque intervallo di ampiezza} \; 2\pi
\]
Questa trasformazione di coordinate si può vedere come una funzione $\vec{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^2$, $(x,y) = f(\rho, \theta)$.\newline
\newline
\textbf{Coordinate cilindriche nello spazio.} Viene utilizzato per descrivere insiemi e funzioni che hanno simetrie rispetto all'asse delle $z$ in $\mathbb{R}^3$ e si ha:
\[
    \begin{cases}
        x = \rho cos(\theta)\\
        y = \rho sin(\theta)\\
        z = t
    \end{cases} \;\;\;\;\text{con}\; \rho \in [0, +\infty), \theta \in [0, 2\pi), t \in \mathbb{R}
\]
Questa trasformazione si può vedere come una funzione $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3$.\newline
\newline
\textbf{Coordinate sferiche nello spazio.} Viene utilizzato per descrivere insiemi e funzioni che hanno simmetrie rispetto all'origine in $\mathbb{R}^3$ e si ha:
\[
    \begin{cases}
        x = \rho sin(\phi)cos(\theta)\\
        y =\rho sin(\phi)sin(\theta)\\
        z = \rho cos(\phi)
    \end{cases} \;\;\;\;\text{con}\; \rho > 0, \ \in[0,\pi], \theta \in[0,2\pi)
\]
Queste trasformazioni si possono vedere come funzioni $f: \mathbb{R}^2 \rightarrow \mathbb{R}^3$. \newline
Da notare che se si ha $\rho$ costante, il sistema rappresenta la superficie di una sfera di raggio $\rho$ in forma parametrica.\newline
\rule{\textwidth}{0,4pt}
\subsubsection*{Campi vettoriali}
Sono solo esempi ed esercizi.\newline
\rule{\textwidth}{2pt}
\subsection*{Limiti, continuità e differenziabilità per funzioni $\vec{f}:\mathbb{R}^n \rightarrow \mathbb{R}^m$}
Se $\vec{f} : A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}^m$, possiamo scrivere
\[
    \vec{f}(x) = (f_1(x), \dots,f_m(x))
\]
dove le $f_i$ sono componenti di $\vec{f}$ e sono funzioni reali di più variabili.\newline
\newline
Il limite si può calcolare componente per componente
\[
    \lim_{x\rightarrow x_0} \vec{f}(x) = (\lim_{x\rightarrow x_0}f_1(x), \dots,\lim_{x\rightarrow x_0}f_m(x))
\]
\newline
Una funzione $\vec{f} : A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}^m$ è continua se e solo se lo sono tutte le sue componenti.\newline
\newline
Diremo che $\vec{f} : A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}^m$ è differenziabile in $x_0$ se tutte le sue componenti lo sono.\newline
\newline
\begin{tcolorbox}
\textbf{Matrice Jacobiana} di $\vec{f}$:
\[
    D \vec{f}(x_0) = \left(\begin{matrix}
        \frac{\delta f_1}{\delta x_1} \;\; & \frac{\delta f_1}{\delta x_2} \;\; & \dots \;\; & \frac{\delta f_1}{\delta x_n}\\
        \frac{\delta f_2}{\delta x_1} & \dots & \dots & \frac{\delta f_2}{\delta x_n}\\
        \dots &\dots&\dots&\dots\\
        \frac{\delta f_m}{\delta x_1} & \dots & \dots & \frac{\delta f_m}{\delta x_n}\\
    \end{matrix}\right) (x_0)
\]
Da notare è che la colonna i-esima rappresenta il vettore delle derivate di tutte le funzioni di $\vec{f}$ derivate rispetto al i-esimo parametro. Indicheremo questo vettore come $\vec{f}_{x_i}$ 
\end{tcolorbox}
\textbf{teor.} Condizione sufficiente affinchè una funzione $\vec{f}: A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$, con $A$ aperto, risulti differenziabile in $A$ è che tutti gli elementi della sua matrice Jacobiana siano funzioni continue in $A$.\newline
\newline
Se $\vec{f}: A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$ è differenziabile, allora è derivabile e continua.\newline
\newline
\textbf{teor.} Siano $\vec{f}: A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^m$, $\vec{g}: B \subseteq \mathbb{R}^m \rightarrow \mathbb{R}^k$ e supponiamo che sia ben definita almeno in un intorno $C$ di $x_0 \in A$ la funzione composta $\vec{g} \circ \vec{f}: C \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^k$. Se $\vec{f}$ è differenziabile in $x_0$ e $\vec{g}$ è differenziabile in $y_0 = \vec{f}(x_0)$, anche $\vec{g} \circ \vec{f}$ è differenziabile in $x_0$ e la sua matrice jacobiana si ottiene come prodotto (matriciale) delle matrici Jacobiane di $\vec{f}$ e $\vec{g}$, calcolate nei punti $x_0$ e $\vec{f}(x_0)$.
\[
    D(\vec{g} \circ \vec{f})(x_0) = D \vec{g}(\vec{f}(x_0))D \vec{f}(x_0)
\]
\rule{\textwidth}{2pt}
\subsection*{Superfici regolari in forma parametrica}
Una superficie in forma parametrica è una funzione del tipo:
\[
    \vec{r}:A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}^3
\]
con $\vec{r} = (x,y,z)$ e
\[
    \begin{cases}
        x=x(u,v) \\
        y=y(u,v)\\
        z=z(u,v)
    \end{cases} \;\;\;\;(u,v) \in A
\] 
\begin{tcolorbox}
Valutare se una superficie in forma parametrica è regolare:\newline
\textbf{def.} Una superficie parametrizzata da $\vec{r} = \vec{r}(u,v)$, con $\vec{r}: A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}^3$ si dice regolare se $\vec{r}$ è differenziabile in $A$ e inoltre la matrice Jacobiana di $\vec{r}$ ha rango due in ogni punto di $A$. Se in qualche punto di $A$ le condizioni vengono violate, chiameremo punti singolari della superficie i punti corrispondenti.\newline
Le condizioni possono essere verificate tramite l'esistenza e il non annullamento di:
\[
    \vec{r}_u(u_0, v_0) \;\text{x}\; \vec{r}_v(u_0, v_0) = det \left(\begin{matrix}
        \vec{i} & \vec{j} & \vec{k}\\
        \\
        \frac{\delta x}{\delta u}(u_0,v_0) & \frac{\delta y}{\delta u}(u_0,v_0) & \frac{\delta z}{\delta u}(u_0,v_0)\\
        \\
        \frac{\delta x}{\delta v}(u_0,v_0) & \frac{\delta y}{\delta v}(u_0,v_0) & \frac{\delta z}{\delta v}(u_0,v_0)
    \end{matrix}\right) \neq 0
\]
con
\[
    \vec{r}_u(u_0, v_0) = ( \frac{\delta x}{\delta u}(u_0,v_0) , \frac{\delta y}{\delta u}(u_0,v_0) , \frac{\delta z}{\delta u}(u_0,v_0) )
\]
\[
    \vec{r}_v(u_0, v_0) = ( \frac{\delta x}{\delta v}(u_0,v_0) , \frac{\delta y}{\delta v}(u_0,v_0) , \frac{\delta z}{\delta v}(u_0,v_0) )
\]
\end{tcolorbox}
Una proprietà di quest'ultimo prodotto vettoriale è che è normale alla superficie nel punto in cui è calcolato. 
\begin{tcolorbox}
    Il versore normale è:
    \[
        \vec{n} = \frac{\vec{r}_u \;\text{x}\; \vec{r}_v}{|\vec{r}_u \;\text{x}\; \vec{r}_v|}
    \]
\end{tcolorbox}
[Equazione del piano tangente (manca).]
\subsubsection*{Superfici cartesiane (grafico di funzioni di due variabili)}
\begin{tcolorbox}
Il grafico di $z = f(x,y)$ è una superficie che si può riscrivere nella forma:
\[
    \begin{cases}
        x= u\\
        y=v\\
        z = f(u,v)
    \end{cases}
\]
Matrice Jacobiana per superfici cartesiane:
\[
    \left( \begin{matrix}
        1 & 0 & f_u(u_0,v_0) \\
        0 & 1 & f_v(u_0,v_0) 
    \end{matrix} \right)
\]
\[
    det\left( \begin{matrix}
        \vec{i} & \vec{j} & \vec{k} \\
        1 & 0 & f_u(u_0,v_0) \\
        0 & 1 & f_v(u_0,v_0) 
    \end{matrix} \right)= -\vec{i}f_u -\vec{j} f_v + \vec{k}
\]
Versore normale per superfici cartesiane:
\[
    \vec{n} =\frac{-\vec{i}f_u -\vec{j} f_u + \vec{k}}{\sqrt{1 + |\nabla f|^2}}
\]
Se $f$ è differenziabile in $A$, allora il suo grafico è sempre una superficie regolare.\newline
\newline
Elemento d'area:
\[
    dS = \sqrt{1 + |\nabla f|^2} du dv
\]
\end{tcolorbox}
\subsubsection*{Superfici di rotazione}
\begin{tcolorbox}
Superfici ottenute facendo ruotare una curva $\gamma$ detta generatrice attorno a un asse.\newline
In un riferimento $(x,y,a)$ sia $y$ l'asse a cui vogliamo far ruotare una curva $\gamma$, inizialmente assegnata al piano $x, z$ in forma parametrica:
\[
    \begin{cases}
        x = a(t)\\
        y = b(t)
    \end{cases} \quad t \in I
\]
La superficie che si ottiene per rotazione è
\[
    \begin{cases}
        x= a(t) cos(\theta)\\
        y = b(t)\\
        z= b(t) sin(\theta)
    \end{cases} \quad t \in I, \theta \in[0,2\pi]
\]
(questo sistema va adattato di caso in caso, badando principalmente a due fatti: che la curva da far ruotare deve essere tutta da una parte (a destra o a sinistra) dell'asse su cui si ruota e che l'asse su cui si ruota la funzione determina come viene scritto il sistema, per esempio, sopra, siccome volevamo far ruoate attorno a $y$, allora non abbiamo aggiunto seni o coseni alla coordinata $y$).\newline
Matrice Jacobiana di superfici di rotazione:
\[
    \begin{matrix}
        a'(t)cos(\theta) & b'(t) & a'(t)sin(\theta)\\
        -a(t)sin(\theta) & 0 & a(t) cos(\theta) 
    \end{matrix}
\]
Elemento d'area di superfici di rotazione:
\[
    dS = |a(t)|\sqrt{a'(t)^2 + b'(t)^2} dt d \theta
\]
Versore normale di superfici di rotazione:
\[
    \vec{n} = \frac{ (b'(t)cos(\theta), -a'(t), b'(t)sin(\theta))}{\sqrt{a'(t)^2 + b'(t)^2}}
\]
\end{tcolorbox}
\rule{\textwidth}{2pt}
\subsection*{Varietà k-dimenzionali in $\mathbb{R}^n$ e funzioni definite implicitamente}
\rule{\textwidth}{0,4pt}
\subsubsection*{Varietà k-dimensionali in $\mathbb{R}^n$ in forma parametrica}
\textbf{def.} Si dice varietà regolare k-dimensionale in forma parametrica, immersa in $\mathbb{R}^n$ (con $1 \leq k \leq n-1$), una funzione $\vec{r} : \Omega \rightarrow \mathbb{R}^n$, con $\Omega$ aperto in $\mathbb{R}^k$, tale che $\vec{r} \in C^1(\Omega)$ e la matriche Jacobiana di $\vec{r}$ ha rango $k$ in ogni punto di $\Omega$.\newline
Nel caso particolare in cui $k = n-1$ (e $n > 3$) la varietà si dice ipersuperficie in $\mathbb{R}^n$.\newline
Dal punto di visa dell'intuizione geometrica, la varietà k-dimensionale è l'immagine della funzione $\vec{r}$ in $\mathbb{R}^n$\newline
\rule{\textwidth}{0,4pt}
\subsubsection*{Funzioni implicite definite da sistemi di equazioni}
Avendo un sistema di $m$ equazioni in $n+m$ incognite, il teorema di Rouchè-Capelli ci dice che se la matrice del sistema ha rango massimo (cioè $m$) si riescono a esplicitare $m$ variabili in funzione di altre $n$. \newline
Nel caso non lineare ci si limita a verificare il teorema di Rouchè-Capelli localmente: nell'intorno di un punto in cui si sa che il sistema è soddisfatto, il rango massimo della matrice del sistema lineare diventa il rango massimo della matrice del sistema linearizzato, cioè della matrice Jacobiana del sistema non lineare.\newline
\newline
\textbf{notazioni:}\newline
Scriviamo un sistema di $m$ equazioni in $n+m$ variabili nella seguente forma vettoriale
\[
    \vec{f}(x, y) = 0
\]
dove $\vec{f}: A \subset \mathbb{R}^{n+m} \rightarrow \mathbb{R}^m$, $\vec{x} \in \mathbb{R}^n$, $\vec{y} \in \mathbb{R}^m$.\newline
Denotiamo con $D_y \vec{f}(x_0, y_0)$ la matrice Jacobiana della funzione $y \rightarrow \vec{f}(x_0, y)$, calcolata nel punto $y_0$, ossia la matrice di $m \; \text{x}\; m$ elementi $\frac{\delta f_i}{\delta y_j}(x_0, y_0)$, con $(i,j = 1, 2, \dots, m)$ (con $D_x \vec{f(x_0, y_0)}$ denotiamo l'analogo per le $x$).\newline
\newline
\textbf{teor.} Teorema di Dini, della funzione implicita: caso generale. \newline
Sia $A$ un aperto di $\mathbb{R}^{n+m}$, $\vec{f}: A \rightarrow \mathbb{R}^m$, $\vec{f} \in C^1(A)$, e supponiamo che nel punto $(x_0, y_0) \in A$ sia
\[
    \vec{f}(x_0, y_0) = 0 ; \;\;\;\;\;\;\;\;det(D_y \vec{f}(x_0, y_0)) \neq 0
\]
Allora esiste un intorno $U \subset \mathbb{R}^n$ di $x_0$ e un unica funzione $\vec{g} : U \rightarrow \mathbb{R}^m$, $g \in C^1(U)$, tale che, per ogni $x \in U$,
\[
    \vec{f} (x,\vec{g}(x)) = 0
\]
\[
    Dg(x) =-D_y \vec{f}(x,\vec{g}(x))^{-1}D_x \vec{f} (x, \vec{g}(x))
\]
\rule{\textwidth}{0,4pt}
\subsubsection*{Varietà k-dimensionali in $\mathbb{R}^n$ in forma implicita}
\textbf{def.} Si dice varietà k-dimensionale in forma implicita, immersa in $\mathbb{R}^n$, un sottoinieme (non vuoto) di $\mathbb{R}^n$ del tipo:
\[
    M = \{x \in \Omega : \vec{f}(x) = 0\}
\]
dove $\Omega$ è un aperto di $\mathbb{R}^n$, $\vec{f}: \Omega \rightarrow \mathbb{R}^{n-k}$, $\vec{f} \in C^1(\Omega)$ e il rango di $D \vec{f}$ è uguale a $n-k$ in ogni punto di $\Omega$. La varietà si dirà di classe $C^m$ (per qualche intero $m \geq 1$) se, inoltre, $\vec{f} \in C^m (\Omega)$; si dirà di classe $C^{\infty}$ se è di classe $C^m$ per ogni intero $m$.\newline
\newline
Si noti che se $k = n-1$ la funzione $f$ ha valori reali; in questo caso $M$ è definita da un'unica equazione in $n$ variabili, si chiamerà ipersuperficie in forma cartesiana implicita. Nel caso $n=3$ si ha semplicemente una superficie in forma cartesiana implicita (es. la sfera: $x^2 + y^2 + z^2 - R^2= 0$).\newline
\newline
Se $n=3$, $k=1$, abbiamo due equazioni in tre variabili, cioè una curva in forma cartesiana implicità (vista come intersezione di due superfici).\newline
\rule{\textwidth}{2pt}
\subsection*{Trasformazioni di coordinate e loro inversione}
\rule{\textwidth}{0,4pt}
\subsubsection*{Il teorema della funzione inversa}
Una funzione lineare $\vec{f}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ nella forma
\[
    \vec{f}(\vec{x}) =A \vec{x}
\]
per una certa matrice $A$, $n \text{x} n$, risulta invertibile se e solo se $det(A)\neq 0$ (teorema di Cramer).\newline
\newline
Per una funzione non lineare $\vec{f}: A \rightarrow \mathbb{R}^n$, tale che $\vec{f}\in C^1(A)$, per qualche aperto $A \subseteq \mathbb{R}^n$ non possiamo usare lo stesso risultato, ma possiamo dire che è invertibile in un intorno di $x_0$, quando la matrice Jacobiana (approssimazione lineare della funzione non lineare) calcolata in $x_0$ è invertibile, $\vec{f}$ risulta invertibile almeno in un intorno di $x_0$.\newline
\newline
\textbf{def.} Una funzione $\vec{f}: A \rightarrow B$ ($A,B$ aperti in $\mathbb{R}^n$) si dice invertibile in $A$ se è iniettiva, ossia se per ogni $x_1, x_2 \in A$
\[
    f(x_1) = f(x_2) \Rightarrow x_1 = x_2
\]
$f$ si dice suriettiva su $B$ se per ogni $y \in B$ esiste $x \in A$ tale che $\vec{f} (x) = y$.\newline
$f$ si dice biiettiva se è iniettiva e suriettiva.\newline
\newline
\textbf{teor.} Teorema di inversione locale. Sia $f: A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}^n$ con $A$ aperto, tale che $\vec{f} \in C^1(A)$. Supponiamo che per un dato punto $x_0 \in A$ sia
\[
    det(Df(x_0)) \neq 0
\]
Allora esistono un intorno $U$ di $x_0$ e un intorno $V$ di $\vec{f}(x_0)$ tra i quali  la funzione $\vec{f}$ è biunivoca; inoltre, detta $g: V \rightarrow U$ la corrispondenza inversa, si ha che $\vec{g}\in C^1(V)$ e 
\[
    D \vec{g}(\vec{f}(x)) = D \vec{f}(x)^{-1} \;\;\; \text{per ogni} \;x \in U
\]
Bisogna far attenzione al fatto che si sta parlando di invertibilità locale e non globale, tutto ciò che si può dire è che c'è un intorno in cui la funzione è invertibile.\newline
\newline
\textbf{Coordinate polari nel piano}\newline
La trasformazione
\[
    \begin{cases}
        x = \rho cos(\theta)\\
        y = \rho sin(\theta)
    \end{cases}
\]
ha matrice Jacobiana
\[
    \left(\begin{matrix}
        cos(\theta) \;\; & -\rho sin(\theta)\\
        sin(\theta) & \rho cos(\theta) 
    \end{matrix}\right)
\]
con determinante $\rho$. La trasformazione è regolare ad eccezione di $\rho = 0$ (origine del piano).\newline
\newline
\textbf{Coordinate cilindriche nello spazio} \newline
La trasformazione
\[
    \begin{cases}
        x= \rho cos(\theta)\\
        y =\rho sin(\theta)\\
        z = t
    \end{cases}
\]
ha matrice Jacobiana
\[
    \left(\begin{matrix}
        cos(\theta) \;\; & -\rho sin(\theta) \;\; & 0\\
        sin(\theta) & \rho cos(\theta) & 0\\
        0 & 0 &1
    \end{matrix}\right)
\]
con determinante $\rho$. La trasformazione è regolare ad eccezione di $\rho = 0$ (l'asse $z$).\newline
\newline
\textbf{Coordinate sferiche nello spazio}\newline
La trasformazione
\[
    \begin{cases}
        x= \rho sin(\phi) cos(\theta)\\
        y= \rho sin(\phi) cos(\theta)\\
        z = \rho cos(\phi)
    \end{cases}
\]
ha matrice Jacobiana
\[
    \left(\begin{matrix}
        sin(phi) cos(\theta) \;\; & \rho cos(phi) cos(\theta) \;\; & -\rho sin(\phi)sin(\theta)\\
        sin(\phi) sin(\theta) & \rho cos(\phi) sin(\theta) & \rho sin(\phi) cos(\theta)\\
        cos(\phi) & -\rho sin(\phi) & 0
    \end{matrix}\right)
\]
con determinante $\rho^2 sin(\phi)$. La trasformazione è regolare tranne che per $\rho = 0$ (origine dello spazio) e $\phi = 0, \pi$ (i due semiassi $z$ nello spazio). In conclusione sono singolari tutti i punti dell'asse $z$. \newline
\rule{\textwidth}{0,4pt}
\subsubsection*{Trasformazione di operatori differenziali}
\begin{tcolorbox}
Sia
\[
    \vec{T}(x,y) = (a(x,y), b(x,y))
\]
una trasformazione regolare $\vec{T}: \mathbb{R}^2 \rightarrow \mathbb{R}^2$.\newline
Sia ora $f(u,v) : \mathbb{R}^2 \rightarrow \mathbb{R}$ una funzione differenziabile. Posto $(u,v) = \vec{T}(x,y)$ definiamo 
\[
    g(x,y) = f(\vec{T}(x,y))
\] 
Sfruttando il teorema della differenziabilità della funzione composta, possiamo scrivere mediante le derivate di $f$ 
\[
    \frac{\delta g}{\delta x} (x,y) = \frac{\delta f}{\delta x}(\vec{T}(x,y)) \cdot \frac{\delta T_1}{\delta x}(x,y) + \frac{\delta f}{\delta y} (\vec{T}(x,y)) \cdot \frac{\delta T_2}{\delta x}(x,y)
\]
dove $T_1$ e $T_2$ rappresentano la prima ($a(x,y)$) e la seconda ($b(x,y)$) componente di $\vec{T}$. Negli esercizi le derivate parziali rimangono indicate come in formula senza essere esplcitate ($f$ non viene definita nelle consegne), mentre le derivate con $T_1$ e $T_2$ si possono risolvere. Dunque la formula effettivamente diventa:
\[
    \frac{\delta g}{\delta x} (x,y) = \frac{\delta f}{\delta x}(a(x,y), b(x,y)) \cdot \frac{\delta a(x,y)}{\delta x} + \frac{\delta f}{\delta y} (a(x,y), b(x,y)) \cdot \frac{\delta b(x,y)}{\delta x}
\]
Analogamente l'altra derivata di $g$ è:
\[
    \frac{\delta g}{\delta y}(x,y)= = \frac{\delta f}{\delta x}(\vec{T}(x,y)) \cdot \frac{\delta T_1}{\delta y}(x,y) + \frac{\delta f}{\delta y} (\vec{T}(x,y)) \cdot \frac{\delta T_2}{\delta y}(x,y)
\]
Negli esercizi sul libro si fanno anche derivate di secondo grado, ma sinceramente non c'ho capito molto, quindi buona fortuna... chiedi appunti sull'argomento a qualcuno.
\end{tcolorbox}
Lascio scritte anche le informazioni trovate sul libro di teoria, anche se non sono molto chiare...\newline
Sia 
\[
    \begin{cases}
        x = g(u,v)\\
        y=h(u,v)
    \end{cases}
\]
una trasformazione regolare di coordinate nel piano e sia $f(x,y)$ una funzione differenziabile. Sotto l'azione della trasformazione di coordinate, $f$ diventerà funzione di $u$ e $v$ e per il teorema sul differenziale di una funzione composta, si può scrivere:
\[
    \frac{\delta f}{\delta u} = \frac{\delta f}{\delta x} \frac{\delta g}{\delta u} + \frac{\delta f}{\delta y} \frac{\delta h}{\delta u}
\]
\[
    \frac{\delta f}{\delta v} = \frac{\delta f}{\delta x} \frac{\delta g}{\delta v} + \frac{\delta f}{\delta y} \frac{\delta h}{\delta v}
\]
\rule{\textwidth}{2pt}
\subsection*{Ottimizzazione. Estremi vincolanti}
\begin{tcolorbox}
    PRESENTE NEL PROGRAMMA DEL CORSO, NON HO TROVATO ESERCIZI SUL LIBRO.
\end{tcolorbox}
\rule{\textwidth}{0,4pt}
\subsubsection*{Vincoli di uguaglianza e moltiplicatori di Lagrange. Funzioni di due variabili}
\textbf{Problema:} Date due funzioni $f= f(x,y)$ e $g = g(x,y)$, dotate di derivate parziali continue in $\mathbb{R}^2$. Si vogliono determinare gli estremi di $f$ sotto la condizione di vincolo $g(x,y) = b$, $b \in \mathbb{R}^2$.\newline
A seconda che sia un caso di massimo o di minimo vincolato scriveremo:
\[
    \begin{cases}
        max \; f\\
        sub\\
        g(x,y) = b
    \end{cases}\quad \quad \begin{cases}
        min \; f \\
        sub\\
        g(x,y) = b
    \end{cases}
\]
Il problema consiste nel massimizzare (minimizzare) la restrizion di $f$ al vincolo specificato.\newline
\begin{itemize}
    \item vincolo esplicitabile: caso in cui il vincolo $g(x,y) = b$ definisce esplicitamente $y=y(x)$ o $x = x(y)$ oppure, più in generale, definisce le equazioni parametriche ($x= x(t), y = y(t)$) di una curva $\gamma$. Il problema allora è ricondotto alla ricerca degli estremi di una funzione reale di variabili reali
    \[
        \phi(t) = f(x(t), y(t))
    \]
    \item metodo dei moltiplicatori di Lagrange: utilizzato quando il vincolo è rappresentato da una curva regolare assegnata in qualsiasi forma (parametrica, cartesiana o implicita).\newline
    \textbf{teor.} Teorema dei moltiplicatori di lagrange. Siano $f,g \in C^1(\mathbb{R}^2)$ e $(x^*, y^*)$ punto di estremo vincolato per $f$ sotto il vincolo:
    \[
        g(x,y) = b
    \]
    Se $(x^*, y^*)$ è regolare per il vincolo, cioè $\nabla g(x^*, y^*) \neq (0,0)$, allora esiste $\lambda^* \in \mathbb{R}$ (detto moltiplicatore di Lagrange) tale che:
    \[
        \nabla f(x^*, y^*) = \lambda \nabla g(x^*, y^*)
    \]
    Questa formula esprime il fatto che se $(x^*, y^*)$, verifica le ipotesi del teorema, allora la derivata di $f$ lungo la tangente al vincolo si deve annullare e in tal caso diciamo che $(x^*, y^*)$ è punto critico vincolato.\newline
    \textbf{oss.} Introducendo la funzione $L = L (x, y , \lambda)$, detta Lagrangiana, definita da
    \[
        L(x,y,\lambda) = f(x,y) - \lambda[g(x,y) - b]
    \] 
    il teorema afferma che se $(x^*, y^*)$ è punto di estremo vincolato, allora esiste $\lambda^*$ tale che il punto $(x^*, y^*, \lambda^*)$ sia punto critico libero per $L$. Infatti i punti critici in $L$ sono soluzioni del sistema:
    \[
        \begin{cases}
            L_x = f_x - \lambda g_x  = 0\\
            L_y = f_y - \lambda g_y = 0\\
            L_\lambda = b-g = 0
        \end{cases}
    \]
    Metodo dei moltiplicatori di Lagrange:
    \begin{itemize}
        \item Si isolano gli eventuali punti non regolari dell'insieme $g(x,y) = b$, che vanno esaminati a parte.
        \item si cercano i punti critici liberi della Lagrangiana e cioè soluzioni del sistema visto nell'osservazione.
        \item si determina la natura dei punti critici. A questo proposito risulta spesso utile il teorema di Weierstrass.
    \end{itemize}
\end{itemize}
\rule{\textwidth}{0,4pt}
\subsubsection*{Moltiplicatore di Lagrange. Il caso generale}
Consideriamo ora problemi di ottimizzazione vincolata, per funzioni di $n$ variabili ($n \geq 2$) con un numero $m$ di vincoli di uguaglianza.\newline
Siano $m+1$ funzioni reali di $n$ variabili $f,g_1,g_2,\dots,g_m : \mathbb{R}^n \rightarrow \mathbb{R}$ tutte $C^1(\mathbb{R}^n)$. Si vogliono determinare gli estremi di $f$ quando le variabili sono soggette alle $m$ condizioni di vincolo:
\[
    \begin{cases}
        g_1(x_1,\dots,x_n) = b_1\\
        \dots\\
        g_m(x_1,\dots,x_n) = b_m
    \end{cases}
\]
\textbf{teor.} Teorema dei moltiplicatori di Lagrange, caso generale.\newline
Sia $f, g_1, \dots, g_m \in C^1(\mathbb{R}^n)$, $\vec{g} = (g_1, g_2, \dots, g_m)$ e sia $\vec{x}^*$ punto di estremo vincolato per $f$ rispetto al vincolo
\[
    \vec{g}(\vec{x}) = \vec{b}
\] 
Se $\vec{x}^*$ è un punto regolare per $\vec{g}$, cioè se il rango di $D \vec{g}(\vec{x}^*)$ è $m$, allora esistono $m$ numeri reali $\lambda_1^*, \dots, \lambda_m^*$, detti moltiplicatori di Lagrange, tali che
\[
    \nabla f(\vec{x}^*) = \sum_{j=1}^{m}\lambda_j^* \nabla g_j (\vec{x}^*)
\]
\newline
Introduciamo la funzione Lagrangiana
\[
    L(x_1, \dots,x_n, \lambda_1, \dots \lambda_m) =f(x_1, \dots, x_n) - \sum_{j=1}^{m} \lambda_j[g_j(x_1,\dots,x_n) - b_j]
\]
dipendente da $n+m$ variabili. Se $\vec{x}^*$ è un punto regolare, allora esiste un vettore di moltiplicatori
\[
    \vec{\lambda}^* =(\lambda_1^*, \dots, \lambda_m^*) 
\]
tale che $(\vec{x}^*, \vec{\lambda}^*)$ è un punto critico libero per la Lagrangiana. Infatti i punti critici liberi per la Lagrangiana si trovano risolvendo il seguente sistema di $n+m$ equazioni in $n+m$ incognite:
\[
    \begin{cases}
        D_{x_1} L = D_{x_1}f - \sum_{j=1}^{m} \lambda_j D_{x_1}g_j = 0\\
        \dots\\
        D_{x_n} L = D_{x_n}f - \sum_{j=1}^{m} \lambda_j D_{x_n} g_j = 0\\
        D_{\lambda_1}L = b_1 - g_1 = 0\\
        \dots\\
        D_{\lambda_m}L = b_m - g_m= 0
    \end{cases}
\]
\rule{\textwidth}{2pt}
\subsection*{Note sugli esercizi}
\begin{tcolorbox}
\textbf{elemento d'area e regolarità}
\[
    dS = |\vec{r}_t \; \text{x}\; \vec{r}_u| dt du
\]
è una scrittura comoda e sintetica che mette in luce la regolarità o meno della superficie (dove $|\vec{r}_t \; \text{x}\; \vec{r}_u|$ si annulla abbiamo punti singolari (vedi superfici regolari in forma parametrica per saperne di più).\newline
\newline
L'elemento d'area per \textbf{trasformazioni} in $\mathbb{R}^2$ o $\mathbb{R}^3$ è calcolato come il modulo del determinante della matrice Jacobiana (che sarà 2x2 o 3x3).
\end{tcolorbox}
Ricordiamo che per $\vec{r}_t$ si intende il vettore delle derivate di tutte le funzioni che compongono $\vec{r}$ derivate rispetto al parametro $t$.\newline