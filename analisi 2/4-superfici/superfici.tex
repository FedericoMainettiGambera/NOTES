\section{Funzioni $\mathbb{R}^n \rightarrow \mathbb{R}$ ("Calcolo differenziale per funzioni reali di più variabili", "Funzioni reali di più variabili")}
\subsection{Topologia in $\mathbb{R}^n$ e proprietà delle funzioni continue}
Dato un punto $M(x_0,y_0) \in \mathbb{R}^2$ e dato $r > 0$ indicheremo con $B_r(M)$ il disco centrato in $M$ di raggio $r$:
\[
    B_r(X_0,y_0) = \{ (x,y) \in \mathbb{R}^2; (x-x_0)^2+ (y-y_0)^2 < r^2\}.
\]
Diremo anche che $B_r(x_0,y_0)$ è un \textbf{intorno} del punto $(x_0,y_0)$: tutti gli intorni del punto si ottengono facendo variare $r>0$. Indicheremo invece con $B_r^0(x_0,y_0)$ un \textbf{intorno bucato} di $(x_0,y_0)$ e cioè:
\[
    B_r^0 (x_0,y_0) = \{ (x,y) \in \mathbb{R}^2; 0 < (x-x_0)^2 + (y-y_0)^2 < r^2\}
\]
Nel caso $n$-dimensionale, dato un punto $M(x_1^0, \dots, x_n^0) \in \mathbb{R}^n$, si definisce
\[
    B_r(M) = \left\{ (x_1, \dots,x_n) \in \mathbb{R}^n; \sum_{i=1}^{n} (x_i-x_i^0)^2 < r^2 \right\}
\]
\ \newline
Sia $E$ un sottoinsieme di $\mathbb{R}^n$, un \textbf{punto} $x_0$ si dice:
\begin{itemize}
    \item \textbf{interno} ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E$;
    \item \textbf{esterno} ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E^c$;
    \item di \textbf{frontiera} per $E$, se ogni intorno centrato in $x_0$ contiene almeno un punto di $E$ e uno di $E^c$.
\end{itemize}
Un insieme $E \subseteq \mathbb{R}^n$ si dice:
\begin{itemize}
    \item \textbf{aperto}, se ogni suo punto è interno a $E$;
    \item \textbf{chiuso}, se il suo complementare è aperto.
\end{itemize} 
Sia $E$ un \textbf{insieme} di $\mathbb{R}^n$, si dice:
\begin{itemize}
    \item \textbf{interno} di $E$, e si indica con $E^o$, l'insieme dei punti interni di $E$;
    \item \textbf{frontiera} o \textbf{bordo} di $E$, e si indica con $\delta E$, l'insieme dei punti di frontiera di $E$;
    \item \textbf{chiusura} di $E$, e si indica con $\bar{E}$, l'insieme $E \cup \delta E$.
\end{itemize}
Alcune informazioni extra: 
\begin{itemize}
    \item si ha sempre $E^o \subseteq \delta E \subseteq \bar{E}$;
    \item il complementare di un aperto è chiuso e viceversa; 
    \item esistono insiemi nè aperti nè chiusi, gli unici insiemi sia aperti sia chiusi sono quello vuoto e $\mathbb{R}^n$;
    \item l'unione di una famiglia qualsiasi (anche infinita) di insiemi aperti e l'intersezione di un numero finito di insiemi aperti sono insiemi aperti 
    \item l'intersezione di una famiglia qualsiasi (anche infinita) di insiemi chiusi è l'unione di un numero finito di insiemi chiusi sono insiemi chiusi;
    \item un insieme aperto non contiene nessuno dei suoi punti di frontiera, un insieme chiuso contiene tutti i suoi punti di frontiera.
\end{itemize}
Un \textbf{insieme} si dice:
\begin{itemize}
    \item \textbf{limitato} se esiste un intorno che lo contiene tutto;
    \item \textbf{connesso} se per ogni coppia di punti dell'insieme, esiste un arco continuo che che li connette contenuto nell'insieme.
\end{itemize}
\ \newline
Estremi:\newline
Sia $\Omega \subset \mathbb{R}^n$ e sia $f_\Omega \rightarrow \mathbb{R}$. Diremo che $(\bar{x}, \bar{y}) \in \Omega$ è un punto di \textbf{massimo relativo} per $f$ se esiste $r > 0$ tale che $f(x,y) \leq f(\bar{x},\bar{y})$ per ogni $(x,y) \in B_r(\bar{x},\bar{y}) \cap \Omega$; Diremo che $(\bar{x},\bar{y}) \in \Omega$ è un punto di \textbf{minimo relativo} per $f$ se vale la disuguaglianza opposta. Diremo che $(\bar{x},\bar{y}) \in \Omega$ è punto di \textbf{massimo assoluto} per $f$ se $f(x,y)\leq f(\bar{x},\bar{y})$ per ogni $(x,y) \in \Omega$; diremo che $(\bar{x},\bar{y}) \in \Omega$ è punto di \textbf{minimo assoluto} per $f$ se vale la disuguaglianza opposta.
\subsection{Calcolo di limiti di funzioni in più variabili}
Sia $l \in \mathbb{R}$, diremo che
\[
    \lim_{(x,y) \rightarrow (x_0,y_0)} f(x,y) = l
\]
se 
\[
    \;\forall\; \epsilon > 0 \; \exists \; \delta > 0 \; \text{tale che}\; (x,y) \in B_\delta^0 (x_0,y_0) \Rightarrow |f(x,y)-l| < \epsilon 
\]
In $\mathbb{R}$ si parla di limite "destro" e "sinistro", in $\mathbb{R}^n$, per via della presenza dell'intorno bucato, ci sono infiniti modi di avvicinarsi al limite: il valore dipende dal cammina che si segue.
\subsubsection{Non esistenza del limite}
Per mostrare che un certa funzione in più varibili non ammette limite in un determinato punto, è sufficiente determinare due curve passasnti per il punto lungo le quali la funzione assume limiti diversi.\newline
\newline
\textbf{es.} 
\[
    \lim_{(x,y)\rightarrow (0,0)} \frac{xy}{x^2+y^2}
\]
Analiziamo la funzione lungo due curve:
\begin{itemize}
    \item con $y=x$ ottengo $f(x,x) = \frac{1}{2}$
    \item con $y=-x$ ottengo $f(x,-x) = - \frac{1}{2}$
\end{itemize}
non ammette limite.
\subsubsection{Uso di maggiorazioni con funzioni radiali per provare l'esistenza del limite}
Il trucco più efficace per calcolare i limiti (in $\mathbb{R}^2$) è quello di passare in \textbf{coordinate polari}.\newline
\newline
Per dimostrare l'esistenza di un limite per $(x,y) \rightarrow (0,0)$ (!), si impone $x=\rho \cdot  cos(\theta)$ e $y= \rho \cdot sin(\theta)$, successivamente si pone l'intera funzione sotto modulo e si fa il limite per $\rho \rightarrow  0$. Il segreto dta nell'usare semplificazioni e maggiorazioni per eliminare i seni e i coseni. E' essenziale che la funzione non dipenda da $\theta$, altrimenti il limite non esiste.\newline
\newline
Più in generale se si volesse calcolare il limite per $(x,y) \rightarrow (x_0, y_0)$ (con $(x_0, y_0)$ anche diverso dall'origine) si pongono $x=x_0 +\rho \cdot  cos(\theta)$ e $y= y_0 + \rho \cdot sin(\theta)$ e si procede come sopra.\newline
\newline
\subsubsection{Note sugli esercizi per il calcolo di limiti}
\begin{itemize}
    \item Se il limite non presenta una forma di indeterminazione allora il valore cercato si ricava sostituendo direttamente il punto nella funzione.
    \item Tecniche standard della maggiorazione: 
        \begin{itemize}
            \item disuguaglianza triangolare:
                \[
                    |a+b| \leq |a| + |b|
                \]
            \item maggiorazione di frazioni, con $a,b,c \geq 0$:
                \[
                    \frac{a}{b+c} \leq \frac{a}{b}
                \]
            \item maggiorazione di funzioni trigonometriche:
                \[
                    |cos(\theta)| \leq 1 \;\;,\;\;|sin(\theta)|\leq 1
                \]
        \end{itemize}
    \item Il criterio che ci permette di trovare il limite richiede di trovare una funzione maggiorante di $|f|$ che sia radiale (dipenda solo da $\rho$, non $\theta$) e infinitesima. Da notare è che è possibile semplificare la funzione anche senza passare subito in coordinate polari.
    \item Solitamente si suddivide la funzione in una serie di somme di funzioni e si studiano quest'ultime separatamente.
\end{itemize}
\subsection{Continuità di funzioni in più variabili}
Una funzione $f \;\;:\;\; \mathbb{R}^n \rightarrow  \mathbb{R}$ è \textbf{continua} in un punto $x_0$ se 
\[
    \lim_{(x,y)\rightarrow (x_0,y_0)} f(x,y) = f(x_0,y_0)
\]
La continuità di una funzione è anche deducibile dal fatto che sia costituita (somma/ prodotto/ quoziente/ certe volte anche composizione) da funzioni elementari continue.
\newline
\newline
\textbf{oss.} Per verificare la continuità di una funzione si usa spesso il concetto di differenziabilità (vedi sotto, "verifica della differenziabilità").
\newline
\newline
Proprietà delle funzioni continue:
\begin{itemize}
    \item \textbf{Teorema di Weierstrass}. Sia $E \subset \mathbb{R}^n$ un insieme chiuso e limitato e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua, allora $f$ ammette massimo e minimo in $E$, cioè esistono $x_m$ e $x_M$ tali che $f(x_m) \leq f(x) \leq f(x_M)$ per ogni $x \in E$.
    \item \textbf{Teorema degli zeri}. Sia $E$ un insieme connesso di $\mathbb{R}^n$ e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua. Se $x$, $y$ sono due punti di $E$ tali che $ f(x) < 0$ e $f(y) > 0$, allora esiste un terzo punto $z \in E$ in cui $f$ si annulla. In particolare, lungo ogni arco di curva continua contenuto in $E$ che congiunge $x$ e $y$, c'è almeno un punto in cui $f$ si annulla.
    \item \textbf{Teorema di permanenza del segno}. Se $f$ è continua in $(x_0, y_0) \in \mathbb{R}^2$ e $f(x_0, y_0) > 0$, allora esiste $\delta>0$ tale che $fx,y) > 0$ per ogni $(x,y) \in B_\delta(x_0,y_0)$.
\end{itemize}
\subsection{Calcolo differenziale di funzioni in più variabili}
\subsubsection{Derivate parziali}
Una funzione $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ si dice \textbf{derivabile} in $(x_0,y_0) \in \mathbb{R}^2$ se esistono finiti i due limiti
\[
    f_x(x_0,y_0) = \lim_{x\rightarrow x_0} \frac{f(x,y_0)- f(x_0,y_0)}{x-x_0}
\]
\[
    f_y(x_0,y_0) = \lim_{y\rightarrow y_0} \frac{f(x_0,y) - f(x_0,y_0)}{y-y_0}
\]
in tal caso, i numeri $f_x(x_0,y_0)$ e $f_y(x_0,y_0)$ si chiamato \textbf{derivate parziali} di $f$ in $(x_0,y_0)$.
\newline
\newline
Geometricamente, le derivate parziali rappresentano le pendenze che si hanno sul fianco della montagna quando si prendono le direzioni degli assi orientati.\newline
\newline
Una funzione $f: A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ si dice \textbf{derivabile} in un punto del suo dominio se in quel punto \textbf{esistono tutte le sue derivate parziali}; si dice derivabile in $A$ se è derivabile in ogni punto di $A$.
\newline
\newline
Se le derivate parziali, oltre ad esistere, sono continue nel punto $(x_0, y_0)$, diremo che la funzione $f$ è di classe $C^1$ in tale punto.\newline
\newline
Se $f$ è derivabile in un punto, chiameremo \textbf{gradiente} ( $\nabla f(x)$ ) il vettore delle sue derivate parziali.\newline
\newline
Diversamente dal caso $n=1$, in $n=2$ la \textbf{derivabilità non implica continuità}.\newline
\newline
\textbf{Proprietà per il calcolo delle derivate}:
\[
    \delta(\alpha \cdot f + \beta \cdot g) = \alpha \delta(f) + \beta \delta (g)
\]
\[
    \delta(f \cdot  g) = g \cdot \delta(f) + f  \cdot \delta(g)
\]
\[
    \delta\left(\frac{f}{g}\right) = \frac{ g \cdot \delta(f) - f \delta(g)}{g^2}
\]
\[
    h(x) = f(g(x)) = g \circ f \Rightarrow h'(x) = f'(g(x)) g'(x)
\]
\[
    \frac{\delta}{\delta x} |x| = \frac{|x|}{x} 
\]
\ \newline
Molto spesso, per calcolare una derivata parziale si può evitare di usare la definizione: \textbf{per derivare rispetto a $x$ basta considerare $y$ come una costante e derivare la funzione come se fosse della sola variabile $x$ e viceversa.}\newline
\newline
Per il \textbf{calcolo di una derivata parziale in punto $(x_0, y_0)$ secondo la definizione}, seguire questo procedimento: se si richiede di calcolare il valore della derivata parziale di $x$ ( cioè $\frac{\delta f}{\delta x}(x_0,y_0)$), si parte dalla funzione $f(x,y)$ e si sostituisce $y= y_0$, ottenendo quindi $f(x,y_0)$, successivamente si calcola la derivata parziale, ottenendo dunque $\frac{\delta f}{\delta x} (x,y_0)$. Come ultima cosa si sostituisce $x = x_0$ e si arriva a un risultato numerico. Per la trovare il valore della derivata parziale di $y$ in un preciso punto seguire lo stesso procedimento opposto.\newline
\newline
In alcuni esercizi è richiesto di \textbf{calcolare le derivate parziali in tutti i punti in cui esistono}. Il procedimento tipico consiste nel calcolare per prima cosa le derivate parziali generiche. Una volta calcolate sapremo che sicuramente esistono dove queste sono definite (dominio), ma non siamo sicuri dei punti in cui non lo sono (al di fuori del dominio). Quindi dobbiamo analizzare singolarmente tutti i punti al di fuori del dominio e per farlo sfruttiamo il procedimento visto sopra ("derivata parziale in un punto secondo la definizione"), calcolando esplicitamente le derivate nei punti richiesti.
Finchè si tratta per esempio di calcolarle per un punto preciso non ci sono problemi, il calcolo è facile, ma ci sono alcuni casi difficili, per esempio:
\begin{itemize}
    \item Calcolare le derivate parziali secondo la definizione lungo una retta. Per esempio in $y=0$, per calcolare la $\frac{\delta f}{\delta x}(x_0, 0)$ non ci sono problemi, si procede come al solito. Ma per la $\frac{\delta f}{\delta y} (x_0, 0)$ ci sono difficoltà, siccome non possiamo sostituire le $y$ con $0$ e poi derivare per la $y$, dobbiamo ragionare così: la derivata non esiste a meno che non ci sia un valore che le $x$ possono assumere che annullino la funzione (per gli es che ho fatto fino ad ora sono solo al numeratore). Il concetto generale è che se non si trovano valori per $x_0$ tali che annullino la funzione e quindi ci permettano di calcolare la derivata parziale, si finisce per tornare a guardare la derivata parziale generica e quindi a non trovarla per quella retta. [spiegato davvero male, ma è un concetto strano].
\end{itemize}
\ \newline
Per \textbf{stabilire dove la funzione sia derivabile} bisogna calcolare le derivate parziali e osservarne il dominio.\newline
(n.b. tipicamente negli esercizi le funzioni sono descritte da un sistema che contiene una funzione prolungata nell'origine, in questo caso bisogna calcolare le derivate parziali al di fuori dell'origine e studiarne il dominio, in seguito bisogna calcolare il valore della derivata parziale nel punto $(0,0)$ col metodo descritto precedentemente).
\subsubsection{Piano tangente a funzioni in due variabili}
Il procedimento che mostriamo permette di individuare il piano tangente nell'ipotesi che esso esista, potrebbe però non esserci (il piano tangente esiste se la funzione è differenziabile in quel punto).\newline
\newline
Costruire il \textbf{piano tangente} a una funzione in due variabili in un punto $(x_0, y_0)$:
\begin{enumerate}
    \item troviamo la retta tangente alla funzione nel piano $y=y_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0)\\
            &y=y_0 \\
        \end{cases}
    \]
    \item troviamo la retta tangente alla funzione nel piano $x=x_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)\\
            &x=x_0 \\
        \end{cases}
    \]
    \item costruiamo il piano che contiene entrambe le rette:
    \[
        z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)
    \]
\end{enumerate}
\subsubsection{Differenziabilità e approssimazione lineare}
\textbf{In due o più variabili la sola derivabilità non implica nè continuità nè l'esistenza del piano tangente.}\newline
\newline
Concetto di differenziabilità in più variabili: l'incremento di $f$ è uguale all'incremento calcolato lungo il piano tangente, più un infinitesimo di ordine superiore rispetto alla lunghezza dell'incremento $(h,k)$ delle variabili indipendenti. In formule:
\[
    f(x_0 + h, y_0 + k) - f(x_0, y_0) = \frac{\delta f}{\delta x} (x_0,y_0) \cdot (x-x_0) + \frac{\delta f}{\delta y}(x_0, y_0) \cdot (y-y_0) + o(\sqrt{h^2 + k^2})
\]
per $(h,k) \rightarrow  (0,0)$.\newline
Tutto ciò che è prima dell'uguale (primo membro) rappresenta l'incremento della funzione, i primi due addendi del secondo membro rappresentano l'incremento calacolato lungo il pinao tangente. Ricordiamo che l'ultimo addendo rappresenta una funzione tale che $\lim_{(h,k)\rightarrow (0,0)}\frac{o(\sqrt{h^2 + k^2})}{\sqrt{h^2 + k^2}} = 0$.\newline
Se l'equazione di prima è soddisfatta, diremo che la funzione è \textbf{differenziabile} in $(x_0, y_0)$.\newline
\newline
Una funzione $f: \mathbb{R}^2 \rightarrow  \mathbb{R}$ si dice \textbf{differenziabile} in $(x_0, y_0)$ se è derivabile e se
\[
    \lim_{(h,k)\rightarrow (0,0)} \frac{f(x_0 + h, y_0+k)-f(x_0,y_0) - h f_x(x_0, y_0) - k f_y(x_0,y_0)}{\sqrt{h^2 + k^2}} = 0
\]
(Ricordiamo che $h = x-x_0$ e $k = y-y_o$).\newline
\newline
Questa scrittura dice che "il grafico della superficie si allontana dal piano tangente con un ordine di infinitesimo superiore a quello della distanza dal punto".
\newline
\[
    f \;\text{\textbf{differenziabile} in $(x_0, y_0)$}\; \Rightarrow f \; \text{\textbf{continua} in $(x_0,y_0)$}\;
\]
\ \newline
Da notare che la \textbf{differenziabilità implica la derivabilità}, cioè se una funzione è differenziabile in un punto, allora è anche derivabile nello stesso.\newline
\newline
Se $f$ è differenziabile in $\vec{x} \in \mathbb{R}^n$, si dice \textbf{differenziale} di $f$ calcolato in $\vec{x}$ la funzione lineare $df(\vec{x}) \;:\; \mathbb{R}^n \rightarrow  \mathbb{R}$ definita da:
\[
    df(\vec{x}) \;\;:\;\;h \rightarrow  \nabla f(\vec{x}) \cdot h.
\] 
dove $\nabla f(\vec{x})$ è il gradiente e $h$ è l'incremento.\newline
Nel caso in due varibiali, il numero $\nabla f(x_0) \cdot h$ rappresenta l'incremento della funzione nel passare da $x_0$ a $x_0+h$, calcolato lungo il piano tangente al grafico di $f$ in $x_0$.\newline
\newline
Negli esercizi viene spesso richeisto di calcolare il differenziale per una funzione $f(x,y)$ in un punto $(x_0,y_0)$: Per prima cosa bisogna verificare che la funzione sia differenziabile in quel punto, in seguito si calcolano le derivate parziali di $f$ nel punto, cioè il valore di $f_x(x_0,y_0)$ e $f_y(x_0,y_0)$, una volta calcolate si pone "$dx$" come l'incremento lungo l'asse delle $x$ e $dy$ l'incremento lungo l'asse delle $y$ e il \textbf{differenziale} si scrive come: $df(x_0,y_0) = f_x(x_0,y_0) \cdot dx + f_y(x_0,y_0) dy$.\newline
\newline
L'approssimazione dell'incremento di $f$ con il suo differenziale prende il nome di \textbf{linearizzazione}.
\subsubsection{Verifica della differenziabilità}
Per dimostrare la differenziabilità in un punto $(x_0,y_0)$ bisogna provare che:
\[
    \lim_{h,k\rightarrow 0,0}\frac{f(x_0+h, y_0+k)-\{
        f(x_0,y_0)+ \frac{\delta f}{\delta x}(x_0,y_0) h + \frac{\delta f}{\delta y}(x_0,y_0)k
        \}}{\sqrt{h^2+k^2}} =0.
\]
dove $h = x-x_0$ e $k = y-y_0$.\newline
\newline
Ma per certi casi particolari esistono criteri molto più comodi e semplici.\newline
\newline
\textbf{Teorema di condizione sufficiente di differenziabilità}: se le derivate parziali di $f$ esistono in un intorno di $x_0$ e sono continue in $x_0$, allora $f$ è differenziabile in $x_0$.\newline
In particolare se le derivate parziali esistono e sono continue in tutto $A$, allora $f$ è differenziabile in tutto $A$.\newline
Una funzione le cui derivate parziali esistono e sono continue in tutto $A$ si dice di classe $C^1(A)$, \textbf{dunque}: $f \in C^1(A) \rightarrow$ f differenziabile in $A$.\newline
\newline
Negli esercizi spesso si usa anche l'omogeneità di una funzione per sapere se essa è differenziabile o continua, oppure le proprietà delle funzioni radiali.\newline
\newline
Negli esercizi seguire quest'ordine:
\begin{enumerate}
    \item E' continua nel punto richiesto? se non lo è, può essere allungata?
    \item Funzione radiale? (vedi più avanti)
    \item Funzione omogenea? (vedi più avanti)
    \item Calcolo delle derivate parziali nel punto. Sono continue in quel punto?
    \item Verifica della differenziabilità tramite la definizione.
\end{enumerate}
\rule{\textwidth}{0.4pt}
\subsubsection{Derivate direzionali}
Si dice \textbf{derivata direzionale} della funzione $f$ rispetto al versore $v_\theta = \begin{cases}
    cos(\theta)\\ sin(\theta)
\end{cases}$, nel punto $(x_0,y_0)$, il limite
\[
    \frac{\delta f}{\delta \theta}(x_0,y_0) = \lim_{t\rightarrow 0}\frac{f(x_0 + t cos(\theta), y_0 + t sin(v_\theta)) - f(x_0,y_0)}{t}
\]
Se tale limite esiste ed è finito diremo che $f$ è derivabile nella direzione $v_\theta$.\newline
\newline
\textbf{Calcolo di una derivata direzionale per un generico vettore} $(cos(\theta), sin(\theta))$ nell'origine di una funzione $f$: Per prima cosa si ottiene la funzione $g(t) = f(t \cdot cos(\theta), t \cdot sin(\theta))$ e la si semplifica per $t \rightarrow 0$ (anche usando asintotici). In seguito si studia la derivata $g'(0) = \frac{\delta f}{\delta t}(t \cdot cos(\theta), t \cdot sin(\theta))$.\newline
Se è richiesto il calcolo in un punto generico, e non nell'origine, è sufficiente usare $t \cdot cos(\theta) + x_0$ e $t \cdot sin(\theta) + y_0$. \newline
\newline
\textbf{Se la funzione è differenziabile, allora le derivate parziali consentono di calcolare tutte le altre derivate direzionali}.\newline
\newline
\textbf{Formula del gradiente}: 
\[
    \frac{\delta f}{\delta v_\theta}(x_0,y_0) = \nabla f(x_0,y_0) \cdot v_\theta = \frac{\delta f}{\delta x} (x_0,y_0)cos(\theta) + \frac{\delta f}{\delta y}(x_0, y_0) sin(\theta)
\]
Cioè la derivata direzionale è il prodotto scalare del gradiente con il versore nella direzione in cui si deriva, quindi tutte le derivate direzionali sono combinazioni lineari delle derivate parziali.\newline
Se la formula del gradiente non vale in un punto, allora la funzione non è differenziabile in quel punto.\newline
Inoltre la formula del gradiente non vale se la generica derivata direzionale non è combinazione lineare di $cos(\theta), sin(\theta)$.\newline
\newline
Negli esercizi se viene dato un vettore come direzione, bisogna prima assicurarsi che esso sia un versore, se non lo è bisogna ottenerlo dividendo per il modulo del vettore.\newline
\newline
Da notare è che $\nabla f(x_0,y_0)$ indica la \textbf{direzione di massima crescita} di $f$, ossia la direzione di massima derivata direzionale, invece $-\nabla f(x_0)$ rappresenta la direzione di minima derivata direzionale, infine nelle direzioni ortogonali al gradiente le derivate direzionali sono nulle, quindi di \textbf{pendenza nulla}.\newline
\newline
Il gradiente è ortogolane il ogni punto alle linee di livello.
\subsubsection{Riepilogo}
\begin{itemize}
    \item $f \in C^1(A) \Rightarrow f$ differenziabile in $A$ (cioè $f$ ha iperpiano tangente) $\Rightarrow f$ è continua, derivabile, ha derivate direzionali, vale la formula del gradiente.
    \item $f$ continua, derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ differenziabile
    \item $f$ derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ continua 
\end{itemize}
\subsubsection{Extra: Gradiente di una funzione radiale, Criterio di continuità e differenziabilità per funzioni radiali, funzioni omogenee, Equazione del trasporto}
\subsubsection*{Gradiente di una funzione radiale}
Si chiama funzione radiale una funzione $h$ che dipende solo dalla distanza di dall'origine, ossia
\[
    h(x) = g(|x|).
\]
ponendo $\rho = |x| = \sqrt{\sum_{j=1}^{n}x_j^2}$ si ha:
\[
    \nabla_\rho = ( \frac{x_1}{\rho}, \frac{x_2}{\rho}, \dots, \frac{x_n}{\rho}).
\]
\[
    \nabla h(x) = g'(|x|)( \frac{x_1}{|x|}, \dots, \frac{x_n}{|x|})
\]
\[
    |\nabla h(x)| = |g'(|x|)|
\]
Le funzioni radiali sono spesso utilizzate negli esercizi in cui le incognite compaiono solo all'interno del termine $\sqrt{\sum_{j=1}^{n}x_j^2}$, in tal caso si ottiene $g(\rho)$ sostituendo ogni $\sqrt{\sum_{j=1}^{n}x_j^2}$ con $ \rho$, successivamente si può procedere sfruttando le proprietà di continuità e differenziabilità delle funzioni radiali.
\subsubsection*{Criterio di continuità e differenziabilità per funzioni radiali}
Sia $f \;\;:\;\; \mathbb{R}^n-\{0\} \rightarrow  \mathbb{R}$ una funzione radiale, cioè $f(x) = g(|x|)$ con $g \;\;:\;\;(0, +\infty) \rightarrow \mathbb{R}$ e sia $f$ continua fuori dall'origine. Allora:
\begin{itemize}
    \item $f$ è continua in $0$, se e solo se esiste finito $\lim_{\rho\rightarrow 0^+}g(\rho)$;
    \item $f$ è differenziabile in $0$ se e solo se esiste $g'(0)=0$.
\end{itemize}
Negli esercizi spesso si controlla prima la continuità nell'origine, se non lo è si allunga la funzione e successivamente si calcola la differenziabilità nell'origine.\newline
\subsubsection*{Funzioni omogenee}
Una funzione $f \;\;:\;\; \mathbb{R}^n \rightarrow \mathbb{R}$ (eventualmente definita solo per $x\neq 0$), non identicamente nulla, si dice positivamente omogenea di grado $\alpha \in \mathbb{R}$ se
\[
    f(\lambda x) = \lambda^\alpha f(x) \;\;\;\;\; \;\forall\;x \in \mathbb{R}^n, x\neq 0, \lambda>0.
\]
La funzione $f$ si dice omogenea di grado $\alpha$ se la formula di prima vale anche per $\lambda<0$.\newline
Se $f$ è positivamente omogenea vale
\[
    f(x) = f( |x| \cdot \frac{x}{|x|}) = |x|^\alpha f(\frac{x}{|x|}).
\]
In particolare se $f$ è omogenea (o positivamente omogenea) di grado zero, significa che è costante su ogni retta (o semiretta) uscente dall'origine. Infatti, indicata con
\[
    r(t)=tv
\]
con $v$ versore fissato, sarà
\[
    f(r(t))=f(tv)=t^0f(v)=f(v)=costante.
\]
Più in generale, per una funzione in due variabili positivamente omogenea di grado $\alpha$ vale la seguente rappresentazione in coordinate polari:
\[
    f(\rho, \theta)=\rho^\alpha g(1,\theta)
\]
per qualche $\alpha \in \mathbb{R}$ e qualche funzione $g \;\;:\;\; [0, 2\pi) \rightarrow \mathbb{R}$.\newline

Sia $f \;\;:\;\; \mathbb{R}^n \rightarrow \mathbb{R}$ una funzione positivamente omogenea di grado $\alpha$, definita e continua per $x\neq 0$. Allora:
\begin{itemize}
    \item $f$ è continua anche nell'origine se $\alpha>0$; in questo caso $f(0) = 0$; $f$ è discontinua nell'origine se $\alpha<0$; è discontinua anche se $\alpha=0$, tranne il caso banale in cui $f$ è costante.
    \item $f$ è differenziabile nell'origine se $\alpha>1$; non è differenziabile nell'origine se $\alpha <1$, tranne il caso banale in cui $\alpha=0$ e $f$ è costante; se $\alpha=1$, $f$ è differenziabile se e solo se è una funzione lineare, (ossia $f(x) = a \cdot x$ per qualche vettore costante $a \in \mathbb{R}^n$).
\end{itemize}
Si ricordi che ogni ogni derivata parziale prima di una funzione posivamente omogenea di grado $\alpha$, se esiste, è una funzione positivamente omogenea di grado $\alpha -1$. \newline
\subsubsection*{Equazione del trasporto}
Si definisce equazione del trasporto la seguente:
\[
    c \frac{\delta u}{\delta x} + \frac{\delta u}{\delta t} = 0 \;\;\;\;\; \;\;\;\;\; (?)
\]
\newline
Teorema del valor medio. Sia $A \subset \mathbb{R}^n$ un aperto e $f \;\;:\;\; A \rightarrow \mathbb{R}$ una funzione differenziabile in $A$. Allora per ogni coppia di punti $x_0, x_1 \in A$, esiste un punto $x^*$ tale per cui:
\[
    f(x_1)- f(x_0) = \nabla f(x^*) \cdot  (x_1 + x_0).
\]
In particolare:
\[
    |f(x_1) - f(x_0)| \leq |\nabla f(x^*)| \cdot |(x_1 + x_0)|.
\]
\subsection{Derivate di ordine superiore e matrice Hessiana}
Per una funzione $f: \mathbb{R}^n \rightarrow  \mathbb{R}$ esistono $n^2$ \textbf{derivate parziali seconde} che indicheremo nel seguente modo:
\[
    f_{x_ix_j} = (f_{x_i})_{x_j} = \frac{\delta^2 f}{\delta x_{j} \delta x_{i}} = \frac{\delta }{\delta x_{j}}\left(\frac{\delta f}{\delta x_i}\right)
\]
Le derivate seconde vengono poi inserite in una matrice chiamata \textbf{matrice Hessiana}:
\[ H_f=\left(\begin{matrix}
        f_{x_1x_1} \;\; &f_{x_1x_2} \;\; &\dots \;\; &f_{x_1x_n}\\
        f_{x_2x_1} \;\; &f_{x_2x_2} \;\; &\dots \;\; &f_{x_2x_n}\\
        \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
        f_{x_nx_1} \;\; &f_{x_nx_2} \;\; &\dots \;\; &f_{x_nx_n}
    \end{matrix}\right)
\]
In particolare, per due variabili:
\[
    H_f(x_0, y_0) = \left(\begin{matrix}
        f_{xx}(x_0, y_0) \;\; f_{xy}(x_0, y_0)\\
        f_{yx}(x_0, y_0) \;\; f_{yy}(x_0, y_0)
    \end{matrix}\right)
\]
\ \newline
Chiameremo \textbf{derivate seconde doppie} quelle del tipo $f_{x_ix_i}$ che giaciono sulla diagonale principale di $H_f$ e \textbf{derivate seconde miste} tutte le altre.\newline
\newline
Dato un sottoinsieme $\Omega \subset \mathbb{R}^n$, indichiamo con $C^2(\Omega)$ l'insieme delle funzioni $f:\Omega \rightarrow \mathbb{R}$ che hanno tutte le derivate seconde continue. La storia poi continua: una funzione si dice di classe $C^k$ se ha tutte le derivate k-esime continue.\newline
\newline
\textbf{Teorema di Schwarz} Sia $\Omega \subset \mathbb{R}^n$ un insieme aperto e sia $f \in C^2(\Omega)$; allora $f_{x_ix_j} = f_{x_jx_i}$ per ogni $i,j = 1,\dots,n$, e cioè la matrice hessiana $H_f$ è simmetrica.\newline
\newline
La traccia della matrice Hessiana viene chiamata \textbf{Laplaciano} e indicato come:
\[
    \Delta f = \sum_{i=1}^{n} f_{x_ix_j}
\]
\ \newline
Se $f \in C^2(A)$ e $x_0 \in A$, si dice \textbf{differenziale secondo} di $f$ in $x_0$ la funzione
\[
    d^2f(x_0) \;\;:\;\; h \rightarrow \sum_{i=1}^{n}\sum_{j=0}^{n}\frac{\delta^2(f)}{\delta(x_i) \delta(x_j)}(x_0)h_ih_j.
\]
\subsubsection{Formula di Taylor (resto secondo Peano)}
\textbf{Formula di Taylor al secondo ordine} (resto secondo peano):\newline
Sia $f$ di classe $C^2$ in $(x_0,y_0) \in \mathbb{R}^2$. Allora, per $(x,y) \rightarrow  (x_0, y_0)$ si ha 
\[
    f(x,y) = f(x_0,y_0) + \nabla f(x_0,y_0) \cdot \binom{x-x_0}{y-y_0} + 
\]
\[
    +\frac{1}{2} H_f(x_0,y_0) \binom{x-x_0}{y-y_0} \cdot \binom{x-x_0}{y-y_0} + o[(x-x_0)^2 + (y-y_0)^2]
\]
dove l'operazione indicata con $\cdot $ è un prodotto scalare, mentre l'operazione $H_f(x_0,y_0) \binom{x-x_0}{y-y_0}$ è un prodotto matriciale righe per colonne.
\newline
\newline
Formula di Taylor (resto secondo Peano) scritta per esteso per $(x,y) \rightarrow  (x_0,y_0)$:
\[
    f(x,y) = f(x_0, y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0) +
\]
\[
    + \frac{1}{2}\left( f_{xx}(x_0,y_0)(x-x_0)^2 + 2f_{xy}(x_0,y_0)(x-x_0)(y-y_0) + f_{yy}(x_0,y_0)(y-y_0)^2 \right) +
\]
\[
    + o[(x-x_0)^2 + (y-y_0)^2]
\]
per $(x,y) \rightarrow (x_0,y_0)$.
\subsection{Ottimizzazione libera}
\subsubsection{Ripasso sugli autovalori e autovetori}
Ricordiamo che un numero complesso $\lambda$ e un vettore non nullo $v \in \mathbb{C}^n$. Si dicono, rispettivamente, \textbf{autovalore e autovettore} (di $\lambda$) di una matrice $M$ di ordine $n$, se soddisfano la relazione:
\[
    Mv = \lambda v
\]
oppure
\[
    (M-\lambda I_n)v = 0.
\]
Quest'ultima equazione ha soluzioni $v$ non nulle se e solo se la matrice dei coefficienti e singolare, ovvero se $\lambda$ è soluzione dell'equazione caratteristica:
\[
    det(M-\lambda I) = 0
\]
esistono esattamente $n$ autovalori di $M$ ciascuno contato secondo la propria molteplicità.\newline
Le matrici $M$ simmetriche hanno prorpietà importanti:
\begin{itemize}
    \item gli autovalori di $M$ sono reali e possiedono autovettori reali;
    \item esistono $n$ autovettori lineari che costituiscono una base ortonormale in $\mathbb{R}^n$;
    \item La matrice $S = {w_1, w_2, \dots, w_n}$ le cui colonne sono gli autovettori lineari è orotognale e diagonalizza $M$, precisamente:
    \[
        S^TMS = \Lambda = \left[\begin{matrix}
            \lambda_1 \;\; &0 \;\; &\dots \;\; &0\\
            0 \;\; &\lambda_2 \;\; &\dots \;\; &0\\
            \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
            0 \;\; &0 \;\; &\dots \;\; &\lambda_n
        \end{matrix}\right]
    \]
\end{itemize}
\subsubsection{Massimi e minimi}
\textbf{Teorema} Sia $f$ di classe $C^2$ in un punto $\vec{x}^0 \in \mathbb{R}^n$ e sia $H_f (\vec{x}^0)$ la sua matrice Hessiana. Se tutti gli autovalori di $H_f(\vec{x}^0)$ sono \textbf{positivi}, allora esiste un intorno di $\vec{x}^0$ dove la superficie di equazione $x_{n+1} = f(x_1,\dots,x_n)$ sta sopra al piano tangente in $\vec{x}^0$. Se tutti gli autovalori di $H_f(\vec{x}^0)$ sono \textbf{negativi}, allora esiste un intorno di $\vec{x}^0$ dove la superficie di equazione $x_{n+1} = f(x_1,\dots,x_n)$ sta sotto al piano tangente in $\vec{x}^0$. Se $H_f(\vec{x}^0)$ ha autovalori di entrambi i segni, allora la superficie di equazione $x_{n+1} = f(x_1,\dots,x_n)$ attraversa il piano tangente in $\vec{x}^0$.\newline
\newline
Questo teorema si può enunciare geometricamente dicendo che se tutti gli autovalori di $H_f(\vec{x}^0)$  sono positivi (negativi) allora la funzione $f$ è convessa (concava) in $\vec{x}^0$.\newline
\newline
Definiamo i punti di estremo:
\begin{itemize}
    \item $x_0$ è detto punto di massimo (minimo) globale se per ogni $x$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$);
    \item $x_0$ è detto punto di massimo (minimo) locale se esiste un intorno di $x_0$ detto $U$ tale per cui per ogni $x \in U$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$).
\end{itemize}
\ \newline
Il teorema enunciato prima è utile per determinare i massimi e minimi relativi della funzione $f$. I punti candidati ad essere tali sono individuati dal:\newline
\textbf{Teorema di Fermat}: Sia $f: \mathbb{R}^n \rightarrow  \mathbb{R}$ derivabile in un punto $\vec{x}^0 \in \mathbb{R}^n$ di estremo relativo: allora $\nabla f (\vec{x}^0) = 0$.
\newline
\newline
I punti in cui il gradiente di una funzione si annulla si dicono punti \textbf{critici} o \textbf{stazionari} di $f$. Una volta individuati tutti i punti stazionari, si può iniziare un'analisi su di essi per verificare se sono o meno punti di massimo o minimo. Se non lo sono essi prendono il nome di punti di \textbf{sella} o \textbf{colle}. Da notare particolarmente è che una funzione può assumere valori di massimo o minimo anche in punti in cui non è derivabile, dunque questi punti vanno analizzati separatamente.\newline
\newline
\textbf{Caso $n = 2$}:\newline
Sia $f$ di classe $C^2$, in un punto critico (trovato col teorema di Fermat) $(x_0, y_0) \in \mathbb{R}^2$.\newline
Se $f_{xx}(x_0,y_0)f_{yy}(x_0,y_0) > f_{xy}(x_0,y_0)^2$ allora $f$ ha un estremo relativo in $(x_0,y_0)$: se $f_{xx} (x_0,y_0) > 0$ allora il punto è di minimo relativo, se $f_{xx}(x_0,y_0) <0$ allora il punto è di massimo relativo.\newline
Si ha $det[H_f(x_0,y_0)] = f_{xx}(x_0,y_0)f_{yy}(x_0,y_0) - f_{xy}(x_0,y_0)^2$. Se i due autovalori sono concordi il determinante è positivo e, in particolare, non può essere $f_{xx}(x_0,y_0) = 0$. Se i due autovalori sono discordi, allora il determinante è negativo.\newline
Se $det[H_f(x_0,y_0)]= 0$ diremo che siamo nel "caso dubbio". Per toglierci il dubbio, potremmo analizzare le derivate terze e quarte (ma diventerebbe troppo complicato), quindi solitamente si preferisce affrontare il problema direttamente analizzando il segno di $f(x,y)- f(x_0,y_0)$.
\subsubsection{Strategia per lo studio dei massimi e dei minimi}
Vediamo una strategia da seguire:
\begin{enumerate}
    \item si isolano i punti di $f$ che non sono regolari (es. non derivabili una o due volte). Questi punti dovranno essere analizzati separatamente;
    \item trovare i punti critici risolvendo:
        \[
            \begin{cases}
                &f_{x_1}(x_1,x_2,\dots,x_n) = 0\\
                &f_{x_2}(x_1,x_2,\dots,x_n) = 0\\
                &\dots\\
                &f_{x_n}(x_1,x_2,\dots,x_n) = 0\\
            \end{cases}
        \]
    \item per ogni punto critico:
        \begin{enumerate}
            \item si calcola l'Hessiana:
                \[
                    H_f(x_0, y_0) = \begin{matrix}
                        f_{xx}(x_0,y_0) \;\;\; & f_{xy}(x_0,y_0)\\
                        f_yx(x_0,y_0) \;\;\; & f_{yy}(x_0,y_0)\\
                    \end{matrix}
                \]
            \item se $det(H_f(x_0, y_0)) > 0$ e
                \begin{itemize}
                    \item $f_{xx}(x_0,y_0)>0$ allora $(x_0, y_0)$ è di minimo locale forte;
                    \item $f_{xx}(x_0,y_0)<0$ allora $(x_0, y_0)$ è di massimo locale forte;
                \end{itemize}
                (si noti che in questo caso $f_{xx}(x_0,y_0)$ e $f_{yy}(x_0,y_0)$ hanno lo stesso segno).
            \item se $det(H_f(x_0, y_0)) < 0$ allora $(x_0, y_0)$ è punto di sella;
            \item se $det(H_f(x_0, y_0)) = 0$ occore un analisi ulteriore. Si possono percorrere due vie: 1.si studiano le derivate successive (terza e quarta) [sconsigliato]; 2. si affronta il problema direttamente analizzando il segno di $f(x,y) - f(x_0, y_0)$
        \end{enumerate}
\end{enumerate}
\begin{comment}
    roba che nel libro del Gazzola non c'è, quindi la tolgo
\subsubsection{Forme quadratiche, classificazione}
Un modo per determinare la natura di un punto stazionario è quello di analizzare il segno dell'incremento $\nabla f(x_0) = f(x_0 + h ) - f(x_0)$. \textbf{Se infatti si riesce a stabilire che $\nabla f(x_0)$ si mantiene di segno positivo o negativo, per ogni $h$ di modulo abbastanza piccolo, possiamo dedurre che $x_0$ è punto di minimo o massimo locale}. Se invece al variare di $h$, $\nabla f(x_0)$ cambia segno, siamo in presenza di un punto si sella.\newline
\begin{tcolorbox}
Lo studio del segno di $\nabla f(x_0)$ riconduce all'analisi del segno del polinomio omogeneo di secondo grado nelle componenti di $h$ (che prende il nome di \textbf{forma quadratica}) dato da
\[
    \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j.
\]
Ogni forma quadratica risulta associata a una matrice simmetrica $M$. Nel caso del differenziale la matrice $M$ coincide con la matrice Hessiana.
\end{tcolorbox}
\begin{tcolorbox}
Il segno della forma quadratica è quindi studiabile analizzando la sua matrice $M = \left(\begin{matrix}
    a \;\;\; b\\
    b \;\;\; c\\
\end{matrix} \right)$ con $a\neq 0$ nel seguente modo:
\begin{itemize}
    \item è definitivamente positiva (negativa) se e solo se $det(M)>0$ e $a >0$ ($a<0$);
    \item indefinita se $det(M)<0$;
    \item semidefinita positiva (negativa) se e solo se $det(M)=0$ e $a>0$ ($a<0$).
\end{itemize}
Se $a = 0$ e $c\neq 0$, nelle affermazioni precedenti occore sostituire $a$ con $c$.
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Forme quadratiche, test degli autovalori}
Un importante test per determinare il segno di una funzione quadratica in $\mathbb{R}^n$ è basato sul segno degli autovalori della matrice $M$.\newline
\begin{tcolorbox}
Tornando allo studio del segno della forma quadratica con la sua matrice $M$:
\begin{itemize}
    \item definitivamente positiva (negativa) se e solo se tutti gli autovalori di $M$ sono positivi (negativi);
    \item semidefinita positiva (negativa) se e solo se tutti gli autovalori di $M$ sono $\geq 0$ ($\leq 0$) e almeno uno di essi è nullo;
    \item indefinita se $M$ ha almeno un autovalore positivo e uno negativo.
\end{itemize}
\end{tcolorbox}
Da notare è che per una forma quadratica l'origine è sempre un punto stazionario. \newline
\rule{\textwidth}{0.4pt}
\subsubsection{Studio della natura dei punti critici}
Per estrarre informazioni su un punto critico $x_0$ occorre studiare e classificare la forma quadratica 
\[
    q(h) = \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j = h^TH_f(x_0)h
\]
dove $H_f(x_0)$ è la matrice Hessiana di $f$ in $x_0$.\newline
\begin{tcolorbox}
\begin{itemize}
    \item se la forma quadratica è definitivamente positiva (negativa), allora $x_0$ è un punto di minimo (massimo) locale forte;
    \item se la forma quadratica è indefinita, allora $x_0$ è un punto di sella;
    \item se la forma quadratica è in $x_0$ semidefinita positiva (negativa) e non nulla, allora $x_0$ è di minimo (massimo) debole oppure di colle; la situazione cambia se la forma quadratica è semidefinita positiva (negativa) non solo in $x_0$, ma anche per ogni $x$ in un intorno di $x_0$, f è convessa (concava), dunque un punto di minimo (massimo) debole;
    \item se la forma quadratica è nulla, allora non possiamo estrarne informazioni significanti.
\end{itemize}
\end{tcolorbox}
\end{comment}
\subsection{Ottimizzazione vincolata}
\begin{comment}
\rule{\textwidth}{0,4pt}
\subsubsection{Vincoli di uguaglianza e moltiplicatori di Lagrange. Funzioni di due variabili}
\textbf{Problema:} Date due funzioni $f= f(x,y)$ e $g = g(x,y)$, dotate di derivate parziali continue in $\mathbb{R}^2$. Si vogliono determinare gli estremi di $f$ sotto la condizione di vincolo $g(x,y) = b$, $b \in \mathbb{R}^2$.\newline
Il problema consiste nel massimizzare (minimizzare) la restrizion di $f$ al vincolo specificato.
\begin{tcolorbox}
\begin{itemize}
    \item \textbf{vincolo esplicitabile}: caso in cui il vincolo $g(x,y) = b$ definisce esplicitamente $y=y(x)$ o $x = x(y)$ oppure, più in generale, definisce le equazioni parametriche ($x= x(t), y = y(t)$) di una curva $\gamma$. Il problema allora è ricondotto alla ricerca degli estremi di una funzione reale di variabili reali
    \[
        \phi(t) = f(x(t), y(t))
    \]
    \item \textbf{metodo dei moltiplicatori di Lagrange}: utilizzato quando il vincolo è rappresentato da una curva regolare assegnata in qualsiasi forma (parametrica, cartesiana o implicita).\newline
    \textbf{teor.} Teorema dei moltiplicatori di lagrange. Siano $f,g \in C^1(\mathbb{R}^2)$ e $(x^*, y^*)$ punto di estremo vincolato per $f$ sotto il vincolo:
    \[
        g(x,y) = b
    \]
    Se $(x^*, y^*)$ è regolare per il vincolo, cioè $\nabla g(x^*, y^*) \neq (0,0)$, allora esiste $\lambda^* \in \mathbb{R}$ (detto moltiplicatore di Lagrange) tale che:
    \[
        \nabla f(x^*, y^*) = \lambda \nabla g(x^*, y^*)
    \]
    Questa formula esprime il fatto che se $(x^*, y^*)$, verifica le ipotesi del teorema, allora la derivata di $f$ lungo la tangente al vincolo si deve annullare e in tal caso diciamo che $(x^*, y^*)$ è punto critico vincolato.\newline
    \textbf{oss.} Introducendo la funzione $L = L (x, y , \lambda)$, detta Lagrangiana, definita da
    \[
        L(x,y,\lambda) = f(x,y) - \lambda[g(x,y) - b]
    \] 
    il teorema afferma che se $(x^*, y^*)$ è punto di estremo vincolato, allora esiste $\lambda^*$ tale che il punto $(x^*, y^*, \lambda^*)$ sia punto critico libero per $L$. Infatti i punti critici in $L$ sono soluzioni del sistema:
    \[
        \begin{cases}
            L_x = f_x - \lambda g_x  = 0\\
            L_y = f_y - \lambda g_y = 0\\
            L_\lambda = b-g = 0
        \end{cases}
    \]
    Metodo dei moltiplicatori di Lagrange:
    \begin{itemize}
        \item Si isolano gli eventuali punti non regolari dell'insieme $g(x,y) = b$, che vanno esaminati a parte.
        \item si cercano i punti critici liberi della Lagrangiana e cioè soluzioni del sistema visto nell'osservazione.
        \item si determina la natura dei punti critici. A questo proposito risulta spesso utile il teorema di Weierstrass.
    \end{itemize}
\end{itemize}
\end{tcolorbox}
\rule{\textwidth}{0,4pt}
\subsubsection{Moltiplicatore di Lagrange. Il caso generale}
Consideriamo ora problemi di ottimizzazione vincolata, per funzioni di $n$ variabili ($n \geq 2$) con un numero $m$ di vincoli di uguaglianza.\newline
Siano $m+1$ funzioni reali di $n$ variabili $f,g_1,g_2,\dots,g_m : \mathbb{R}^n \rightarrow \mathbb{R}$ tutte $C^1(\mathbb{R}^n)$. Si vogliono determinare gli estremi di $f$ quando le variabili sono soggette alle $m$ condizioni di vincolo:
\[
    \begin{cases}
        g_1(x_1,\dots,x_n) = b_1\\
        \dots\\
        g_m(x_1,\dots,x_n) = b_m
    \end{cases}
\]
\textbf{teor.} Teorema dei moltiplicatori di Lagrange, caso generale.\newline
Sia $f, g_1, \dots, g_m \in C^1(\mathbb{R}^n)$, $\vec{g} = (g_1, g_2, \dots, g_m)$ e sia $\vec{x}^*$ punto di estremo vincolato per $f$ rispetto al vincolo
\[
    \vec{g}(\vec{x}) = \vec{b}
\] 
Se $\vec{x}^*$ è un punto regolare per $\vec{g}$, cioè se il rango di $D \vec{g}(\vec{x}^*)$ è $m$, allora esistono $m$ numeri reali $\lambda_1^*, \dots, \lambda_m^*$, detti moltiplicatori di Lagrange, tali che
\[
    \nabla f(\vec{x}^*) = \sum_{j=1}^{m}\lambda_j^* \nabla g_j (\vec{x}^*)
\]
\newline
Introduciamo la funzione Lagrangiana
\[
    L(x_1, \dots,x_n, \lambda_1, \dots \lambda_m) =f(x_1, \dots, x_n) - \sum_{j=1}^{m} \lambda_j[g_j(x_1,\dots,x_n) - b_j]
\]
dipendente da $n+m$ variabili. Se $\vec{x}^*$ è un punto regolare, allora esiste un vettore di moltiplicatori
\[
    \vec{\lambda}^* =(\lambda_1^*, \dots, \lambda_m^*) 
\]
tale che $(\vec{x}^*, \vec{\lambda}^*)$ è un punto critico libero per la Lagrangiana. Infatti i punti critici liberi per la Lagrangiana si trovano risolvendo il seguente sistema di $n+m$ equazioni in $n+m$ incognite:
\[
    \begin{cases}
        D_{x_1} L = D_{x_1}f - \sum_{j=1}^{m} \lambda_j D_{x_1}g_j = 0\\
        \dots\\
        D_{x_n} L = D_{x_n}f - \sum_{j=1}^{m} \lambda_j D_{x_n} g_j = 0\\
        D_{\lambda_1}L = b_1 - g_1 = 0\\
        \dots\\
        D_{\lambda_m}L = b_m - g_m= 0
    \end{cases}
\]
\rule{\textwidth}{0,4pt}
\end{comment}
\subsubsection{Metodo dei moltiplicatori di Lagrange}
Un punto di ottimo per $f$ vincolato a $g = 0$ si ottiene c ercando una curva di livello di $f$ che sia tangente alla curva di livello $0$ di $g$. Dato che il gradiente di una funzione è ortogonale alla sua curva di livello, la condizione di tangenza tra le due curve di livello si traduce in una relazione di proporzionalità tra $\nabla f$ e $\nabla g$. Questo è il principio che governa il \textbf{metodo dei moltiplicatori di Lagrange}.\newline
\newline
Siano $f,g \in C^1$ e supponiamo di volere ottimizzare (massimizzare o minimizzare) la funzione $z = f(x,y)$ sotto il vincolo $g(x,y) = 0$ (se fosse $g(x,y) = b$ possiamo trasformarla in $g(x,y) - b = 0$). Introduciamo la funzione $L : \mathbb{R}^3 \rightarrow \mathbb{R}$ definita da
\[
    L(x,y,\lambda) = f(x,y) - \lambda g(x,y)
\]
e che si chiama \textbf{Lagrangiana}. Se cerchiamo i punti stazionari (in $\mathbb{R}^3$!) della funzione $L$ siamo portati a risolvere il sistema $\nabla L = 0$ e cioè
\[
    \begin{cases}
        f_x(x,y) = \lambda g_x(x,y) \\  
        f_y (x,y) = \lambda g_y(x,y)\\
        g(x,y) = 0
    \end{cases}
\]
Le prime due condizioni si possono riscrivere coem $\nabla f = \lambda \nabla g$ ed esprimono la proporzionalità tra gradienti chem a sua volta, descrive la tangenza delle curve di livello di $f$ e $g$. La terza equazione è l'equazione del vincolo e dice che la suddetta tangenza va cercata tra i punto che appartengono al vincolo.\newline
Chiaramente da queste equazioni non distinguiamo gli eventuali massimi dai minimi e non è nemmeno detto che una soluzione sia un estremo relativo, tuttavia: \textbf{gli eventuali punti di ottimo di $f$ soggetti al vincolo $g = 0$ vanno cercati tra le soluzioni di queste tre equazioni}.\newline
\newline
Per capire quali dei punti che sono soluzione del sistema sono estremi, è sufficiente calcolare i valori della funzione in tali punti e valutare se sono massimi o minimi.\newline
\newline
Il \textbf{teorema di Weierstrass} può essere di aiuto per garantire l'esistenza dei massimi e dei minimi: Sia $[a,b]\subset \mathbb {R}$ un intervallo chiuso e limitato non vuoto e sia $f\colon [a,b]\to \mathbb {R}$ una funzione continua. Allora $f(x)$ ammette (almeno) un punto di massimo assoluto e un punto di minimo assoluto nell'intervallo $[a,b]$. Da notare è che se il vincolo non è chiuso e limitato non si può usare il teorema di Weierstrass.\newline
\newline
Notiamo che il valore esatto del moltiplicatore $\lambda$ non è essenziale: permette di trovare i punti candidati, ma non serve per calcolare il livello delal funzione.\newline
\newline
Un punto più delicato diguarda i punti in cui i vettori delle derivate parziali di $f$ e di $g$ ($\nabla f$ e $\nabla g$) si annullano sul vincolo. Se si annulla $\nabla f$ troveremo un moltiplicatore nullo e saremo in presenza di un punto critico libero di $f$ che posiamo tranquillamente inserire tra i candidati e valutarlo insieme agli altri punti. Se invece di annulla $\nabla g$ il metodo di Lagrange non funziona dato che non si riesce a determinare il moltiplicatore, quindi questi punti dovranno essere inseriti fra i candidati e valutati a parte.
\subsection{Funzioni convesse di n variabili}
\subsubsection{Generalità sulle funzioni convesse}
Un insieme $\Omega \subseteq \mathbb{R}^n$ si dice \textbf{convesso} se per ogni coppia di punti $x_1, x_2 \in \Omega$ si ha che $[x_1,x_2]\subseteq \Omega$ (dove col simbolo $[x_1,x_2]$ si denota il segmento con estremi $x_1,x_2$); si dice strettamente convesso se per ogni coppia di punti $x_1,x_2 \in \Omega$ il segmento $(x_1,x_2)$ privato degli estremi è strettamente contenuto in $\Omega$.\newline
\newline
Si dice \textbf{epigrafico} di una funzione $f \;\;:\;\; \Omega \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ l'insieme
\[
    epi(f)=\{(x,z) \in \mathbb{R}^{n+1} \;\;:\;\; z\geq f(x), x \in\Omega\}
\]
\newline
\newline
Si dice che una funzione è \textbf{convessa} se $epi(f)$ è un sottoinsieme convesso, si dice che una funzione è \textbf{concava} se $-f$ è convessa.\newline
Formalmente si dice che una funzione è convessa se e solo se per ogni $x_1,x_2$, $t \in [0,1]$ vale la condizione
\[
    f(tx_2 + (1-t)x_1) \leq tf(x_2) + (1-t)f(x_1).
\]
Si noti che $tx_2 + (1-t)x_1$ percorre il segmento $[x_1,x_2]$ al variare di $t \in[0,1]$\newline
\newline
Se $f$ è convessa allora:
\begin{itemize}
    \item $f$ è continua;
    \item $f$ ha derivate parziali destre e sinistre in ogni punto;
    \item nei punti in cui è derivabile, $f$ è anche dierenziabile.
\end{itemize}
\ \newline
\textbf{Teorema di convessità e piano tangente}. Sia $f \;\;:\;\; \Omega \rightarrow \mathbb{R}$ differenziabile in $\Omega$. Allora $f$ è convessa in $\Omega$ se e solo se per ogni coppia di punti $x_0, x \in \Omega$ si ha:
\[
    f(x) \geq f(x_0) + \nabla f(x_0) \cdot (x-x_0).
\]
In due dimensioni:
\[
    f(x,y) \geq f(x_0,y_0) + \frac{\delta f}{\delta x}(x_0,y_0)(x-x_0)+\frac{\delta f}{\delta y}(x_0, y_0)(y-y_0).
\]
che geometricamente signifca che il paino tangente in $x-0, y_0$ sta sotto $f$.\newline
\newline
\textbf{Teorema di convessità e matrice Hessiana}. Sia $f \in C^2(\Omega)$, con $\Omega$ aperto convesso in $\mathbb{R}^n$. Se per ogni $x_0$ in $\Omega$ la forma quadratica $d^2f(x_0)$ è semidefinita positiva, allora $f$ è convessa in $\Omega$.
\subsubsection{Ottimizzazione di funzioni convesse e concave}
Nelle funzioni convesse (concave) i punti stazionari, se esistono, rappresentano minimi (massimi) globali. Inoltre se la funzione è strettamente convessa (concava), il punto critico è di minimo (massimo) globale forte, quindi, in particolare, è unico.
\subsection{Funzioni definite implicitamente}
\subsubsection{Funzione implicita di una variabile}
\textbf{Teorema di Dini della funzione implicita}. Sia $A$ un aperto in $\mathbb{R}^2$ e $f \;\;:\;\; A \rightarrow \mathbb{R}$ una funzione $C^1(A)$. Supponiamo che in un punto $(x_0,y_0) \in A$ sia:
\[
    f(x_0,y_0)= 0 \;\;\;\; e \;\;\;\; f_y(x_0,y_0) \neq 0.
\] 
Allora esiste un intorno $I$ di $x_0$ in $\mathbb{R}$ e un'unica funzione $g \;\;:\;\; I \rightarrow \mathbb{R}$, tale che $y_0=g(x_0)$ e
\[
    f(x,g(x)) = 0 \;\;\;\; \;\forall\;x \in I.
\]
Inoltre, $g \in C^1(I)$ e 
\[
    g'(x) = - \frac{f_x(x,g_x)}{f_y(x,g_x)} \;\;\;\; \;\forall\;x \in I.
\]
Notiamo che se $f(x_0,y_0) = 0$ e $f_y(x_0, y_0) = 0$, ma $f_x(x_0, y_0) \neq 0$, il teorema è ancora applicabile scambiando gli ruoli di $x$ e $y$.\newline
In sostanza i punti in cui il teorema di Dini non è applicabile sono quelli in cui il gradiente di $f$ si annulla, ossia i punti critici.
\subsubsection{Note sugli esercizi}
Tipicamente negli esercizi la richiesta è di calcolare la funzione $g'(x)$ in un punto. Si inizia calcolando i punti per cui $f(x,y) = 0$ (solitamente vengono forniti). Una volta trovati questi punti si deve verificare che $\frac{\delta f}{\delta y} f(x,y)$ (oppure $\frac{\delta f}{\delta x} f(x,y)$) sia $\neq 0$. Se queste condizioni si verificano, allora il Teorema di Dini è applicabile e si può procedere a calcolare $g'(x) = - \frac{\frac{\delta f}{\delta x}(x,y)}{\frac{\delta f}{\delta y} (x,y)}$ e a trovarne il valore in un punto.\newline
\newline
Se in un esercizio viene chiesto di calcolare, oltre a $g'(x)$, anche $g''(x)$, allora si può usare un altro procedimento: si può derivare rispetto a $x$ l'equazione $f(x, g(x)) = 0$ e quindi ricavare $g'(x)$. Succesivamente si può derivare ancora l'equazione e ricavare $g''(x)$. Spesso per risolvere queste equazioni è più semplice derivarle e poi sostituire $x = x_0$ con l'$x_0$ richiesto dall'esercizio.