\section{Calcolo differenziale per funzioni reali di più variabili}
\rule{\textwidth}{2pt}
\subsection{Continuità di una funzione in più variabili:}
Una funzione $f \;\;:\;\; \mathbb{R}^n \rightarrow  \mathbb{R}$ è continua in un punto $x_0$ se 
\[
    \lim_{(x,y)\rightarrow (x_0,y_0)} f(x,y) = f(x_0,y_0)
\]
La continuità di una funzione è anche deducibile dal fatto che sia costituita (somma/ prodotto/ quoziente/ certe volte anche composizione) da funzioni elementari continue.
\begin{tcolorbox}
    Per verificare la continuità di una funzione si usa spesso il concetto di differenziabilità (vedi sotto, "verifica della differenziabilità").
\end{tcolorbox}
\rule{\textwidth}{2pt}
\subsection{Calcolo di limiti in più variabili}
\rule{\textwidth}{0.4pt}
\subsubsection{Non esistenza del limite}
\begin{tcolorbox}
Per mostrare che un certa funzione in più varibili non ammette limite in un determinato punto, è sufficiente determinare due curve passasnti per il punto lungo le quali la funzione assume limiti diversi.
\end{tcolorbox}
\textbf{es.} 
\[
    \lim_{(x,y)\rightarrow (0,0)} \frac{xy}{x^2+y^2}
\]
Analiziamo la funzione lungo due curve:
\begin{itemize}
    \item con $y=x$ ottengo $f(x,x) = \frac{1}{2}$
    \item con $y=-x$ ottengo $f(x,-x) = - \frac{1}{2}$
\end{itemize}
non ammette limite.\newline
\rule{\textwidth}{0.4pt}
\subsubsection{Uso di maggiorazioni con funzioni radiali per provare l'esistenza del limite}
\begin{tcolorbox}
Per dimostrare l'esistenza di un limite per $(x,y) \rightarrow (0,0)$, si impone $x=\rho \cdot  cos(\theta)$ e $y= \rho \cdot sin(\theta)$, successivamente si pone l'intera funzione sotto modulo e si procede con semplificazioni e maggiorazioni (per eliminare i seni e i coseni). E' essenziale che la funzione non dipenda da $\theta$.\newline
Più in generale se si volesse calcolare il limite per $(x,y) \rightarrow (x_0, y_0)$ si pongono $x=x_0 +\rho \cdot  cos(\theta)$ e $y= y_0 + \rho \cdot sin(\theta)$
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Note sugli esercizi}
\begin{itemize}
    \begin{tcolorbox}
    \item Se il limite non presenta una forma di indeterminazione allora il valore cercato si ricava sostituendo direttamente il punto nella funzione.
    \end{tcolorbox}
    \begin{tcolorbox}
    \item Tecniche standard della maggiorazione: 
        \begin{itemize}
            \item disuguaglianza triangolare:
                \[
                    |a+b| \leq |a| + |b|
                \]
            \item maggiorazione di frazioni, con $a,b,c \geq 0$:
                \[
                    \frac{a}{b+c} \leq \frac{a}{b}
                \]
            \item maggiorazione di funzioni trigonometriche:
                \[
                    |cos(\theta)| \leq 1 \;\;,\;\;|sin(\theta)|\leq 1
                \]
        \end{itemize}
    \end{tcolorbox}
    \begin{tcolorbox}
    \item Il criterio che ci permette di trovare il limite richiede di trovare una funzione maggiorante di $|f|$ che sia radiale (dipenda solo da $\rho$, non $\theta$) e infinitesima. Da notare è che è possibile semplificare la funzione anche senza passare subito in coordinate polari.
    \end{tcolorbox}
    \begin{tcolorbox}
    \item Solitamente si suddivide la funzione in una serie di somme di funzioni e si studiano quest'ultime separatamente.
    \end{tcolorbox}
\end{itemize}
\rule{\textwidth}{2pt}
\subsection{Topologia in $\mathbb{R}^n$ e proprietà delle funzioni continue}
Sia $E$ un sottoinsieme di $\mathbb{R}^n$, un punto $x_0$ si dice:
\begin{itemize}
    \item interno ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E$;
    \item esterno ad $E$, se esiste un intorno centrato in $x_0$ contenuto in $E^c$;
    \item di frontiera per $E$, se ogni intorno centrato in $x_0$ contiene almeno un punto di $E$ e uno di $E^c$.
\end{itemize}
Un insieme $E \subseteq \mathbb{R}^n$ si dice:
\begin{itemize}
    \item aperto, se ogni suo punto è interno a $E$;
    \item chiuso, se il suo complementare è aperto.
\end{itemize} 
Sia $E$ un sottoinsieme di $\mathbb{R}^n$, si dice:
\begin{itemize}
    \item interno di $E$, e si indica con $E^o$, l'insieme dei punti interni di $E$;
    \item frontiera o brodo di $E$, e si indica con $\delta E$, l'insieme dei punti di frontiera di $E$;
    \item chiusura di $E$, e si indica con $\bar{E}$, l'insieme $E \cup \delta E$.
\end{itemize}
Alcune informazioni extra: 
\begin{itemize}
    \item si ha sempre $E^o \subseteq \delta E \subseteq \bar{E}$;
    \item il complementare di un aperto è chiuso e viceversa; 
    \item esistono insiemi nè aperti nè chiusi, gli unici insiemi sia aperti sia chiusi sono quello vuoto e $\mathbb{R}^n$;
    \item l'unione di una famiglia qualsiasi (anche infinita) di insiemi aperti e l'intersezione di un numero finito di insiemi aperti sono insiemi aperti 
    \item l'intersezione di una famiglia qualsiasi (anche infinita) di insiemi chiusi è l'unione di un numero finito di insiemi chiusi sono insiemi chiusi;
    \item un insieme aperto non contiene nessuno dei suoi punti di frontiera, un insieme chiuso contiene tutti i suoi punti di frontiera.
\end{itemize}
Un inieme si dice:
\begin{itemize}
    \item limitato se esiste un intorno che lo contiene tutto;
    \item connesso se per ogni coppia di punti dell'insieme, esiste un arco continuo che che li connette contenuto nell'insieme.
\end{itemize}
Proprietà topologiche delle funzioni continue:
\begin{itemize}
    \item \textbf{teor.} Teorema di Weierstrass. Sia $E \subset \mathbb{R}^n$ un insieme chiuso e limitato e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua, allora $f$ ammette massimo e minimo in $E$, cioè esistono $x_m$ e $x_M$ tali che $f(x_m) \leq f(x) \leq f(x_M)$ per ogni $x in E$.
    \item \textbf{teor.} Teorema degli zeri. Sia $E$ un insieme connesso di $\mathbb{R}^n$ e $f \;\;:\;\; E \rightarrow \mathbb{R}$ sia continua. Se $x$, $y$ sono due punti di $E$ tali che $ f(x) < 0$ e $f(y) > 0$, allora esiste un terzo punto $z \in E$ in cui $f$ si annulla. In particolare, lungo ogni arco di curva continua contenuto in $E$ che congiunge $x$ e $y$, c'è almeno un punto in cui $f$ si annulla.
    \item \textbf{teor.} Permanenza del segno. Se $f$ è continua in $(x_0, y_0) \in \mathbb{R}^2$ e $f(x_0, y_0) > 0$, allora esiste $\delta>0$ tale che $fx,y) > 0$ per ogni $(x,y) \in B_\delta(x_0,y_0)$.
\end{itemize}
\rule{\textwidth}{2pt}
\subsection{Derivate parziali, piano tangente, differenziale}
\rule{\textwidth}{0.4pt}
\subsubsection{Derivata parziale}
Calcolo di una derivata parziale tramite la definizione di rapporto incrementale in un punto $(x_0, y_0)$. \newline
Per prima cosa fissiamo $y=y_0$ e deriviamo rispetto alla $x$:
\[
    \frac{\delta f}{\delta x} (x_0,y_0) = \lim_{h\rightarrow 0}\frac{f(x_0+h, y_0)- f(x_0,y_0)}{h}.
\]
Successivamente facciamo l'opposto, cioè fissiamo $x=x_0$ e deriviamo rispetto alla $y$:
\[
    \frac{\delta f}{\delta y} (x_0,y_0) = \lim_{k\rightarrow 0}\frac{f(x_0, y_0+k)- f(x_0,y_0)}{k}
\]
\begin{tcolorbox}
Molto spesso, per calcolare una derivata parziale si può evitare di usare la definizione: per derivare rispetto a $x$ basta considerare $y$ come una costante e derivare la funzione come se fosse della sola variabile $x$.
\end{tcolorbox}
Geometricamente, le derivate parziali rappresentano le pendenze che si hanno sul fianco della montagna quando si prendono le direzioni degli assi orientati.\newline
\begin{tcolorbox}
Una funzione $f: A \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ si dice derivabile in un punto del suo dominio se in quel punto esistono tutte le sue derivate parziali; si dice derivabile in $A$ se è derivabile in ogni punto di $A$.
\end{tcolorbox}
\begin{tcolorbox}
Se $f$ è derivabile in un punto, chiameremo \textbf{gradiente} ( $\nabla f(x)$ ) il vettore delle sue derivate parziali.
\end{tcolorbox}
\begin{tcolorbox}
    \textbf{Derivabilità non implica continuità}
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Calcolo delle derivate}
\[
    \delta(\alpha \cdot f + \beta \cdot g) = \alpha \delta(f) + \beta \delta (g)
\]
\[
    \delta(f \cdot  g) = g \cdot \delta(f) + f  \cdot \delta(g)
\]
\[
    \delta(\frac{f}{g}) = \frac{ g \cdot \delta(f) - f \delta(g)}{g^2}
\]
\[
    h(x) = f(g(x)) = g \circ f \Rightarrow h'(x) = f'(g(x)) g'(x)
\]
\[
    \frac{\delta}{\delta x} [|x|] = \frac{|x|}{x} 
\]

\begin{tcolorbox}
    Per calcolare il valore di una \textbf{derivata parziale in punto $(x_0, y_0)$ secondo la definizione}, seguire questo procedimento:\newline
    Se si richiede di calcolare il valore della derivata parziale di $x$ ( cioè $\frac{\delta f}{\delta x}(x_0,y_0)$), si parte dalla funzione $f(x,y)$ e si sostituisce $y= y_0$, ottenendo quindi $f(x,y_0)$, successivamente si calcola la derivata parziale, ottenendo dunque $\frac{\delta f}{\delta x} (x,y_0)$. Come ultima cosa si sostituisce $x = x_0$ e si arriva a un risultato numerico. Per la trovare il valore della derivata parziale di $y$ in un preciso punto seguire lo stesso procedimento opposto.
\end{tcolorbox}
\begin{tcolorbox}
In alcuni esercizi è richiesto di \textbf{calcolare le derivate parziali in tutti i punti in cui esistono}. Il procedimento tipico consiste nel calcolare per prima cosa le derivate parziali generiche. Una volta calcolate sapremo che sicuramente esistono dove queste sono definite (dominio), ma non siamo sicuri dei punti in cui non lo sono (al di fuori del dominio). Quindi dobbiamo analizzare singolarmente tutti i punti al di fuori del dominio e per farlo sfruttiamo il procedimento visto sopra ("derivata parziale in un punto secondo la definizione"), calcolando esplicitamente le derivate nei punti richiesti.
Finchè si tratta per esempio di calcolarle per un punto preciso non ci sono problemi, il calcolo è facile, ma ci sono alcuni casi difficili, per esempio:
\begin{itemize}
    \item Calcolare le derivate parziali secondo la definizione lungo una retta. Per esempio in $y=0$, per calcolare la $\frac{\delta f}{\delta x}(x_0, 0)$ non ci sono problemi, si procede come al solito. Ma per la $\frac{\delta f}{\delta y} (x_0, 0)$ ci sono difficoltà, siccome non possiamo sostituire le $y$ con $0$ e poi derivare per la $y$, dobbiamo ragionare così: la derivata non esiste a meno che non ci sia un valore che le $x$ possono assumere che annullino la funzione (per gli es che ho fatto fino ad ora sono solo al numeratore). Il concetto generale è che se non si trovano valori per $x_0$ tali che annullino la funzione e quindi ci permettano di calcolare la derivata parziale, si finisce per tornare a guardare la derivata parziale generica e quindi a non trovarla per quella retta. [spiegato davvero male, ma è un concetto strano].
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}
Per \textbf{stabilire dove la funzione sia derivabile} bisogna calcolare le derivate parziali e osservarne il dominio.\newline
(n.b. tipicamente negli esercizi le funzioni sono descritte da un sistema che contiene una funzione prolungata nell'origine, in questo caso bisogna calcolare le derivate parziali al di fuori dell'origine e studiarne il dominio, in seguito bisogna calcolare il valore della derivata parziale nel punto $(0,0)$ col metodo descritto precedentemente).
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Piano tangente}
Costruire il piano tangente a una funzione in due variabili in un punto $(x_0, y_0)$:
\begin{enumerate}
    \item troviamo la retta tangente alla funzione nel piano $y=y_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0)\\
            &y=y_0 \\
        \end{cases}
    \]
    \item troviamo la retta tangente alla funzione nel piano $x=x_0$:
    \[
        \begin{cases}
            &z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)\\
            &x=x_0 \\
        \end{cases}
    \]
    \item costruiamo il piano che contiene entrambe le rette:
    \begin{tcolorbox}
    \[
        z = f(x_0, y_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (x-x_0) + \frac{\delta f}{\delta x}(x_0, y_0) \cdot (y-y_0)
    \]
    \end{tcolorbox}
\end{enumerate}
Il procedimento appena mostrato individua il piano tangente nell'ipotesi che esso esista, potrebbe però non esserci.\newline
\rule{\textwidth}{0.4pt}
\subsubsection{Differenziabilità e approssimazione lineare}
\begin{tcolorbox}
In due o più variabili la sola derivabilità non implica nè continuità nè l'esistenza del piano tangente.
\end{tcolorbox}
Concetto di differenziabilità in più variabili: l'incremento di $f$ è uguale all'incremento calcolato lungo il piano tangente, più un infinitesimo di ordine superiore rispetto alla lunghezza dell'incremento $(h,k)$ delle variabili indipendenti. In formule:
\[
    f(x_0 + h, y_0 + k) - f(x_0, y_0) = \frac{\delta f}{\delta x} (x_0,y_0) \cdot (x-x_0) + \frac{\delta f}{\delta y}(x_0, y_0) \cdot (y-y_0) + o(\sqrt{h^2 + k^2})
\]
per $(h,k) \rightarrow  (0,0)$.\newline
Tutto ciò che è prima dell'uguale (primo membro) rappresenta l'incremento della funzione, i primi due addendi del secondo membro rappresentano l'incremento calacolato lungo il pinao tangente. Ricordiamo che l'ultimo addendo rappresenta una funzione tale che $\lim_{(h,k)\rightarrow (0,0)}\frac{o(\sqrt{h^2 + k^2})}{\sqrt{h^2 + k^2}} = 0$.\newline
Se l'equazione di prima è soddisfatta, diremo che la funzione è differenziabile in $(x_0, y_0)$.\newline

\begin{tcolorbox}
    Una funzione $f: \mathbb{R}^2 \rightarrow  \mathbb{R}$ si dice \textbf{differenziabile} in $(x_0, y_0)$ se è derivabile e se
    \[
        \lim_{(h,k)\rightarrow (0,0)} \frac{f(x_0 + h, y_0+k)-f(x_0,y_0) - h f_x(x_0, y_0) - k f_y(x_0,y_0)}{\sqrt{h^2 + k^2}} = 0
    \]
    (Ricordiamo che $h = x-x_0$ e $k = y-y_o$)
\end{tcolorbox}
Questa scrittura dice che "il grafico della superficie si allontana dal piano tangente con un ordine di infinitesimo superiore a quello della distanza dal punto".
\newline
\begin{tcolorbox}
\[
    f \;\text{differenziabile in $(x_0, y_0)$}\; \Rightarrow f \; \text{continua in $(x_0,y_0)$}\;
\]
\end{tcolorbox}
Da notare che la differenziabilità implica la derivabilità, cioè se una funzione è differenziabile in un punto, allora è anche derivabile nello stesso.\newline

Se $f$ è differenziabile in $x_0$, si dice differenziale di $f$ calcolato in $x_0$ la funzione lineare $df(x_0) \;:\; \mathbb{R}^n \rightarrow  \mathbb{R}$ definita da:
\[
    df(x_0) \;\;:\;\;h \rightarrow  \nabla f(x_0) \cdot h.
\] 
Nel caso in due varibiali, il numero $\nabla f(x_0) \cdot h$ rappresenta l'incremento della funzione nel passare da $x_0$ a $x_0+h$, calcolato lungo il piano tangente al grafico di $f$ in $x_0$.\newline
\begin{tcolorbox}
L'approssimazione dell'incremento di $f$ con il suo differenziale prende il nome di linearizzazione.
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Verifica della differenziabilità}
Per dimostrare la differenziabilità in un punto $(x_0,y_0)$ bisogna provare che:
\[
    \lim_{h,k\rightarrow 0,0}\frac{f(x_0+h, y_0+k)-\{
        f(x_0,y_0)+ \frac{\delta f}{\delta x}(x_0,y_0) h + \frac{\delta f}{\delta y}(x_0,y_0)k
        \}}{\sqrt{h^2+k^2}} =0.
\]
dove $h = x-x_0$ e $k = y-y_0$.\newline
Ma per certi casi particolari esistono criteri molto più comodi e semplici.

\begin{tcolorbox}
Teorema di condizione sufficiente di differenziabilità: se le derivate parziali di $f$ esistono in un intorno di $x_0$ e sono continue in $x_0$, allora $f$ è differenziabile in $x_0$.\newline
In particolare se le derivate parziali esistono e sono continue in tutto $A$, allora $f$ è differenziabile in tutto $A$.\newline
Una funzione le cui derivate parziali esistono e sono continue in tutto $A$ si dice di classe $C^1(A)$, dunque: $f \in C^1(A) \rightarrow$ f differenziabile in $A$.
\end{tcolorbox}

Negli esercizi spesso si usa anche l'omogeneità di una funzione per sapere se essa è differenziabile o continua, oppure le proprietà delle funzioni radiali.\newline

Negli esercizi seguire quest'ordine:
\begin{tcolorbox}
\begin{itemize}
    \item E' continua nel punto richiesto? se non lo è, può essere allungata?
    \item Funzione radiale? (vedi più avanti)
    \item Funzione omogenea? (vedi più avanti)
    \item Calcolo delle derivate parziali nel punto. Sono continue in quel punto?
    \item Verifica della differenziabilità tramite la definizione.
\end{itemize}
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Derivate direzionali}
\begin{tcolorbox}
Si dice derivata direzionale della funzione $f$ rispetto al versore $v$, nel punto $x_0$, il limite
\[
    D_vf(x_0) = \lim_{t\rightarrow 0}\frac{f(x_0 + tv) - f(x_0)}{t}
\]
purchè esista finito.
\end{tcolorbox}
Detto in maniera diversa signifca considerare la restrizione della funzione $f$ alla direzione della retta passante per $x_0$ con direzione $v$, cioè $g(t)= f(x_0+tv)$, e calcolarne la derivata, cioè $D_vf(x_0)= g'(0)$.\newline
\begin{tcolorbox}
Calcolo di una derivata direzionale per un generico vettore $(cos(\theta), sin(\theta))$ nell'origine di una funzione $f$: Per prima cosa si ottiene la funzione $g(t) = f(t \cdot cos(\theta), t \cdot sin(\theta))$ e la si semplifica per $t \rightarrow 0$ (anche usando asintotici). In seguito si studia la derivata $g'(0) = \frac{\delta f}{\delta t}(t \cdot cos(\theta), t \cdot sin(\theta))$.\newline
Se è richiesto il calcolo in un punto generico, e non nell'origine, è sufficiente usare $t \cdot cos(\theta) + x_0$ e $t \cdot sin(\theta) + y_0$.
\end{tcolorbox}
Se la funzione è differenziabile, allora le derivate parziali consentono di calcolare tutte le altre derivate direzionali.
\begin{tcolorbox}
\textbf{Formula del gradiente}: 
\[
    D_vf(x_0) = \nabla f(x_0) \cdot v =  \sum_{i=1}^{n} \frac{\delta f}{\delta x_i}(x_0) \cdot  v_i
\]
Cioè la derivata direzionale è il prodotto scalare del gradiente con il versore nella direzione in cui si deriva, quindi tutte le derivate direzionali sono combinazioni lineari delle derivate parziali. Nel caso in due variabili la formula si riduce a $D_vf(x_0) = \nabla f(x_0,y_0) \cdot v = \frac{\delta f}{\delta x} (x_0,y_0)cos(\theta) + \frac{\delta f}{\delta y}(x_0, y_0) sin(\theta)$.\newline
Se la formula del gradiente non vale in un punto, allora la funzione non è differenziabile in quel punto.\newline
Inoltre la formula del gradiente non vale se la generica derivata direzionale non è combinazione lineare di $cos(\theta), sin(\theta)$.
\end{tcolorbox}

\begin{tcolorbox}
Da notare è che $\nabla f(x_0)$ indica la \textbf{direzione di massima crescita} di $f$, ossia la direzione di massima derivata direzionale, invece $-\nabla f(x_0)$ rappresenta la direzione di minima derivata direzionale, infine nelle direzioni ortogonali al gradiente le derivate direzionali sono nulle, quindi di \textbf{pendenza nulla}.
\end{tcolorbox}
\begin{tcolorbox}
Il gradiente è ortogolane il ogni punto alle linee di livello.
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Riepilogo}
\begin{itemize}
    \item $f \in C^1(A) \Rightarrow f$ differenziabile in $A$ (cioè $f$ ha iperpiano tangente) $\Rightarrow f$ è continua, derivabile, ha derivate direzionali, vale la formula del gradiente.
    \item $f$ continua, derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ differenziabile
    \item $f$ derivabile, dotata di tutte le derivate direzionali $\nRightarrow f$ continua 
\end{itemize}
\rule{\textwidth}{0.4pt}
\subsubsection{Gradiente di una funzione radiale}
Si chiama funzione radiale una funzione $h$ che dipende solo dalla distanza di dall'origine, ossia
\[
    h(x) = g(|x|).
\]
ponendo $\rho = |x| = \sqrt{\sum_{j=1}^{n}x_j^2}$ si ha:
\[
    \nabla_\rho = ( \frac{x_1}{\rho}, \frac{x_2}{\rho}, \dots, \frac{x_n}{\rho}).
\]
\[
    \nabla h(x) = g'(|x|)( \frac{x_1}{|x|}, \dots, \frac{x_n}{|x|})
\]
\[
    |\nabla h(x)| = |g'(|x|)|
\]
Le funzioni radiali sono spesso utilizzate negli esercizi in cui le incognite compaiono solo all'interno del termine $\sqrt{\sum_{j=1}^{n}x_j^2}$, in tal caso si ottiene $g(\rho)$ sostituendo ogni $\sqrt{\sum_{j=1}^{n}x_j^2}$ con $ \rho$, successivamente si può procedere sfruttando le proprietà di continuità e differenziabilità delle funzioni radiali.\newline
\rule{\textwidth}{0.4pt}
\subsubsection{Criterio di continuità e differenziabilità per funzioni radiali}
Sia $f \;\;:\;\; \mathbb{R}^n-\{0\} \rightarrow  \mathbb{R}$ una funzione radiale, cioè $f(x) = g(|x|)$ con $g \;\;:\;\;(0, +\infty) \rightarrow \mathbb{R}$ e sia $f$ continua fuori dall'origine. Allora:
\begin{itemize}
    \item $f$ è continua in $0$, se e solo se esiste finito $\lim_{\rho\rightarrow 0^+}g(\rho)$;
    \item $f$ è differenziabile in $0$ se e solo se esiste $g'(0)=0$.
\end{itemize}
Negli esercizi spesso si controlla prima la continuità nell'origine, se non lo è si allunga la funzione e successivamente si calcola la differenziabilità nell'origine.\newline
\rule{\textwidth}{0.4pt}
\subsubsection{Funzioni omogenee}
Una funzione $f \;\;:\;\; \mathbb{R}^n \rightarrow \mathbb{R}$ (eventualmente definita solo per $x\neq 0$), non identicamente nulla, si dice positivamente omogenea di grado $\alpha \in \mathbb{R}$ se
\[
    f(\lambda x) = \lambda^\alpha f(x) \;\;\;\;\; \;\forall\;x \in \mathbb{R}^n, x\neq 0, \lambda>0.
\]
La funzione $f$ si dice omogenea di grado $\alpha$ se la formula di prima vale anche per $\lambda<0$.\newline
Se $f$ è positivamente omogenea vale
\[
    f(x) = f( |x| \cdot \frac{x}{|x|}) = |x|^\alpha f(\frac{x}{|x|}).
\]
In particolare se $f$ è omogenea (o positivamente omogenea) di grado zero, significa che è costante su ogni retta (o semiretta) uscente dall'origine. Infatti, indicata con
\[
    r(t)=tv
\]
con $v$ versore fissato, sarà
\[
    f(r(t))=f(tv)=t^0f(v)=f(v)=costante.
\]
Più in generale, per una funzione in due variabili positivamente omogenea di grado $\alpha$ vale la seguente rappresentazione in coordinate polari:
\[
    f(\rho, \theta)=\rho^\alpha g(1,\theta)
\]
per qualche $\alpha \in \mathbb{R}$ e qualche funzione $g \;\;:\;\; [0, 2\pi) \rightarrow \mathbb{R}$.\newline

Sia $f \;\;:\;\; \mathbb{R}^n \rightarrow \mathbb{R}$ una funzione positivamente omogenea di grado $\alpha$, definita e continua per $x\neq 0$. Allora:
\begin{itemize}
    \item $f$ è continua anche nell'origine se $\alpha>0$; in questo caso $f(0) = 0$; $f$ è discontinua nell'origine se $\alpha<0$; è discontinua anche se $\alpha=0$, tranne il caso banale in cui $f$ è costante.
    \item $f$ è differenziabile nell'origine se $\alpha>1$; non è differenziabile nell'origine se $\alpha <1$, tranne il caso banale in cui $\alpha=0$ e $f$ è costante; se $\alpha=1$, $f$ è differenziabile se e solo se è una funzione lineare, (ossia $f(x) = a \cdot x$ per qualche vettore costante $a \in \mathbb{R}^n$).
\end{itemize}

Si ricordi che ogni ogni derivata parziale prima di una funzione posivamente omogenea di grado $\alpha$, se esiste, è una funzione positivamente omogenea di grado $\alpha -1$. \newline
\rule{\textwidth}{0.4pt}
\subsubsection{Equazione del trasporto}
Si definisce equazione del trasporto la seguente:
\[
    c \frac{\delta u}{\delta x} + \frac{\delta u}{\delta t} = 0 \;\;\;\;\; \;\;\;\;\; (?)
\]
\newline
Teorema del valor medio. Sia $A \subset \mathbb{R}^n$ un aperto e $f \;\;:\;\; A \rightarrow \mathbb{R}$ una funzione differenziabile in $A$. Allora per ogni coppia di punti $x_0, x_1 \in A$, esiste un punto $x^*$ tale per cui:
\[
    f(x_1)- f(x_0) = \nabla f(x^*) \cdot  (x_1 + x_0).
\]
In particolare:
\[
    |f(x_1) - f(x_0)| \leq |\nabla f(x^*)| \cdot |(x_1 + x_0)|.
\]
\rule{\textwidth}{2pt}

\subsection{Derivate di ordine superiore e approssimazioni successive}
\rule{\textwidth}{0.4pt}
\subsubsection{Derivate di ordine superiore}
\begin{tcolorbox}
\textbf{Teor. di Schwartz}. Sia $f \;\;:\;\; A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$ con $A$ aperto. Supponiamo che (per certi indici $i,j \in \{1,2,\dots,n\}$) le derivate seconde miste $f_{x_i, x_j}$ e $f_{x_j, x_i}$ esistano in un certo $x_0$ e siano continue in $x_0$; allora esse coincidono in $x_0$.\newline
\end{tcolorbox}
Una funzione che ha tutte le derivate parziali seconde continue in un aperto $A$ si dice di classe $C^2(A)$.\newline
Se $f \in C^2(A)$, allora $f \in C^1(A)$ (in particolare $f$ è differenziabile), le derivate parziali prime sono differenziabili, le derivate parziali seconde sono continue, le derivate seconde miste sono uguali.
\rule{\textwidth}{2pt}
\subsection{Differenziale secondo, matrice hessiana, formula di Taylor al secondo ordine}
Se $f \in C^2(A)$ e $x_0 \in A$, si dice differenziale secondo di $f$ in $x_0$ la funzione
\[
    d^2f(x_0) \;\;:\;\; h \rightarrow \sum_{i=1}^{n}\sum_{j=0}^{n}\frac{\delta^2(f)}{\delta(x_i) \delta(x_j)}(x_0)h_ih_j.
\]
I vari coefficienti $\frac{\delta^2(f)}{\delta(x_i) \delta(x_j)}(x_0)$ possono essere ordinati in una matrice detta Hessiana:
\[
    H_f(x_0) =\left(\begin{matrix}
        f_{x_1x_1}(x_0) \;\; &f_{x_1x_2}(x_0) \;\; &\dots \;\; &f_{x_1x_n}(x_0)\\
        f_{x_2x_1}(x_0) \;\; &f_{x_2x_2}(x_0) \;\; &\dots \;\; &f_{x_2x_n}(x_0)\\
        \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
        f_{x_nx_1}(x_0) \;\; &f_{x_nx_2}(x_0) \;\; &\dots \;\; &f_{x_nx_n}(x_0)
    \end{matrix}\right)
\]  
\begin{tcolorbox}
In particolare, per due variabili:
\[
    H_f(x_0, y_0) = \left(\begin{matrix}
        f_{xx}(x_0, y_0) \;\; f_{xy}(x_0, y_0)\\
        f_{yx}(x_0, y_0) \;\; f_{yy}(x_0, y_0)
    \end{matrix}\right)
\]
Se $f$ è di classe $C^2$, la matrice Hessiana è simmetrica.
\end{tcolorbox}
Formula di Taylor (resto secondo Lagrange). Sia $f \in c^2(A)$; per ogni $x_0 \in A$ e $h \in \mathbb{R}^n$, tale che $x_0 + h \in A$, esiste un numero reale $\delta \in (0,1)$, dipendente da $x_0$ e $h$, tale che:
\[
    f(x_0 + h) = f(x_0) +\sum_{i=1}^{n}\frac{\delta f}{\delta x_i}(x_0)h_i + \frac{1}{2}\sum_{i,j=1}^{n}\frac{\delta^2f}{\delta x_i \delta x_j}(x_0 + \delta h) h_i h_j.
\]\newline

Formula di Taylor (resto secondo Peano). Sia $f \in C^2(a)$. Per ogni $x_0 \in A$ vale la formula:
\[
    f(x_0 + h) = f(x_0) +\sum_{i=1}^{n}\frac{\delta f}{\delta x_i}(x_0)h_i + \frac{1}{2}\sum_{i,j=1}^{n}\frac{\delta^2f}{\delta x_i \delta x_j}(x_0) h_i h_j +o(|h|^2).
\]
\begin{tcolorbox}
    \textbf{Formula di Taylor al secondo ordine} (resto secondo peano):\newline
    Sia $f$ di classe $C^2$ in $(x_0,y_0) \in \mathbb{R}^2$. Allora, per $(x,y) \rightarrow  (x_0, y_0)$ si ha 
    \[
        f(x,y) = f(x_0,y_0) + \nabla f(x_0,y_0) \cdot \binom{x-x_0}{y-y_0} + 
    \]
    \[
        +\frac{1}{2} H_f(x_0,y_0) \binom{x-x_0}{y-y_0} \cdot \binom{x-x_0}{y-y_0} + o[(x-x_0)^2 + (y-y_0)^2]
    \]
    dove l'operazione indicata con $\cdot $ è un prodotto scalare, mentre l'operazione $H_f(x_0,y_0) \binom{x-x_0}{y-y_0}$ è un prodotto matriciale righe per colonne.
\end{tcolorbox}
\begin{tcolorbox}
    Formula di Taylor (resto secondo Peano) scritta per esteso:
    \[
        f(x,y) = f(x_0, y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0) +
    \]
    \[
        + \frac{1}{2}\left( f_{xx}(x_0,y_0)(x-x_0)^2 + 2f_{xy}(x_0,y_0)(x-x_0)(y-y_0) + f_{yy}(x_0,y_0)(y-y_0)^2 \right) +
    \]
    \[
        + o[(x-x_0)^2 + (y-y_0)^2]
    \]
    per $(x,y) \rightarrow (x_0,y_0)$.
\end{tcolorbox}

\rule{\textwidth}{2pt}
\subsection{Ottimizzazione, estremi liberi}
\rule{\textwidth}{0.4pt}
\subsubsection{Generalità sui problemi di ottimizzazione}
\begin{itemize}
    \item $x_0$ è detto punto di massimo (minimo) globale se per ogni $x$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$);
    \item $x_0$ è detto punto di massimo (minimo) locale se esiste un intorno di $x_0$ detto $U$ tale per cui per ogni $x \in U$ si ha $f(x) \leq f(x_0)$ ($f(x) \geq f(x_0)$).
\end{itemize}
\rule{\textwidth}{0.4pt}
\subsubsection{Estremi liberi, condizioni necessarie del prim'ordine}
\textbf{Teor. di Fermat}. Sia $f \;\;:\;\; A \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$, con $A$ aperto e $x_0 \in A$ un punto di massimo o minimo locale per $f$. Se $f$ è derivabile in $x_0$, allora $\nabla f(x_0) = 0$.\newline
\begin{tcolorbox}
    I punti in cui il gradiente di una funzione si annulla si dicono punti \textbf{critici} o \textbf{stazionari} di $f$. Una volta individuati tutti i punti stazionari, si può iniziare un'analisi su di essi per verificare se sono o meno punti di massimo o minimo. Se non lo sono essi prendono il nome di punti di \textbf{sella} o \textbf{colle}. Da notare particolarmente è che una funzione può assumere valori di massimo o minimo anche in punti in cui non è derivabile, dunque questi punti vanno analizzati separatamente.
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Forme quadratiche, classificazione}
Un modo per determinare la natura di un punto stazionario è quello di analizzare il segno dell'incremento $\nabla f(x_0) = f(x_0 + h ) - f(x_0)$. \textbf{Se infatti si riesce a stabilire che $\nabla f(x_0)$ si mantiene di segno positivo o negativo, per ogni $h$ di modulo abbastanza piccolo, possiamo dedurre che $x_0$ è punto di minimo o massimo locale}. Se invece al variare di $h$, $\nabla f(x_0)$ cambia segno, siamo in presenza di un punto si sella.\newline
\begin{tcolorbox}
Lo studio del segno di $\nabla f(x_0)$ riconduce all'analisi del segno del polinomio omogeneo di secondo grado nelle componenti di $h$ (che prende il nome di \textbf{forma quadratica}) dato da
\[
    \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j.
\]
Ogni forma quadratica risulta associata a una matrice simmetrica $M$. Nel caso del differenziale la matrice $M$ coincide con la matrice Hessiana.
\end{tcolorbox}
\begin{tcolorbox}
Il segno della forma quadratica è quindi studiabile analizzando la sua matrice $M = \left(\begin{matrix}
    a \;\;\; b\\
    b \;\;\; c\\
\end{matrix} \right)$ con $a\neq 0$ nel seguente modo:
\begin{itemize}
    \item è definitivamente positiva (negativa) se e solo se $det(M)>0$ e $a >0$ ($a<0$);
    \item indefinita se $det(M)<0$;
    \item semidefinita positiva (negativa) se e solo se $det(M)=0$ e $a>0$ ($a<0$).
\end{itemize}
Se $a = 0$ e $c\neq 0$, nelle affermazioni precedenti occore sostituire $a$ con $c$.
\end{tcolorbox}
\rule{\textwidth}{0.4pt}
\subsubsection{Forme quadratiche, test degli autovalori}
Un importante test per determinare il segno di una funzione quadratica in $\mathbb{R}^n$ è basato sul segno degli autovalori della matrice $M$.\newline
Ricordiamo che un numero complesso $\lambda$ e un vettore non nullo $v \in \mathbb{C}^n$. Si dicono, rispettivamente, \textbf{autovalore e autovettore} (di $\lambda$) di una matrice $M$ di ordine $n$, se soddisfano la relazione:
\[
    Mv = \lambda v
\]
oppure
\[
    (M-\lambda I_n)v = 0.
\]
Quest'ultima equazione ha soluzioni $v$ non nulle se e solo se la matrice dei coefficienti e singolare, ovvero se $\lambda$ è soluzione dell'equazione caratteristica:
\[
    det(M-\lambda I) = 0
\]
esistono esattamente $n$ autovalori di $M$ ciascuno contato secondo la propria molteplicità.\newline
Le matrici $M$ simmetriche hanno prorpietà importanti:
\begin{itemize}
    \item gli autovalori di $M$ sono reali e possiedono autovettori reali;
    \item esistono $n$ autovettori lineari che costituiscono una base ortonormale in $\mathbb{R}^n$;
    \item La matrice $S = {w_1, w_2, \dots, w_n}$ le cui colonne sono gli autovettori lineari è orotognale e diagonalizza $M$, precisamente:
    \[
        S^TMS = \Lambda = \begin{matrix}
            \lambda_1 \;\; &0 \;\; &\dots \;\; &0\\
            0 \;\; &\lambda_2 \;\; &\dots \;\; &0\\
            \dots \;\; &\dots \;\; &\dots \;\; &\dots\\
            0 \;\; &0 \;\; &\dots \;\; &\lambda_n
        \end{matrix}
    \]
\end{itemize}
\begin{tcolorbox}
Tornando allo studio del segno della forma quadratica con la sua matrice $M$:
\begin{itemize}
    \item definitivamente positiva (negativa) se e solo se tutti gli autovalori di $M$ sono positivi (negativi);
    \item semidefinita positiva (negativa) se e solo se tutti gli autovalori di $M$ sono $\geq 0$ ($\leq 0$) e almeno uno di essi è nullo;
    \item indefinita se $M$ ha almeno un autovalore positivo e uno negativo.
\end{itemize}
\end{tcolorbox}
Da notare è che per una forma quadratica l'origine è sempre un punto stazionario. \newline
\rule{\textwidth}{0.4pt}
\subsubsection{Studio della natura dei punti critici}
Per estrarre informazioni su un punto critico $x_0$ occorre studiare e classificare la forma quadratica 
\[
    q(h) = \sum_{i,j=1}^{n}f_{x_ix_j}(x_0)h_ih_j = h^TH_f(x_0)h
\]
dove $H_f(x_0)$ è la matrice Hessiana di $f$ in $x_0$.\newline
\begin{tcolorbox}
\begin{itemize}
    \item se la forma quadratica è definitivamente positiva (negativa), allora $x_0$ è un punto di minimo (massimo) locale forte;
    \item se la forma quadratica è indefinita, allora $x_0$ è un punto di sella;
    \item se la forma quadratica è in $x_0$ semidefinita positiva (negativa) e non nulla, allora $x_0$ è di minimo (massimo) debole oppure di colle; la situazione cambia se la forma quadratica è semidefinita positiva (negativa) non solo in $x_0$, ma anche per ogni $x$ in un intorno di $x_0$, f è convessa (concava), dunque un punto di minimo (massimo) debole;
    \item se la forma quadratica è nulla, allora non possiamo estrarne informazioni significanti.
\end{itemize}
\end{tcolorbox}
\begin{tcolorbox}
Vediamo una strategia da seguire:
\begin{enumerate}
    \item si isolano i punti di $f$ che non sono regolari (es. non derivabili una o due volte). Questi punti dovranno essere analizzati separatamente;
    \item trovare i punti critici risolvendo:
        \[
            \begin{cases}
                &f_{x_1}(x_1,x_2,\dots,x_n) = 0\\
                &f_{x_2}(x_1,x_2,\dots,x_n) = 0\\
                &\dots\\
                &f_{x_n}(x_1,x_2,\dots,x_n) = 0\\
            \end{cases}
        \]
    \item si studia il segno della forma quadratica per ogni punto critico, se è definita o indefinita si giunge a una conclusione con le regole dette precedentemente, se è nulla o semidefinita si ricorre a uno studio diretto di $\nabla f(x_0)$ in un intorno di $x_0$.
\end{enumerate}
Più precisamente, nel caso bidimensionale, per ogni punto critico:
\begin{enumerate}
    \item si calcola l'Hessiana:
        \[
            H_f(x_0, y_0) = \begin{matrix}
                f_{xx}(x_0,y_0) \;\;\; & f_{xy}(x_0,y_0)\\
                f_yx(x_0,y_0) \;\;\; & f_{yy}(x_0,y_0)\\
            \end{matrix}
        \]
    \item se $det(H_f(x_0, y_0)) > 0$ e
        \begin{itemize}
            \item $f_{xx}(x_0,y_0)>0$ allora $(x_0, y_0)$ è di minimo locale forte;
            \item $f_{xx}(x_0,y_0)<0$ allora $(x_0, y_0)$ è di massimo locale forte;
        \end{itemize}
        (si noti che in questo caso $f_{xx}(x_0,y_0)$ e $f_{yy}(x_0,y_0)$ hanno lo stesso segno).
    \item se $det(H_f(x_0, y_0)) < 0$ allora $(x_0, y_0)$ è punto di sella;
    \item se $det(H_f(x_0, y_0)) = 0$ occore un analisi ulteriore. Si possono percorrere due vie: 1.si studiano le derivate successive (terza e quarta) [sconsigliato]; 2. si affronta il problema direttamente analizzando il segno di $f(x,y) - f(x_0, y_0)$
\end{enumerate}
\end{tcolorbox}
\rule{\textwidth}{2pt}
\subsection{Ottimizzazione. Estremi vincolanti}
\rule{\textwidth}{0,4pt}
\subsubsection{Vincoli di uguaglianza e moltiplicatori di Lagrange. Funzioni di due variabili}
\textbf{Problema:} Date due funzioni $f= f(x,y)$ e $g = g(x,y)$, dotate di derivate parziali continue in $\mathbb{R}^2$. Si vogliono determinare gli estremi di $f$ sotto la condizione di vincolo $g(x,y) = b$, $b \in \mathbb{R}^2$.\newline
Il problema consiste nel massimizzare (minimizzare) la restrizion di $f$ al vincolo specificato.
\begin{tcolorbox}
\begin{itemize}
    \item \textbf{vincolo esplicitabile}: caso in cui il vincolo $g(x,y) = b$ definisce esplicitamente $y=y(x)$ o $x = x(y)$ oppure, più in generale, definisce le equazioni parametriche ($x= x(t), y = y(t)$) di una curva $\gamma$. Il problema allora è ricondotto alla ricerca degli estremi di una funzione reale di variabili reali
    \[
        \phi(t) = f(x(t), y(t))
    \]
    \item \textbf{metodo dei moltiplicatori di Lagrange}: utilizzato quando il vincolo è rappresentato da una curva regolare assegnata in qualsiasi forma (parametrica, cartesiana o implicita).\newline
    \textbf{teor.} Teorema dei moltiplicatori di lagrange. Siano $f,g \in C^1(\mathbb{R}^2)$ e $(x^*, y^*)$ punto di estremo vincolato per $f$ sotto il vincolo:
    \[
        g(x,y) = b
    \]
    Se $(x^*, y^*)$ è regolare per il vincolo, cioè $\nabla g(x^*, y^*) \neq (0,0)$, allora esiste $\lambda^* \in \mathbb{R}$ (detto moltiplicatore di Lagrange) tale che:
    \[
        \nabla f(x^*, y^*) = \lambda \nabla g(x^*, y^*)
    \]
    Questa formula esprime il fatto che se $(x^*, y^*)$, verifica le ipotesi del teorema, allora la derivata di $f$ lungo la tangente al vincolo si deve annullare e in tal caso diciamo che $(x^*, y^*)$ è punto critico vincolato.\newline
    \textbf{oss.} Introducendo la funzione $L = L (x, y , \lambda)$, detta Lagrangiana, definita da
    \[
        L(x,y,\lambda) = f(x,y) - \lambda[g(x,y) - b]
    \] 
    il teorema afferma che se $(x^*, y^*)$ è punto di estremo vincolato, allora esiste $\lambda^*$ tale che il punto $(x^*, y^*, \lambda^*)$ sia punto critico libero per $L$. Infatti i punti critici in $L$ sono soluzioni del sistema:
    \[
        \begin{cases}
            L_x = f_x - \lambda g_x  = 0\\
            L_y = f_y - \lambda g_y = 0\\
            L_\lambda = b-g = 0
        \end{cases}
    \]
    Metodo dei moltiplicatori di Lagrange:
    \begin{itemize}
        \item Si isolano gli eventuali punti non regolari dell'insieme $g(x,y) = b$, che vanno esaminati a parte.
        \item si cercano i punti critici liberi della Lagrangiana e cioè soluzioni del sistema visto nell'osservazione.
        \item si determina la natura dei punti critici. A questo proposito risulta spesso utile il teorema di Weierstrass.
    \end{itemize}
\end{itemize}
\end{tcolorbox}
\rule{\textwidth}{0,4pt}
\subsubsection{Moltiplicatore di Lagrange. Il caso generale}
Consideriamo ora problemi di ottimizzazione vincolata, per funzioni di $n$ variabili ($n \geq 2$) con un numero $m$ di vincoli di uguaglianza.\newline
Siano $m+1$ funzioni reali di $n$ variabili $f,g_1,g_2,\dots,g_m : \mathbb{R}^n \rightarrow \mathbb{R}$ tutte $C^1(\mathbb{R}^n)$. Si vogliono determinare gli estremi di $f$ quando le variabili sono soggette alle $m$ condizioni di vincolo:
\[
    \begin{cases}
        g_1(x_1,\dots,x_n) = b_1\\
        \dots\\
        g_m(x_1,\dots,x_n) = b_m
    \end{cases}
\]
\textbf{teor.} Teorema dei moltiplicatori di Lagrange, caso generale.\newline
Sia $f, g_1, \dots, g_m \in C^1(\mathbb{R}^n)$, $\vec{g} = (g_1, g_2, \dots, g_m)$ e sia $\vec{x}^*$ punto di estremo vincolato per $f$ rispetto al vincolo
\[
    \vec{g}(\vec{x}) = \vec{b}
\] 
Se $\vec{x}^*$ è un punto regolare per $\vec{g}$, cioè se il rango di $D \vec{g}(\vec{x}^*)$ è $m$, allora esistono $m$ numeri reali $\lambda_1^*, \dots, \lambda_m^*$, detti moltiplicatori di Lagrange, tali che
\[
    \nabla f(\vec{x}^*) = \sum_{j=1}^{m}\lambda_j^* \nabla g_j (\vec{x}^*)
\]
\newline
Introduciamo la funzione Lagrangiana
\[
    L(x_1, \dots,x_n, \lambda_1, \dots \lambda_m) =f(x_1, \dots, x_n) - \sum_{j=1}^{m} \lambda_j[g_j(x_1,\dots,x_n) - b_j]
\]
dipendente da $n+m$ variabili. Se $\vec{x}^*$ è un punto regolare, allora esiste un vettore di moltiplicatori
\[
    \vec{\lambda}^* =(\lambda_1^*, \dots, \lambda_m^*) 
\]
tale che $(\vec{x}^*, \vec{\lambda}^*)$ è un punto critico libero per la Lagrangiana. Infatti i punti critici liberi per la Lagrangiana si trovano risolvendo il seguente sistema di $n+m$ equazioni in $n+m$ incognite:
\[
    \begin{cases}
        D_{x_1} L = D_{x_1}f - \sum_{j=1}^{m} \lambda_j D_{x_1}g_j = 0\\
        \dots\\
        D_{x_n} L = D_{x_n}f - \sum_{j=1}^{m} \lambda_j D_{x_n} g_j = 0\\
        D_{\lambda_1}L = b_1 - g_1 = 0\\
        \dots\\
        D_{\lambda_m}L = b_m - g_m= 0
    \end{cases}
\]
\rule{\textwidth}{0,4pt}
\subsubsection{Metodo dei moltiplicatori di Lagrange (dal libro di Gazzola)}
\begin{tcolorbox}
Siano $f,g \in C^1$ e supponiamo di volere ottimizzare (massimizzare o minimizzare) la funzione $z = f(x,y)$ sotto il vincolo $g(x,y) = 0$ (se fosse $g(x,y) = b$ possiamo trasformarla in $g(x,y) - b = 0$). Introduciamo la funzione $L : \mathbb{R}^3 \rightarrow \mathbb{R}$ definita da
\[
    L(x,y,\lambda) = f(x,y) - \lambda g(x,y)
\]
e che si chiama \textbf{Lagrangiana}. Se cerchiamo i punti stazionari (in $\mathbb{R}^3$!) della funzione $L$ siamo portati a risolvere il sistema $\nabla L = 0$ e cioè
\[
    \begin{cases}
        f_x(x,y) = \lambda g_x(x,y) \\  
        f_y (x,y) = \lambda g_y(x,y)\\
        g(x,y) = 0
    \end{cases}
\]
Chiaramente da queste equazioni non distinguiamo gli eventuali massimi dai minimi e non è nemmeno detto che una soluzione sia un estremo relativo, tuttavia 
\textbf{Gli eventuali punti di ottimo di $f$ soggetti al vincolo $g = 0$ vanno cercati tra le soluzioni di queste equazioni}.\newline
Per capire quali dei punti che sono soluzione del sistema sono estremi, è sufficiente calcolare i valori della funzione in tali punti e valutare se sono massimi o minimi.
\end{tcolorbox}
\begin{tcolorbox}
Il teorema di Weierstrass può essere di aiuto per garantire l'esistenza dei massimi e dei minimi: Sia $[a,b]\subset \mathbb {R}$ un intervallo chiuso e limitato non vuoto e sia $f\colon [a,b]\to \mathbb {R}$ una funzione continua. Allora $f(x)$ ammette (almeno) un punto di massimo assoluto e un punto di minimo assoluto nell'intervallo $[a,b]$.
\end{tcolorbox}
\begin{tcolorbox}
Notiamo che il valore esatto del moltiplicatore $\lambda$ non è essenziale: permette di trovare i punti candidati, ma non serve per calcolare il livello delal funzione.
\end{tcolorbox}
\begin{tcolorbox}
Un punto più delicato diguarda i punti in cui i vettori delle derivate parziali di $f$ e di $g$ ($\nabla f \;\;e \;\; \nabla g$) si annullano. Se si annulla $\nabla f$ troveremo un moltiplicatore nullo e saremo in presenza di un punto critico libero di $f$ che posiamo tranquillamente inserire tra i candidati e valutarlo insieme agli altri punti. Se invece di annulla $\nabla g$ il metodo di Lagrange non funziona dato che non si riesce a determinare il moltiplicatore.
\end{tcolorbox}
\rule{\textwidth}{0,4pt}
\subsubsection{Il metodo delle restrizioni}

\rule{\textwidth}{2pt}
\subsection{Funzioni convesse di n variabili}
\rule{\textwidth}{0.4pt}
\subsubsection{Generalità sulle funzioni convesse}
Un insieme $\Omega \subseteq \mathbb{R}^n$ si dice convesso se per ogni coppia di punti $x_1, x_2 \in \Omega$ si ha che $[x_1,x_2]\subseteq \Omega$ (dove col simbolo $[x_1,x_2]$ si denota il segmento con estremi $x_1,x_2$); si dice strettamente convesso se per ogni coppia di punti $x_1,x_2 \in \Omega$ il segmento $(x_1,x_2)$ privato degli estremi è strettamente contenuto in $\Omega$.\newline

Si dice epigrafico di una funzione $f \;\;:\;\; \Omega \subseteq \mathbb{R}^n \rightarrow  \mathbb{R}$ l'insieme
\[
    epi(f)=\{(x,z) \in \mathbb{R}^{n+1} \;\;:\;\; z\geq f(x), x \in\Omega\}
\]
\newline

Si dice che una funzione è convessa se $epi(f)$ è un sottoinsieme convesso, si dice che una funzione è concava se $-f$ è convessa.\newline
Formalmente si dice che una funzione è convessa se e solo se per ogni $x_1,x_2$, $t \in [0,1]$ vale la condizione
\[
    f(tx_2 + (1-t)x_1) \leq tf(x_2) + (1-t)f(x_1).
\]
Si noti che $tx_2 + (1-t)x_1$ percorre il segmento $[x_1,x_2]$ al variare di $t \in[0,1]$\newline

Se $f$ è convessa allora:
\begin{itemize}
    \item $f$ è continua;
    \item $f$ ha derivate parziali destre e sinistre in ogni punto;
    \item nei punti in cui è derivabile, $f$ è anche dierenziabile.
\end{itemize}

Teorema di convessità e piano tangente. Sia $f \;\;:\;\; \Omega \rightarrow \mathbb{R}$ differenziabile in $\Omega$. Allora $f$ è convessa in $\Omega$ se e solo se per ogni coppia di punti $x_0, x \in \Omega$ si ha:
\[
    f(x) \geq f(x_0) + \nabla f(x_0) \cdot (x-x_0).
\]
In due dimensioni:
\[
    f(x,y) \geq f(x_0,y_0) + \frac{\delta f}{\delta x}(x_0,y_0)(x-x_0)+\frac{\delta f}{\delta y}(x_0, y_0)(y-y_0).
\]
che geometricamente signifca che il paino tangente in $x-0, y_0$ sta sotto $f$.\newline

Teorema di convessità e matrice Hessiana. Sia $f \in C^2(\Omega)$, con $\Omega$ aperto convesso in $\mathbb{R}^n$. Se per ogni $x_0$ in $\Omega$ la forma quadratica $d^2f(x_0)$ è semidefinita positiva, allora $f$ è convessa in $\Omega$. \newline
\rule{\textwidth}{0.4pt}
\subsubsection{Ottimizzazione di funzioni convesse e concave}
Nelle funzioni convesse (concave) i punti stazionari, se esistono, rappresentano minimi (massimi) globali. Inoltre se la funzione è strettamente convessa (concava), il punto critico è di minimo (massimo) globale forte, quindi, in particolare, è unico. \newline
\rule{\textwidth}{2pt}
\subsection{Funzioni definite implicitamente}
\rule{\textwidth}{0.4pt}
\subsubsection{Funzione implicita di una variabile}
\textbf{Teor. di Dini della funzione implicita}. Sia $A$ un aperto in $\mathbb{R}^2$ e $f \;\;:\;\; A \rightarrow \mathbb{R}$ una funzione $C^1(A)$. Supponiamo che in un punto $(x_0,y_0) \in A$ sia:
\[
    f(x_0,y_0)= 0 \;\;\;\; e \;\;\;\; f_y(x_0,y_0) \neq 0.
\] 
Allora esiste un intorno $I$ di $x_0$ in $\mathbb{R}$ e un'unica funzione $g \;\;:\;\; I \rightarrow \mathbb{R}$, tale che $y_0=g(x_0)$ e
\[
    f(x,g(x)) = 0 \;\;\;\; \;\forall\;x \in I.
\]
Inoltre, $g \in C^1(I)$ e 
\[
    g'(x) = - \frac{f_x(x,g_x)}{f_y(x,g_x)} \;\;\;\; \;\forall\;x \in I.
\]
Notiamo che se $f(x_0,y_0) = 0$ e $f_y(x_0, y_0) = 0$, ma $f_x(x_0, y_0) \neq 0$, il teorema è ancora applicabile scambiando gli ruoli di $x$ e $y$.\newline
In sostanza i punti in cui il teorema di Dini non è applicabile sono quelli in cui il gradiente di $f$ si annulla, ossia i punti critici. \newline
\rule{\textwidth}{0.4pt}
\subsubsection{Note sugli esercizi}
Tipicamente negli esercizi la richiesta è di calcolare la funzione $g'(x)$ in un punto. Si inizia calcolando i punti per cui $f(x,y) = 0$ (solitamente vengono forniti). Una volta trovati questi punti si deve verificare che $\frac{\delta f}{\delta y} f(x,y)$ (oppure $\frac{\delta f}{\delta x} f(x,y)$) sia $\neq 0$. Se queste condizioni si verificano, allora il Teorema di Dini è applicabile e si può procedere a calcolare $g'(x) = - \frac{\frac{\delta f}{\delta x}(x,y)}{\frac{\delta f}{\delta y} (x,y)}$ e a trovarne il valore in un punto.\newline

Se in un esercizio viene chiesto di calcolare, oltre a $g'(x)$, anche $g''(x)$, allora si può usare un altro procedimento: si può derivare rispetto a $x$ l'equazione $f(x, g(x)) = 0$ e quindi ricavare $g'(x)$. Succesivamente si può derivare ancora l'equazione e ricavare $g''(x)$. Spesso per risolvere queste equazioni è più semplice derivarle e poi sostituire $x = x_0$ con l'$x_0$ richiesto dall'esercizio.\newline
\rule{\textwidth}{2pt}