\fontsize{6pt}{8pt}\selectfont
\newgeometry{
    lmargin=0.2cm,
    rmargin=0.2cm,
    tmargin=0.2cm,
    bmargin=0.2cm,
}
\begin{landscape}
\begin{multicols*}{3}
    \subsection*{Terminologia}
    \[
        \begin{cases}
            \dot{x}(t) = f(x(t),u(t),t) = Ax + bu\\
            y(t) = g(x(t),u(t),t) = cx +du
        \end{cases}
    \]
    \textbf{Stato}: le varie $x$;\newline
    \textbf{Uscita}: $y$;\newline
    \textbf{Sistema dinamico}: se la conoscenza degli ingressi su un intervallo di tempo
    non `e sufficiente per determinare l’andamento delle uscite sullo stesso intervallo di tempo.\newline
    \textbf{Ordine del sistema}: numero di variabili di stato (quante $x$ ci sono).\newline
    \textbf{Lineare}: se $f$ e $g$ sono lineari in $x$ e in $u$ (non importa che lo siano in $t$), da notare che una funzione del tipo $x(t) \cdot u(t)$ non è lineare.\newline
    \textbf{Tempo invariante}: se in $f$ e $g$ non compare esplicitamente $t$;\newline
    \textbf{Strettamente proprio}: se in $g$ non compare $u$.\newline
    \textbf{SISO}: single input, single output; cioè se $u$ e $y$ sono scalari.
    \subsection*{Equilibrio}
    \subsubsection*{Caso generale}
    Dato l'ingresso costante $u = \bar{u}$ e detto $\bar{x}$ l'equilibrio cercato, in generale si ha:\newline
    \textbf{TC}: Soluzioni di $f(\bar{x},\bar{u}) = 0$.\newline
    \textbf{TD}: Soluzioni di $f(\bar{x}, \bar{u}) = \bar{x}$.\newline
    Per trovare le uscite di equilibrio $\bar{y}$ è sufficiente applicare gli stati di equilibrio $\bar{x}$ trovati all'uscita e vedere se l'espressione non perde senso e se produce soluzioni reali.
    \subsubsection*{Caso lineare}
    Se il sistema è lineare:\newline
    \textbf{TC, LTI}: Soluzioni di $A\bar{x} + b \bar{u} = 0$.\newline
    \textbf{TD, LTI}: Soluzioni di $A\bar{x} + b \bar{u} = \bar{x}$.\newline
    Se esiste $\bar{x}$, allora per forza esiste un uscite di equilibrio $\bar{y}$
    \subsubsection*{Caso non lineare}
    Se il sistema \textbf{non} è lineare:\newline
    L'uscita $\bar{y}$ potrebbe non avere senso per gli equilibri $\bar{x}$ trovati. 
    \subsection*{Movimento}
    \subsubsection*{Formula di Lagrange TC, LTI}
    \textbf{Per lo stato}:
    \[
            \begin{split}
            x(t) &= x_L(t) + x_F(t) =\\
            &=e^{At} x(0) + \int_{0}^{t}e^{A(t-\tau)}bu(\tau)d \tau
            \end{split}
    \]
    \[
        \begin{cases}
            x_L(t) = e^{At} x(0)\\
            x_F(t) = \int_{0}^{t}e^{A(t-\tau)}bu(\tau)d \tau
        \end{cases}
    \]
    \textbf{Per l'uscita}:
    \[
            \begin{split}
            y(t) 1 &= y_L(t) + y_F(t) =\\
            &= ce^{At}x(0) + c \int_{0}^{t}e^{A(t-\tau)}bu(\tau)d \tau + du(t)
            \end{split}
    \]
    \[
        \begin{cases}
            y_L(t) = ce^{At}x(0)\\
            y_F(t) = c \int_{0}^{t}e^{A(t-\tau)}bu(\tau)d \tau + du(t)
        \end{cases}
    \]
    \subsubsection*{Formula di Lagrange TD, LTI}
    \textbf{Per lo stato}:
    \[
            \begin{split}
            x(k) &= x_L(k) + x_F(k) =\\
            &=A^k x(0) + \sum_{l=0}^{k-1}A^{k-l-1}bu(l)
            \end{split}
    \]
    \[
        \begin{cases}
            x_L(k) A^k x(0)\\
            x_F(k) \sum_{l=0}^{k-1}A^{k-l-1}bu(l)
        \end{cases}
    \]
    \textbf{Per l'uscita}:
    \[
        \begin{split}
            y(k) &= y_L(k) + y_F(k) =\\
            &= cx(k)+du(k) = \\
            &=cA^k x(0) + c\sum_{l=0}^{k-1}A^{k-l-1}bu(l) + du(k)
        \end{split}
    \]
    \[
        \begin{cases}
            y_L(k) = cA^k x(0)\\
            y_F(k) = c\sum_{l=0}^{k-1}A^{k-l-1}bu(l) + du(k)
        \end{cases}
    \]
    \subsubsection*{Esponenziale di matrice (con $A$ diagonalizzabile)}
    $A$ è \textbf{diagonalizzabile} se:
    \begin{itemize}
        \item il numero di autovalori contati con la loro molteplicità è pari all’ordine della matrice;
        \item la molteplicità geometrica di ciascun autovalore coincide con la relativa molteplicità algebrica;
        \item una matrice quadrata di ordine $n$ che ammette esattamente $n$ autovalori distinti è sicuramente diagonalizzabile.
    \end{itemize}
    Per calcolare $e^{At}$ si seguono i seguenti passaggi:
    \begin{itemize}
        \item Calcolare gli autovalori di $A$ (risolvere $det[A-\lambda I] = 0$, oppure se matrice triangolare, gli autovalori sono sulla diagonale);
        \item Calcolare gli autovettori corrsipondenti per ogni autovalore \newline (risolvere $[A-\lambda_i I] \cdot \left[\begin{matrix}
            x\\y
        \end{matrix}\right] = \left[\begin{matrix}
            0\\0
        \end{matrix}\right]$);
        \item definire la matrice diagonalizzante $T$ come l'accostamento degli autovettori trovati (ricordando l'ordine) e calcolare $T^{-1}$. Per l’inversa della generica matrice $T$ 2x2 si moltiplica $\frac{1}{det(T)}$ per la matrice ottenuta da $T$ scambiando di posto i termini sulla diagonale principale e invertendo
        il segno dei termini sulla diagonale secondaria.
        \item notare che $T^{-1} A T = D = $ matrice con gli autovalori di $A$ lungo la diagonale nell'ordine in cui compaiono gli autovettori in $T$.
        \item Calcolare $e^{At}$:\newline
        $e^{At} = e^{TDT^{-1} t} = T e^{Dt} T^{-1} = \dots$
    \end{itemize}
    \subsubsection*{Movimento libero con Jordanizzazione (con $A$ non diagonalizzabile)}
    Se la matrice $A$ non è diagonalizzabile calcolare $e^{At}$ col metodo spiegato prima non è fattibile, piuttosto usare questo procedimento:
    \begin{itemize}
        \item Calcolare gli autovalori di $A$;
        \item Se ci sono autovalori coincidenti, cercare di calcolare gli autovettori corrispondenti (risulterà possibile se e solo se la molteplicità geometrica è pari alla molteplicità algebrica). Se non si riescono a trovare abbastanza autovettori bisogna usare la Jordanizzazione, vediamo come fare.
        \item per ogni autovalore multiplo, chiamiamolo per semplicità $\lambda$, bisogna cercare gli autovettori generalizzati corrispondenti e da questi creare una catena di autovettori.
        \item Il primo autovettore generalizzato è quello classico calcolato come soluzione di $(A-\lambda I_n) x = 0$.
        \item Il secondo autovettore generalizzato è calcolato come soluzione di $(A-\lambda I_n)^2 x = 0$, dove per $(A-\lambda I_n)^2$ è sufficiente fare il prodotto della matrice per sè stessa. E' tipico che un autovettore generalizzato abbia come soluzione un insieme di soluzioni, se siamo al secondo, tipicamente ce ne saranno due.
        \item Tutti gli altri autovettori generalizzati si calcolano allo stesso modo incrementando di uno l'esponente ogni volta.
        \item Una volta trovati tutti gli autovettori generalizzati bisogna fare una catena di autovettori.
        \item A partire dall'ultimo autovettore generalizzato trovato si sceglie un vettore, che chiamo qua $v_n$, fra quelli della soluzione che non appartenga all'insieme dei vettori soluzione dell'autovettore generalizzato precedente. Una volta selezionato, si trova l'autovettore precedente come $v_{n-1} = (A- \lambda I_n)v_n$. E si prosegue così anche per quello prima: $v_{n-2} = (A-\lambda I_n) v_{n-1}$.
        \item una volta terminata la catena di autovettori, questi, accostati in colonna, rappresentano la matrrice jordanizzante $P$, tale che $P^{-1} A P = J =$ matrice in forma canonica di Jordan.
    \end{itemize}
    Vediamo ora come trovare la forma canonica di Jordan della matrice $A$ nel caso 2x2:
    \begin{itemize}
        \item se $\lambda_1 \neq \lambda_2$, allora la matrice di Jordan è la matrice diagonale classica;
        \item se $\lambda_1 = \lambda_2$:
        \begin{itemize}
            \item se ammette due autovettori, allora siamo ancora nella matrice diagonale classica;
            \item se la matrice ammette un solo autovettore, allora abbiamo una matrice di Jordan: $\left[\begin{matrix}
                \lambda & 1\\ 0 & \lambda
            \end{matrix}\right]$
        \end{itemize}
    \end{itemize}
    Vediamo ora come trovare la forma canonica di Jordan della matrice $A$ nel caso 3x3:
    \begin{itemize}
        \item se $\lambda_1 \neq \lambda_2 \neq \lambda_3$ allora la matrice di jordan è la solita diagonale;
        \item se $\lambda_1 = \lambda_2 = \lambda$ e $\lambda_3 \neq \lambda$ allora:
        \begin{itemize}
            \item se $\lambda$ ammette due autovettori, allora siamo alla diagonale solita;
            \item se $\lambda$ non ammette due autovettori, allora la matrice di Jordan:$\left[\begin{matrix}
                \lambda & 1 & 0 \\
                0 & \lambda & 0\\
                0 & 0 & \lambda_3
            \end{matrix}\right]$
        \end{itemize}
        \item se $\lambda_1 = \lambda_2 = \lambda_3 = \lambda$, allora:
        \begin{itemize}
            \item se $\lambda$ ammette tre autovettori, allora siamo alla solita diagonale;
            \item se $\lambda$ ammette due autovettori, allora la matrice di jordan: $\left[\begin{matrix}
                \lambda & 1 & 0 \\
                0 & \lambda & 0\\
                0 & 0 & \lambda
            \end{matrix}\right]$
            \item se $\lambda$ ammette un solo autovettore, allora la matrice di Jordan: $\left[\begin{matrix}
                \lambda & 1 & 0 \\
                0 & \lambda & 1\\
                0 & 0 & \lambda
            \end{matrix}\right]$
        \end{itemize}
    \end{itemize}
    Una volta determinate le matrici jordanizzanti e la matrice di jordan si può proseguire a calcolare il movimento libero:
    \begin{itemize}
        \item partendo dal sistema $\dot{x} = A x$, il movimento libero è $x_L(t) = e^{At}x(0)$;
        \item per prima cosa si fa un cambio di variabile: $\bar{x} = P^{-1}x$, per cui il sistema diventa $P \bar{\dot{x}} = A P \bar{x}$, e spostando $P$ otteniamo $\bar{\dot{x}} = P^{-1} A P \bar{x} = J \bar{x}$.
        \item Siccome la matrice di Jordan è triangolare alta si può ora evitare di usare le matrici e trasformare il sistema in forma scalare. Per esempio, ipotizzando $J = \left[\begin{matrix}
            a & b \\ 0 & c
        \end{matrix}\right]$ otteniamo il sistema $\begin{cases}
            \bar{\dot{x}}_1 = a \bar{x}_1 + b \bar{x}_2\\
            \bar{\dot{x}}_2 = c \bar{x}_2
        \end{cases}$.
        \item la seconda equazione contiene solo $\bar{x}_2$ e quindi il suo movimento libero è $\bar{x}_2 = e^{ct}\bar{x}_2(0)$
        \item Il movimento libero della prima equazione è ora calcolabile come moviemento completo (!) considerando $b \bar{x}_2$ come fosse un ingresso: $\bar{x}_1 = e^{at}\bar{x}_1(0) + \int_{0}^{t} e^{a(t-\tau)} b \bar{x}_2(\tau) d \tau$ e sostituendo il $\bar{x}_2(\tau)$ trovato al punto precedente.
        \item Abbiamo ora trovato le due componenti del vettore movimento libero per $\bar{x}_L$, bisogna però sostituire i valori iniziali $\bar{x}(0) = P^{-1}x(0)$, e, infine, non ci resta che tornare alla variabile iniziale: $x_L (t) = P \bar{x}_L$
    \end{itemize}
    \subsubsection*{Osservazioni}
    \begin{itemize}
        \item Se è richiesto il calcolo del movimento dell'uscita, è sufficiente calcolare il movimento dello stato e sostituirlo nell'equazione dell'uscita del sistema.
        \item Se gli autovalori di $A$ sono numeri complessi: una volta ricondotti i numeri complessi in $x_L$ alla loro forma trigonometrica ($cos(\theta) + i sin(\theta) = e^{i \theta}$), si prende la sola parte reale, l'unità immaginaria deve scomparire.
        \item Alla fine in $x_L$ devono comparire somme di termini del tipo $e^{\alpha t} cos(\omega t)$ e/o $e^{\alpha t} sin(\omega t)$, dove $\alpha$ e $\omega$ sono rispettivamente perti reali e immaginarie di autovalori di $A$. In realtà si possono trovare anche altri termini, ma in via generale se si trova un coefficiente del tempo all'esponente che non è parte reale di nessun autovalore di $A$, il risultato è certamente errato.
        \item TODO: esercizio 2.3, caso autovalore multipli, questioni sui autovettori generalizzati\dots.
        \item Il movimento libero dipende linearmente solo dallo stato iniziale e non dall’ingresso, il movimento forzato dipende linearmente solo dall’ingresso e non dallo stato iniziale.
    \end{itemize}
    \subsection*{Stabilità}
    \begin{itemize}
        \item Tutti gli equilibri hanno le stesse caratteristiche di stabilità.
        \item Nei sistemi lineari (tempo invarianti) la stabilità è una proprietà del sistema, e non una proprietà dell’equilibrio. 
        \item Nei sistemi \textbf{asintoticamente stabili} i movimenti \textbf{liberi} di stato e di uscita tendono a $0$ per $t \rightarrow \infty$ (dimenticano lo stato inizile, movimento libero trascurabile a lungo andare) qualunque sia lo stato iniziale, mentre se li si sottopone a ingresso costante i movimenti (complessivi, non solo liberi) di stato e di uscita tendono ai medesimi valori costanti qualunque sia lo stato iniziale;
        \item Nei sistemi \textbf{instabili} il movimento \textbf{libero} di stato e di uscita diverge a patto di alcune eccezioni. Diverge in generale anche il movimento (complessivo, non solo libero) di stato e di uscita a frornte di un ingresso costante.
        \item nei sistemi \textbf{semplicemente stabili} i movimenti \textbf{liberi} di stato e di uscita nè divergono nè tendono a zero e il loro comportamento asintotico non è lo stesso per qualunque stato iniziale.
    \end{itemize}
    \subsubsection*{Criteri di stabilità}
    \textbf{tempo continuo}:
    \begin{itemize}
        \item Tutti gli autovalori di $A$ hanno $Re < 0 \Longleftrightarrow $ sistema AS;
        \item Almeno un autovalore di $A$ ha $Re > 0 \Longrightarrow$ sistema I;
        \item Tutti gli autovalori di $A$ hanno $Re \leq 0$ e ne esiste almeno uno con $Re = 0$ $\Longrightarrow$ $\begin{cases}
            \text{sistema I;}\;\\
            \text{oppure sistema S, ma non AS.}\;
        \end{cases}$. \newline
        Per capire se si tratta di un sistema stabile o instabile bisogna andare a calcolare il movimento libero per un generico ingresso $x(0)$ e vederne il comportamento per $t \rightarrow \infty$. Se il movimento libero rimane sempre limitato allora il sistema è stabile, se il movimento libero diverge (anche una sola delle componenti del movimento libero) allora il sistema è instabile. \newline
        Esiste anche un altro metodo: guardando la matrice di Jordan, se il più grande miniblocco di Jordan ha dimensione $1$ allora il sistema è stabile, altrimenti è instabile. Un buon metodo per ricordarlo è usare i seguenti due esempi che abbiamo visto a esercitazione:
        \[
            \left[\begin{matrix}
                0 & 0 \\
                0 & 0
            \end{matrix}\right] \;\;\text{due miniblocchi di dimensione 1, quindi S}\;
        \]
        \[
            \left[\begin{matrix}
                0&1\\
                0&0
            \end{matrix}\right] \;\; \text{un miniblocco di dimensione 2, quindi I}\;
        \]
    \end{itemize}
    \textbf{tempo discreto}:
    \begin{itemize}
        \item Tutti gli autovalori di $A$ hanno $|\lambda_i| < 1$ $\Longleftrightarrow$ sistema AS.
        \item Almeno un autovalore di $A$ con modulo $|\lambda_i| > 1$ $\Longrightarrow$ sistema I.
        \item Tutti gli autovalori di $A$ hanno $|\lambda_i| \leq 1$ e ne esiste almeno uno tale che $|\lambda_1| = 1$ $\Longrightarrow$ $\begin{cases}
            \text{sistema I;}\;\\
            \text{oppure sistema S, ma non AS.}\;
        \end{cases}$. \newline
        Per capire se il sistema è instabile o stabile nell'ultimo punto guarda il caso a tempo continuo.
    \end{itemize}
    \textbf{oss.} Errore tipico: Nel caso a tempo continuo si guarda la parte reale degli autovalori, nel caso a tempo discreto si guarda il modulo (!) degli autovalori.
    \subsubsection*{Criteri di stabilità dedotti dalla matrice $A$}
    \begin{itemize}
        \item Se il sistema è asintoticamente stabile, allora $tr(A)$ è negativa, ma non vale il viceversa.
        \item se $det(A) = 0$ esiste $S_i = 0$ $\Longrightarrow$ no AS.
        \item se $tr(A) >0$ $\Longrightarrow$ sistema I.
        \item se $tr(A) = 0$ $\Longrightarrow$ no AS.
        \item Se $Re(S_i)<0$ per ogni $i$ (cioè se il sistema è asintoticamente stabile), allora i coefficienti di $\Pi(S)$ sono tutti concordi e non nulli \newline
        \textbf{oss.} Errore tipico: il viceversa vale solo per polinomi del secondo ordine.
    \end{itemize}
    \textbf{oss.} Per stabilire se un sistema è asintoticamente stabile abbiamo due possibilità: per prima cosa con i primi due dei tre criteri appena esposti stabiliamo se non è asintoticamente stabile, altrimenti, se non abbiamo avuto fortuna con questi criteri, usiamo il criterio di Routh. Un caso particolare è rappresentato dai polinomi di secondo ordine, in cui possiamo evitare di usare Routh e usare il terzo criterio appena visto.
    \subsubsection*{Routh}
    \[
        \Pi(s) = a_0s^n + a_1 s^{n-1} + \dots + a_{n-1}s + a_n
    \]
    Prime due righe
    \[
        \begin{matrix}
            a_0 & \;\;\;\;\;\; a_2 & \;\;\;\;\;\;\dots\\
            \;\;\downarrow & \nearrow \;\; \downarrow & \nearrow \;\; \downarrow\\
            a_1 & \;\;\;\;\;\; a_3 & \;\;\;\;\;\;\dots 
        \end{matrix}
    \]
    a seconda che il numero di termini sia pari (le due righe sono di pari lunghezza) o dispari (la prima riga ha un termine in più della seconda, per cui si aggiunge uno $0$) l'ultima colonna può terminare in due modi:
    \[
        \begin{matrix}
            \dots & a_{n-1}\\
            \;\\
            \dots & a_n
        \end{matrix}\;\;\;\;\;\;\;\;\;\;\; \text{oppure}\;\;\;\;\;\;\;\;\;\;\; \begin{matrix}
            \dots &a_n\\
            \;\\
            \dots & 0 
        \end{matrix}
    \]
    In totale, considerando anche le prime due righe, ci sono $n+1$ righe.\newline
    Ogni riga dalla terza in poi dipende dalle due precedenti seguendo una regola:
    \[
        \begin{matrix}
            h_1 & h_2 & h_3 &\dots\\
            \;\\
            q_1 & q_2 & q_3 & \dots\\
            \;\\
            w_1 & w_2 & w_3 & \dots
        \end{matrix}
    \]
    prese due generiche righe ($h_i$ e $q_i$), i termini della riga successiva ($w_i$) si costruiscono come $w_i = - \frac{1}{q_1} det\left[\begin{matrix}
        h_1 & h_{i+1} \\
        q_1 & q_{i+1}
    \end{matrix}\right]$.\newline
    Se manca un termine in una delle righe precedenti ($h$ e $q$) si assume nullo.\newline
    Se troviamo un elemento nullo in prima colonna, ci si ferma, sicuramente il sistema non è asintoticamente stabile, e siamo in presenza di un caso particolare che non ci permette di calcolare la tabella di Routh.\newline
    \newline
    Un sistema dinamico con polinomio caratteristico $\Pi(s)$ è asintoticamente stabile se e solo se tutti gli elementi della prima colonna della tabella di Routh sono concordi (e non nulli).
    \subsection*{Linearizzazione}
    \subsubsection*{Linearizzazione di sistemi tempo invarianti non lineari nell'intorno di un equilibrio}
    Dato il sistema dinamico 
    \[
        \begin{cases}
            \dot{x}_1(t) = f_1(x_1(t),x_2(t),\dots,x_n(t),u(t))\\
            \dot{x}_2(t) = f_2(x_1(t),x_2(t),\dots,x_n(t),u(t))\\
            \dots\\
            \dot{x}_n(t) = f_n(x_1(t),x_2(t),\dots,x_n(t),u(t))\\
            y(t) = g(x_1(t),x_2(t),\dots,x_n(t),u(t))
        \end{cases}
    \]
    e un suo equilibrio $(\bar{x}, \bar{u})$ a fronte dell'ingresso costante $\bar{u}$, tale sistema è approssimabile con uno lineare e stazionario ottenuto dall'espansione in serie delle $f_i$ e $g$ al prim'ordine:
    \[
        \begin{cases}
            \dot{\delta x} (t) = A \delta x(t) + b \delta u(t)\\
            \delta y (t) = c \delta x (t) + d \delta u (t)
        \end{cases}
    \]
    con:
    \[
        A = \left[\begin{matrix}
            \frac{\delta f_1 (x,u)}{\delta x_1}_{\bar{x}, \bar{u}} & \frac{\delta f_1 (x,u)}{\delta x_2}_{\bar{x}, \bar{u}} & \dots & \frac{\delta f_2 (x,u)}{\delta x_n}_{\bar{x}, \bar{u}}\\
            \frac{\delta f_2(x,u)}{\delta x_1}_{\bar{x}, \bar{u}} & \frac{\delta f_2(x,u)}{\delta x_2}_{\bar{x}, \bar{u}} & \dots & \frac{\delta f_1(x,u)}{\delta x_n}_{\bar{x}, \bar{u}}\\
            \dots & \dots & \dots & \dots\\
            \frac{\delta f_n(x,u)}{\delta x_1}_{\bar{x}, \bar{u}} & \frac{\delta f_n (x,u)}{\delta x_2}_{\bar{x}, \bar{u}} & \dots & \frac{\delta f_n (x,u)}{\delta x_n}_{\bar{x}, \bar{u}} 
        \end{matrix}\right]
    \]
    \[
        b = \left[\begin{matrix}
            \frac{\delta f_1 (x,u)}{\delta u}_{\bar{x}, \bar{u}}\\
            \frac{\delta f_2 (x,u)}{\delta u}_{\bar{x}, \bar{u}}\\
            \dots\\
            \frac{\delta f_n (x,u)}{\delta u}_{\bar{x}, \bar{u}}
        \end{matrix}\right]
    \]
    \[
        c = \left[\begin{matrix}
            \frac{\delta g(x,u)}{\delta x_1}_{\bar{x}, \bar{u}} &
            \frac{\delta g (x,u)}{\delta x_2}_{\bar{x}, \bar{u}} & 
            \dots &
            \frac{\delta g (x,u)}{\delta x_n}_{\bar{x}, \bar{u}}
        \end{matrix}\right]
    \]
    \[
        d = \frac{\delta g (x,u)}{\delta u}_{\bar{x}, \bar{u}}
    \]
    dove il pedice $|_{\bar{x}, \bar{u}}$ indica che sono derivate parziali valutate nell'equilibrio, e
    \[
        \delta x := x-\bar{x} \;\;\;\;\;\delta u := u- \bar{u} \;\;\;\;\; \delta y = := y- \bar{y}
    \]
    \subsubsection*{Stabilità dell'equilibrio di sistemi tempo invarianti non lineari}
    La stabilità di un equilibrio si può studiare basandosi sul sistema linearizzato nell'intorno di quell'equilibrio:
    \begin{itemize}
        \item se il sistema linearizzato è AS, allora anche l'equilibrio è AS;
        \item se il sistema linearizzato è I, allora anche l'equilibrio è I;
        \item se il sistema linearizzato è S, allora nulla si può dire sulla stabilità dell'equilibrio, che in tal caso dipende da termini dello sviluppo in serie di $f$ di grado superiore a uno.
    \end{itemize}
    \subsection*{Trasformate e funzioni di trasferimento a TC}
    \subsubsection*{Laplace}
    \[
        V(s) := \mathcal{L}[v(t)]
    \]
    \textbf{Calcolo della trasformata di Laplace}: $\mathcal{L}[\_\_] =  \int_{0}^{\infty}\_\_ e^{-st} dt$.\newline
    \textbf{Linearità}: $\mathcal{L}[\alpha_1 v_1(t) + \alpha_2 v_2(t)] = \alpha_1 \mathcal{L}[v_1(t)] + \alpha_2 \mathcal{L}[v_2(t)]$.\newline
    \textbf{Derivata}: $\mathcal{L}\left[ \frac{d v(t)}{dt} \right] = s \mathcal{L}[v(t)] - v(0)$.\newline
    \textbf{Integrale}: $\mathcal{L}\left[ \int_{0}^{t} v(\tau) d \tau \right] = \frac{1}{s} \mathcal{L}[v(t)]$.\newline
    \textbf{Ritardo}: $ \mathcal{L}[ v(t - \tau)] = e^{-s \tau} \mathcal{L}[v(t)]$ (spesso utilizzata al contrario per Heaviside: $\mathcal{L}^{-1}\left[\frac{N(s)}{D(s)}e^{-s \tau}\right] \rightarrow $ Heaviside di $\frac{N(s)}{D(s)}$ ottengo $y(t)$, poi con $e^{-s \tau}$ aggiungo ritardo $\tau$ e $\rightarrow  y(t- \tau)$).\newline
    \renewcommand{\arraystretch}{2}
    \begin{center}
        \begin{tabular}{ |c|c| } 
        \hline
        \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$v(t)$ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$V(s)$ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\ 
        \hline
        $k \cdot imp(t)$ & $k \cdot 1$ \\ 
        $k \cdot sca(t)$ & $k \cdot \frac{1}{s}$  \\ 
        $k \cdot ram(t) =k \cdot t \cdot  sca(t)$ & $k \cdot \frac{1}{s^2}$ \\
        $k \cdot e^{at}sca(t)$ & $k \cdot \frac{1}{s-a}$ \\ 
        $t^{n}\cdot e^{at}sca(t)$ & $\frac{n!}{(s-a)^{n+1}}$\\ 
        \hline
        \end{tabular}
    \end{center}
    \renewcommand{\arraystretch}{1}
    Notiamo che i segnali trigonometrici sono rappresentabili come esponenziali complessi, per esempio $sin(\omega t) = \frac{e^{j \omega t}- e^{-j \omega t}}{2j} \dots$ e così possiamo ricavare le \textbf{trasformate di Laplace di tutte le funzioni trigonometriche}.
    \subsubsection*{Antitrasformazione con Heaviside}
    \[
        V(s) = \frac{N(s)}{D(s)}
    \]
    con $N,D$ polinomi in $s$.\newline
    Le radici di $N(s)$ sono gli \textbf{zeri}, mentre le radici di $D(s)$ sono i \textbf{poli}.\newline
    \begin{itemize}
        \item fattorizzare $D(s)$ in modo che sia espresso come prodotto di termini del tipo $(s-p)$ (\textbf{polo reale semplice}) oppure $(s-p)^n$ (\textbf{polo reale multiplo}):
        \item \textbf{polo reale semplice}: $\frac{N(s)}{\dots(s-p)\dots} = \dots + \frac{\alpha}{s-p} + \dots$.
        \item \textbf{polo reale multiplo}: $\frac{N(s)}{\dots (s-p)^n \dots} = \dots + \frac{\alpha_1}{s-p} + \frac{\alpha_2}{(s-p)^2} + \dots + \frac{\alpha_n}{(s-p)^n} + \dots$
    \end{itemize}
    A seguito della scomposizione l'antitrasformata è facilmente calcolabile come usando le trasformate notevoli.
    \subsubsection*{Formula di Lagrange nel dominio delle trasformate}
    \textbf{Per lo stato}:
    \[
        X(s) = X_L(s) + X_F(s)
    \]
    \[
        \begin{cases}
            X_L(s) = (sI-A)^{-1} x(0)\\
            X_F(s) = (sI-A) ^{-1} b U(s)
        \end{cases}
    \]
    \textbf{Per l'uscita}:
    \[
        Y(s) = cX(s) + dU(s) = Y_L(s) + Y_F(s)
    \]
    \[
        \begin{cases}
            Y_L(s) = c(sI-A)^{-1} x(0)\\
            Y_F(s) = [c(sI-A)^{-1} b + d]U(s)
        \end{cases}
    \]
    \subsubsection*{FdT}
    Il tipico sistema dinamico diventa:
    \[
        \begin{cases}
            s X(s) = AX(s) + b U(s)\\
            Y(s) = cX(s) + d U(s)
        \end{cases}
    \]
    \textbf{funzione di trasferimento}:
    \[
        G(s) := c(sI-A)^{-1} b + d = \frac{\tilde{N}(s)}{D(s)} + d = \frac{N(s)}{D(s)}
    \]
    La funzione di trasferimento è un rapporto di polinomi il cui denominatore $D(s)$ è il polinomio caratteristico della matrice $A$, quindi i poli sono gli autovalori di $A$. In alcuni casi può capitare che alcuni termini del numeratore si semplifichino con alcuni temrini del denominatore e quindi non tutti gli autovalori di $A$ compaiano come poli.\newline
    Se e solo se $d=0$ (cioè se il sistema è strettamente proprio) abbiamo $G(s) = \frac{\bar{N}(s)}{D(s)}$ in cui il grado del numeatore è minore del grado del denominatore.\newline
    Altrimenti (se e solo se $d\neq 0$) abbiamo $G(s) = \frac{\bar{N}(s) + dD(s)}{D(s)} = \frac{N(s)}{D(s)}$, con $N(s) = \bar{N}(s) +dD(s)$, in cui il grado del numeratore è uguale al grado del denominatore.
    \end{multicols*}
\end{landscape}